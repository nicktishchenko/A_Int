{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaef450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries and Setup\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "from paddleocr import PaddleOCR\n",
    "import zipcodes\n",
    "import platform\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# Set up OCR engines\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Force PaddleOCR to use CPU on macOS\n",
    "\n",
    "paddle_ocr = PaddleOCR(\n",
    "    use_angle_cls=True, lang=\"en\", show_log=False\n",
    ")  # CPU-only for PaddleOCR\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"reports\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cf718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Environment Setup with Unified Schema & Visualization Support\n",
    "\n",
    "# Setup OCR engine and file discovery\n",
    "test_folder = os.path.join(os.getcwd(), \"test\")\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang=\"en\", show_log=False)\n",
    "\n",
    "image_extensions = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".tif\"]\n",
    "file_list = [\n",
    "    f\n",
    "    for f in os.listdir(test_folder)\n",
    "    if any(f.lower().endswith(ext) for ext in image_extensions)\n",
    "]\n",
    "print(f\"Found {len(file_list)} image files in test folder\")\n",
    "\n",
    "EXPECTED_COLUMNS = [\n",
    "    \"serial_number\",\n",
    "    \"event_type\",\n",
    "    \"event_date\",\n",
    "    \"associated_name\",\n",
    "    \"associated_address\",\n",
    "    \"filename\",\n",
    "    \"file_creation_date\",\n",
    "    \"file_modification_date\",\n",
    "    \"file_location\",\n",
    "]\n",
    "_reader_instance = None\n",
    "\n",
    "\n",
    "def ensure_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in EXPECTED_COLUMNS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    return df[EXPECTED_COLUMNS].copy()\n",
    "\n",
    "\n",
    "if \"df\" not in globals():\n",
    "    df = pd.DataFrame(columns=EXPECTED_COLUMNS)\n",
    "else:\n",
    "    df = ensure_schema(df)\n",
    "\n",
    "print(\"Environment setup complete with target schema:\")\n",
    "print(EXPECTED_COLUMNS)\n",
    "\n",
    "# Visualization helper with semantic highlighting\n",
    "CATEGORY_COLORS = {\n",
    "    \"serial_number\": (255, 0, 0),      # Red - Serial Number\n",
    "    \"event_type\": (255, 140, 0),       # Orange - Event Type\n",
    "    \"event_date\": (255, 215, 0),       # Gold/Yellow - Event Date\n",
    "    \"associated_name\": (30, 144, 255),  # Blue - Associated Name\n",
    "    \"associated_address\": (148, 0, 211), # Purple - Associated Address\n",
    "    \"other\": (128, 128, 128),          # Gray - Other\n",
    "}\n",
    "\n",
    "\n",
    "def classify_token(text_lower):\n",
    "    # Lightweight classification for event_type cues / names / address parts\n",
    "    event_keywords = [\n",
    "        \"inventory\",\n",
    "        \"inspection\",\n",
    "        \"theft\",\n",
    "        \"loss\",\n",
    "        \"stolen\",\n",
    "        \"missing\",\n",
    "        \"burglary\",\n",
    "        \"incident\",\n",
    "        \"transfer\",\n",
    "        \"disposal\",\n",
    "        \"larceny\",\n",
    "    ]\n",
    "\n",
    "    date_keywords = [\n",
    "        \"date\",\n",
    "        \"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\",\n",
    "        \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"\n",
    "    ]\n",
    "    \n",
    "    date_patterns = [\n",
    "        r'\\d{1,2}/\\d{1,2}/\\d{2,4}',  # MM/DD/YYYY or DD/MM/YYYY\n",
    "        r'\\d{1,2}-\\d{1,2}-\\d{2,4}',  # MM-DD-YYYY or DD-MM-YYYY\n",
    "    ]\n",
    "\n",
    "    address_keywords = [\n",
    "        \"street\",\n",
    "        \"st\",\n",
    "        \"ave\",\n",
    "        \"avenue\",\n",
    "        \"road\",\n",
    "        \"rd\",\n",
    "        \"dr\",\n",
    "        \"drive\",\n",
    "        \"lane\",\n",
    "        \"ln\",\n",
    "        \"blvd\",\n",
    "        \"boulevard\",\n",
    "        \"suite\",\n",
    "        \"ste\",\n",
    "        \"apt\",\n",
    "        \"unit\",\n",
    "        \"city\",\n",
    "        \"state\",\n",
    "        \"zip\"\n",
    "    ]\n",
    "    \n",
    "    # Check for event type\n",
    "    if any(k in text_lower for k in event_keywords):\n",
    "        return \"event_type\"\n",
    "    \n",
    "    # Check for date patterns\n",
    "    if any(k in text_lower for k in date_keywords) or any(re.search(pattern, text_lower) for pattern in date_patterns):\n",
    "        return \"event_date\"\n",
    "    \n",
    "    # Check for address components\n",
    "    if any(k in text_lower for k in address_keywords):\n",
    "        return \"associated_address\"\n",
    "    \n",
    "    # Default category\n",
    "    return \"other\"\n",
    "\n",
    "\n",
    "def visualize_ocr(\n",
    "    img_bgr,\n",
    "    ocr_results,\n",
    "    serial_candidate=None,\n",
    "    metadata_info=None,\n",
    "    show=True,\n",
    "    title=None,\n",
    "):\n",
    "    draw = img_bgr.copy()\n",
    "    serial_norm = str(serial_candidate) if serial_candidate else None\n",
    "    \n",
    "    # Extract metadata for each component\n",
    "    component_tokens = {\n",
    "        \"associated_name\": set(),\n",
    "        \"associated_address\": set(),\n",
    "        \"event_type\": set(),\n",
    "        \"event_date\": set()\n",
    "    }\n",
    "    \n",
    "    if metadata_info:\n",
    "        # Process associated_name\n",
    "        if metadata_info.get(\"associated_name\"):\n",
    "            for part in str(metadata_info[\"associated_name\"]).split():\n",
    "                component_tokens[\"associated_name\"].add(part.lower())\n",
    "        \n",
    "        # Process associated_address\n",
    "        if metadata_info.get(\"associated_address\"):\n",
    "            for part in re.split(r\"[\\s,]\", str(metadata_info[\"associated_address\"])):\n",
    "                if part:\n",
    "                    component_tokens[\"associated_address\"].add(part.lower())\n",
    "        \n",
    "        # Process event_type\n",
    "        if metadata_info.get(\"event_type\"):\n",
    "            for part in str(metadata_info[\"event_type\"]).split():\n",
    "                component_tokens[\"event_type\"].add(part.lower())\n",
    "        \n",
    "        # Process event_date\n",
    "        if metadata_info.get(\"event_date\"):\n",
    "            for part in str(metadata_info[\"event_date\"]).split():\n",
    "                component_tokens[\"event_date\"].add(part.lower())\n",
    "    \n",
    "    # Process each OCR result\n",
    "    for bbox, text, conf in ocr_results:\n",
    "        t_norm = text.strip()\n",
    "        t_lower = t_norm.lower()\n",
    "        \n",
    "        # Determine category for color coding\n",
    "        if serial_norm and t_norm == serial_norm:\n",
    "            color_key = \"serial_number\"\n",
    "        elif any(t_lower in tokens for component, tokens in component_tokens.items() if component == \"associated_name\"):\n",
    "            color_key = \"associated_name\"\n",
    "        elif any(t_lower in tokens for component, tokens in component_tokens.items() if component == \"associated_address\"):\n",
    "            color_key = \"associated_address\"\n",
    "        elif any(t_lower in tokens for component, tokens in component_tokens.items() if component == \"event_type\"):\n",
    "            color_key = \"event_type\"\n",
    "        elif any(t_lower in tokens for component, tokens in component_tokens.items() if component == \"event_date\"):\n",
    "            color_key = \"event_date\"\n",
    "        else:\n",
    "            color_key = classify_token(t_lower)\n",
    "        \n",
    "        # Get color for the category\n",
    "        color = CATEGORY_COLORS.get(color_key, CATEGORY_COLORS[\"other\"])\n",
    "        \n",
    "        # Draw bounding box\n",
    "        pts = np.array(bbox, np.int32).reshape((-1, 1, 2))\n",
    "        cv2.polylines(\n",
    "            draw,\n",
    "            [pts],\n",
    "            isClosed=True,\n",
    "            color=color,\n",
    "            thickness=2 if color_key != \"serial_number\" else 3,  # Fixed key name\n",
    "        )\n",
    "        \n",
    "        # Add text label\n",
    "        x, y = pts[0][0]\n",
    "        label = f\"{t_norm} ({conf:.2f})\"\n",
    "        \n",
    "        # Handle long labels gracefully\n",
    "        if len(label) > 38:\n",
    "            label = label[:35] + \"...\"\n",
    "        \n",
    "        cv2.putText(\n",
    "            draw,\n",
    "            label,\n",
    "            (x, y - 6),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.55,\n",
    "            color,\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "    \n",
    "    # Display visualization if requested\n",
    "    if show:\n",
    "        img_rgb = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "        plt.figure(figsize=(13, 9))\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\n",
    "            title\n",
    "            or \"OCR Visualization (serial=red, event_type=orange, event_date=yellow, name=blue, address=purple, other=gray)\"\n",
    "        )\n",
    "        plt.show()\n",
    "    \n",
    "    return draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f12a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Metadata Extraction\n",
    "import os\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_file_metadata(file_path):\n",
    "    \"\"\"Get comprehensive file metadata - platform agnostic, honest timestamps\"\"\"\n",
    "    try:\n",
    "        file_stats = os.stat(file_path)\n",
    "        system = platform.system().lower()\n",
    "\n",
    "        # Platform-specific creation time handling - report what the OS actually provides\n",
    "        if system == \"windows\":\n",
    "            # Windows: st_ctime is actual creation time\n",
    "            creation_time = file_stats.st_ctime\n",
    "            creation_source = \"st_ctime (Windows creation time)\"\n",
    "            creation_reliable = True\n",
    "        elif hasattr(file_stats, \"st_birthtime\"):\n",
    "            # macOS/BSD: st_birthtime is true creation time\n",
    "            creation_time = file_stats.st_birthtime\n",
    "            creation_source = \"st_birthtime (macOS/BSD creation time)\"\n",
    "            creation_reliable = True\n",
    "        else:\n",
    "            # Linux/Other: No reliable creation time available\n",
    "            # Report st_ctime but mark as unreliable\n",
    "            creation_time = file_stats.st_ctime\n",
    "            creation_source = \"st_ctime (Linux - metadata change time, NOT creation)\"\n",
    "            creation_reliable = False\n",
    "\n",
    "        # Modification time is consistent across platforms\n",
    "        modification_time = file_stats.st_mtime\n",
    "\n",
    "        metadata = {\n",
    "            \"filename\": os.path.basename(file_path),\n",
    "            \"file_creation_date\": datetime.fromtimestamp(creation_time).strftime(\n",
    "                \"%Y-%m-%d %H:%M:%S\"\n",
    "            ),\n",
    "            \"file_modification_date\": datetime.fromtimestamp(\n",
    "                modification_time\n",
    "            ).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"file_location\": os.path.abspath(file_path),\n",
    "            \"platform\": system,\n",
    "            \"creation_source\": creation_source,\n",
    "            \"creation_reliable\": creation_reliable,  # Indicates if creation time is trustworthy\n",
    "        }\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting metadata for {file_path}: {e}\")\n",
    "        return {\n",
    "            \"filename\": os.path.basename(file_path) if file_path else \"Unknown\",\n",
    "            \"file_creation_date\": \"Unknown\",\n",
    "            \"file_modification_date\": \"Unknown\",\n",
    "            \"file_location\": (\n",
    "                os.path.abspath(file_path)\n",
    "                if file_path and os.path.exists(file_path)\n",
    "                else str(file_path)\n",
    "            ),\n",
    "            \"platform\": platform.system().lower(),\n",
    "            \"creation_source\": \"Error occurred\",\n",
    "            \"creation_reliable\": False,\n",
    "        }\n",
    "\n",
    "\n",
    "# Optional: Diagnostic function to show what timestamps are actually available\n",
    "def diagnose_file_timestamps(file_path):\n",
    "    \"\"\"Show all available timestamps for debugging\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        file_stats = os.stat(file_path)\n",
    "        system = platform.system().lower()\n",
    "\n",
    "        print(f\"\\nFile: {os.path.basename(file_path)}\")\n",
    "        print(f\"Platform: {system}\")\n",
    "        print(\"\\nAvailable timestamps:\")\n",
    "\n",
    "        if hasattr(file_stats, \"st_birthtime\"):\n",
    "            print(\n",
    "                f\"  st_birthtime: {datetime.fromtimestamp(file_stats.st_birthtime).strftime('%Y-%m-%d %H:%M:%S')} (creation time)\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"  st_birthtime: Not available\")\n",
    "\n",
    "        print(\n",
    "            f\"  st_ctime:     {datetime.fromtimestamp(file_stats.st_ctime).strftime('%Y-%m-%d %H:%M:%S')} ({'creation' if system == 'windows' else 'metadata change'} time)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  st_mtime:     {datetime.fromtimestamp(file_stats.st_mtime).strftime('%Y-%m-%d %H:%M:%S')} (modification time)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  st_atime:     {datetime.fromtimestamp(file_stats.st_atime).strftime('%Y-%m-%d %H:%M:%S')} (access time)\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading timestamps: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaace9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Function Definitions for Serial Number Extraction\n",
    "\"\"\"\n",
    "Function definitions for the Advanced Serial Number Detection System.\n",
    "This cell contains all helper functions used by the main processing pipeline for serial number extraction.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_file_metadata(file_path):\n",
    "    \"\"\"Extract file creation and modification timestamps.\"\"\"\n",
    "    try:\n",
    "        stat = os.stat(file_path)\n",
    "        modification_time = datetime.fromtimestamp(stat.st_mtime).strftime(\n",
    "            \"%m/%d/%y %H:%M\"\n",
    "        )\n",
    "        if platform.system() == \"Windows\":\n",
    "            creation_time = datetime.fromtimestamp(stat.st_ctime).strftime(\n",
    "                \"%m/%d/%y %H:%M\"\n",
    "            )\n",
    "        else:\n",
    "            try:\n",
    "                creation_time = datetime.fromtimestamp(stat.st_birthtime).strftime(\n",
    "                    \"%m/%d/%y %H:%M\"\n",
    "                )\n",
    "            except AttributeError:\n",
    "                creation_time = modification_time\n",
    "        return {\n",
    "            \"file_creation_date\": creation_time,\n",
    "            \"file_modification_date\": modification_time,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"file_creation_date\": \"Unknown\", \"file_modification_date\": \"Unknown\"}\n",
    "\n",
    "\n",
    "def combine_spaced_alphanumeric(text):\n",
    "    \"\"\"Combine spaced single characters into continuous alphanumeric strings.\"\"\"\n",
    "    parts = [part for part in text.split() if part]\n",
    "    if len(parts) >= 3:\n",
    "        single_chars = [\n",
    "            part\n",
    "            for part in parts\n",
    "            if len(part) == 1 and (part.isalpha() or part.isdigit())\n",
    "        ]\n",
    "        if len(single_chars) >= 2:\n",
    "            combined = \"\".join(parts)\n",
    "            if 5 <= len(combined) <= 20 and combined.isalnum():\n",
    "                return combined\n",
    "    return text\n",
    "\n",
    "\n",
    "def simple_clustering(text_positions, eps=150, min_samples=2):\n",
    "    \"\"\"Perform distance-based clustering of text positions without sklearn dependency.\"\"\"\n",
    "    if len(text_positions) < min_samples:\n",
    "        return [-1] * len(text_positions)\n",
    "\n",
    "    labels = [-1] * len(text_positions)\n",
    "    cluster_id = 0\n",
    "\n",
    "    for i, (x1, y1, _, _, _) in enumerate(text_positions):\n",
    "        if labels[i] != -1:\n",
    "            continue\n",
    "\n",
    "        cluster = [i]\n",
    "        labels[i] = cluster_id\n",
    "\n",
    "        for j, (x2, y2, _, _, _) in enumerate(text_positions):\n",
    "            if i == j or labels[j] != -1:\n",
    "                continue\n",
    "\n",
    "            if ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5 <= eps:\n",
    "                cluster.append(j)\n",
    "                labels[j] = cluster_id\n",
    "\n",
    "        if len(cluster) >= min_samples:\n",
    "            cluster_id += 1\n",
    "        else:\n",
    "            for idx in cluster:\n",
    "                labels[idx] = -1\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def detect_image_regions(processed_results, image_dimensions):\n",
    "    \"\"\"Identify and classify different regions within an image based on text clustering.\"\"\"\n",
    "    if len(processed_results) < 3:\n",
    "        return [{\"type\": \"unknown\", \"texts\": processed_results, \"region_id\": 0}]\n",
    "\n",
    "    text_positions = [\n",
    "        (\n",
    "            (bbox[0][0] + bbox[2][0]) / 2,\n",
    "            (bbox[0][1] + bbox[2][1]) / 2,\n",
    "            text,\n",
    "            bbox,\n",
    "            confidence,\n",
    "        )\n",
    "        for bbox, text, confidence in processed_results\n",
    "    ]\n",
    "\n",
    "    labels = simple_clustering(text_positions, eps=150, min_samples=2)\n",
    "\n",
    "    regions = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        regions.setdefault(label, []).append(processed_results[i])\n",
    "\n",
    "    classified_regions = []\n",
    "    for region_id, region_texts in regions.items():\n",
    "        region_type = (\n",
    "            \"isolated\" if region_id == -1 else classify_region_type(region_texts)\n",
    "        )\n",
    "        classified_regions.append(\n",
    "            {\n",
    "                \"type\": region_type,\n",
    "                \"texts\": region_texts,\n",
    "                \"region_id\": region_id if region_id != -1 else len(classified_regions),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return classified_regions\n",
    "\n",
    "\n",
    "def classify_region_type(region_texts):\n",
    "    \"\"\"Classify a region as document, firearm, or mixed based on content analysis.\"\"\"\n",
    "    all_text = \" \".join([text for _, text, _ in region_texts]).upper()\n",
    "    text_count = len(region_texts)\n",
    "\n",
    "    if region_texts:\n",
    "        xs = [bbox[0][0] for bbox, _, _ in region_texts] + [\n",
    "            bbox[2][0] for bbox, _, _ in region_texts\n",
    "        ]\n",
    "        ys = [bbox[0][1] for bbox, _, _ in region_texts] + [\n",
    "            bbox[2][1] for bbox, _, _ in region_texts\n",
    "        ]\n",
    "        area = (max(xs) - min(xs)) * (max(ys) - min(ys))\n",
    "        text_area = sum(\n",
    "            (bbox[2][0] - bbox[0][0]) * (bbox[2][1] - bbox[0][1])\n",
    "            for bbox, _, _ in region_texts\n",
    "        )\n",
    "        density = text_area / area if area > 0 else 0\n",
    "    else:\n",
    "        density = 0\n",
    "\n",
    "    doc_keywords = [\"LICENSE\", \"NAME\", \"ADDRESS\", \"DATE\", \"ISSUED\", \"EXPIRES\"]\n",
    "    firearm_keywords = [\"GLOCK\", \"SMITH\", \"COLT\", \"CAL\", \"MM\", \"MODEL\", \"MADE IN\"]\n",
    "\n",
    "    doc_score = (\n",
    "        (2 if density > 0.1 else 0)\n",
    "        + (2 if text_count > 8 else 0)\n",
    "        + sum(2 for kw in doc_keywords if kw in all_text)\n",
    "    )\n",
    "    firearm_score = (\n",
    "        (2 if density < 0.05 else 0)\n",
    "        + (2 if text_count < 6 else 0)\n",
    "        + sum(2 for kw in firearm_keywords if kw in all_text)\n",
    "    )\n",
    "\n",
    "    if doc_score > firearm_score + 2:\n",
    "        return \"document\"\n",
    "    elif firearm_score > doc_score + 2:\n",
    "        return \"firearm\"\n",
    "    else:\n",
    "        return \"mixed\"\n",
    "\n",
    "\n",
    "def analyze_region_context(region_texts, region_type):\n",
    "    \"\"\"Perform detailed context analysis for a classified region.\"\"\"\n",
    "    all_texts = [text for _, text, _ in region_texts]\n",
    "\n",
    "    if region_type == \"document\":\n",
    "        word_freq = {}\n",
    "        for text in all_texts:\n",
    "            for word in re.findall(r\"[A-Z]{3,}\", text.upper()):\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "        frequent_words = {word for word, count in word_freq.items() if count > 1}\n",
    "        document_labels = {\n",
    "            \"LICENSE\",\n",
    "            \"NAME\",\n",
    "            \"ADDRESS\",\n",
    "            \"DATE\",\n",
    "            \"DOB\",\n",
    "            \"EXPIRES\",\n",
    "            \"ISSUED\",\n",
    "            \"POBOX\",\n",
    "            \"PO\",\n",
    "            \"BOX\",\n",
    "        }\n",
    "\n",
    "        for text in all_texts:\n",
    "            text_upper = text.upper()\n",
    "            frequent_words.update(\n",
    "                label for label in document_labels if label in text_upper\n",
    "            )\n",
    "\n",
    "        y_groups = {}\n",
    "        for bbox, text, _ in region_texts:\n",
    "            y_bucket = int((bbox[0][1] + bbox[2][1]) / 100) * 50\n",
    "            y_groups.setdefault(y_bucket, []).append(text)\n",
    "\n",
    "        label_zones = {\n",
    "            y\n",
    "            for y, texts in y_groups.items()\n",
    "            if len(texts) > 2 and sum(len(t) for t in texts) / len(texts) < 15\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"type\": \"document\",\n",
    "            \"frequent_words\": frequent_words,\n",
    "            \"label_zones\": label_zones,\n",
    "        }\n",
    "\n",
    "    elif region_type == \"firearm\":\n",
    "        all_text = \" \".join(all_texts).upper()\n",
    "        manufacturers = [\"GLOCK\", \"SMITH\", \"COLT\", \"RUGER\", \"SIG\"]\n",
    "        manufacturer = next((mfg for mfg in manufacturers if mfg in all_text), None)\n",
    "\n",
    "        manufacturing_marks = set()\n",
    "        for text in all_texts:\n",
    "            text_upper = text.upper()\n",
    "            if (\n",
    "                re.search(r\"\\d+MM|CAL\", text_upper)\n",
    "                or any(\n",
    "                    country in text_upper for country in [\"USA\", \"AUSTRIA\", \"GERMANY\"]\n",
    "                )\n",
    "                or (\n",
    "                    manufacturer\n",
    "                    and text_upper.startswith(manufacturer[:4])\n",
    "                    and len(text) <= 8\n",
    "                )\n",
    "            ):\n",
    "                manufacturing_marks.add(text)\n",
    "\n",
    "        return {\n",
    "            \"type\": \"firearm\",\n",
    "            \"manufacturer\": manufacturer,\n",
    "            \"manufacturing_marks\": manufacturing_marks,\n",
    "        }\n",
    "\n",
    "    elif region_type == \"isolated\":\n",
    "        if len(region_texts) == 1:\n",
    "            text = region_texts[0][1]\n",
    "            if len(text) >= 5 and text.isalnum():\n",
    "                return {\n",
    "                    \"type\": \"firearm\",\n",
    "                    \"manufacturer\": None,\n",
    "                    \"manufacturing_marks\": set(),\n",
    "                }\n",
    "        return {\n",
    "            \"type\": \"mixed\",\n",
    "            \"frequent_words\": set(),\n",
    "            \"label_zones\": set(),\n",
    "            \"manufacturing_marks\": set(),\n",
    "        }\n",
    "\n",
    "    elif region_type == \"unknown\":\n",
    "        if (\n",
    "            len(region_texts) == 1\n",
    "            and len(region_texts[0][1]) >= 5\n",
    "            and region_texts[0][1].replace(\" \", \"\").isalnum()\n",
    "        ):\n",
    "            return {\n",
    "                \"type\": \"firearm\",\n",
    "                \"manufacturer\": None,\n",
    "                \"manufacturing_marks\": set(),\n",
    "            }\n",
    "        return {\n",
    "            \"type\": \"mixed\",\n",
    "            \"frequent_words\": set(),\n",
    "            \"label_zones\": set(),\n",
    "            \"manufacturing_marks\": set(),\n",
    "        }\n",
    "\n",
    "    else:  # mixed\n",
    "        return {\n",
    "            \"type\": \"mixed\",\n",
    "            \"frequent_words\": set(),\n",
    "            \"label_zones\": set(),\n",
    "            \"manufacturing_marks\": set(),\n",
    "        }\n",
    "\n",
    "\n",
    "def is_valid_serial_number(candidate):\n",
    "    \"\"\"Validate serial number format and detect potential OCR errors.\"\"\"\n",
    "    clean = candidate.replace(\"-\", \"\")\n",
    "\n",
    "    if (\n",
    "        not (5 <= len(clean) <= 12)\n",
    "        or candidate.count(\"-\") > 1\n",
    "        or not re.match(r\"^[A-Z0-9-]+$\", candidate)\n",
    "    ):\n",
    "        return False\n",
    "\n",
    "    has_letters = bool(re.search(r\"[A-Z]\", clean))\n",
    "    has_digits = bool(re.search(r\"[0-9]\", clean))\n",
    "\n",
    "    # Flag digits-only sequences as potential OCR errors\n",
    "    if not has_letters and has_digits:\n",
    "        return \"potential_ocr_error\"\n",
    "\n",
    "    return has_letters and has_digits\n",
    "\n",
    "\n",
    "def calculate_context_score(candidate, source_text, bbox, context):\n",
    "    \"\"\"Calculate context-aware confidence score for serial number candidates.\"\"\"\n",
    "    score = 0.5\n",
    "    candidate_upper = candidate.upper()\n",
    "    is_standalone = source_text.strip() == candidate\n",
    "\n",
    "    if context[\"type\"] == \"document\":\n",
    "        # Extreme Document Label Penalties\n",
    "        extreme_penalties = [\n",
    "            (\"POBOX\", -1.5),\n",
    "            (\"PO\", -1.5),\n",
    "            (r\"DOB\\d+\", -1.5),\n",
    "            (r\"LICENSE\\d+\", -1.4),\n",
    "            (r\"DATE\\d+\", -1.3),\n",
    "            (r\"EXPIRES\\d+\", -1.3),\n",
    "            (r\"ISSUED\\d+\", -1.2),\n",
    "        ]\n",
    "\n",
    "        for pattern, penalty in extreme_penalties:\n",
    "            if (\n",
    "                pattern.startswith(\"r\") and re.search(pattern[1:], candidate_upper)\n",
    "            ) or pattern in candidate_upper:\n",
    "                score += penalty\n",
    "                break\n",
    "        else:\n",
    "            # Strong Document Penalties\n",
    "            if any(\n",
    "                word in context[\"frequent_words\"]\n",
    "                for word in re.findall(r\"[A-Z]+\", candidate_upper)\n",
    "            ):\n",
    "                score -= 0.8\n",
    "            elif any(\n",
    "                addr in candidate_upper for addr in [\"ST\", \"AVE\", \"BLVD\", \"RD\", \"DR\"]\n",
    "            ):\n",
    "                score -= 0.8\n",
    "            elif any(name in candidate_upper for name in [\"MR\", \"MS\", \"DR\"]):\n",
    "                score -= 0.8\n",
    "\n",
    "        y_bucket = int((bbox[0][1] + bbox[2][1]) / 100) * 50\n",
    "        if y_bucket in context[\"label_zones\"]:\n",
    "            score -= 0.6\n",
    "\n",
    "        if is_standalone:\n",
    "            score += 0.2\n",
    "\n",
    "    elif context[\"type\"] in [\"firearm\", \"isolated\"]:\n",
    "        # Manufacturing penalties\n",
    "        if re.search(r\"\\d+MM|CAL\", candidate_upper):\n",
    "            score -= 0.5\n",
    "        elif candidate_upper in [\"USA\", \"AUSTRIA\", \"GERMANY\", \"ITALY\"]:\n",
    "            score -= 0.4\n",
    "        elif candidate in context.get(\"manufacturing_marks\", set()):\n",
    "            score -= 0.3\n",
    "\n",
    "        if is_standalone:\n",
    "            score += 0.8\n",
    "\n",
    "    else:  # mixed\n",
    "        if any(\n",
    "            label in candidate_upper\n",
    "            for label in [\"POBOX\", \"DOB\", \"DATE\", \"EXP\", \"ISSUED\", \"LIC\"]\n",
    "        ):\n",
    "            score += -1.0 if not is_standalone else -0.5\n",
    "\n",
    "        if is_standalone:\n",
    "            score += 0.4\n",
    "\n",
    "    # Common bonuses\n",
    "    if 6 <= len(candidate.replace(\"-\", \"\")) <= 10:\n",
    "        score += 0.2\n",
    "\n",
    "    letters = sum(1 for c in candidate if c.isalpha())\n",
    "    digits = sum(1 for c in candidate if c.isdigit())\n",
    "    if letters + digits > 0 and 0.3 <= letters / (letters + digits) <= 0.7:\n",
    "        score += 0.2\n",
    "\n",
    "    return max(0, min(2, score))\n",
    "\n",
    "\n",
    "def get_method_description(method):\n",
    "    \"\"\"Get human-readable description of pattern matching method.\"\"\"\n",
    "    descriptions = {\n",
    "        \"Pattern_0\": \"Mixed Letters+Digits 5-12 chars (Highest Priority)\",\n",
    "        \"Pattern_1\": \"Letters followed by Digits (High Priority)\",\n",
    "        \"Pattern_2\": \"Digits followed by Letters (Medium-High Priority)\",\n",
    "        \"Pattern_3\": \"Mixed alphanumeric with hyphen (Medium Priority)\",\n",
    "        \"Pattern_4\": \"Digits-only (potential OCR error) (Low Priority)\",\n",
    "    }\n",
    "    base_desc = descriptions.get(method.replace(\"Combined_\", \"\"), method)\n",
    "    return (\n",
    "        f\"Combined Text: {base_desc}\" if method.startswith(\"Combined_\") else base_desc\n",
    "    )\n",
    "\n",
    "\n",
    "def find_serial_number(\n",
    "    processed_results, all_text_combined, image_dimensions, debug=True\n",
    "):\n",
    "    \"\"\"Main serial number detection function with multi-region analysis.\"\"\"\n",
    "    regions = detect_image_regions(processed_results, image_dimensions)\n",
    "\n",
    "    if debug:\n",
    "        if len(regions) > 1:\n",
    "            print(\n",
    "                f\"  Image classified as: multi-region ({len(regions)} regions detected)\"\n",
    "            )\n",
    "            for i, region in enumerate(regions):\n",
    "                print(\n",
    "                    f\"    Region {i+1}: {region['type']} ({len(region['texts'])} texts)\"\n",
    "                )\n",
    "        else:\n",
    "            print(f\"  Image classified as: {regions[0]['type']}\")\n",
    "\n",
    "    patterns = [\n",
    "        r\"\\b[A-Z0-9]{5,12}\\b\",\n",
    "        r\"\\b[A-Z]{1,6}[0-9]{1,11}\\b\",\n",
    "        r\"\\b[0-9]{1,11}[A-Z]{1,6}\\b\",\n",
    "        r\"\\b[A-Z0-9]{2,6}-[A-Z0-9]{2,6}\\b\",\n",
    "        r\"\\b[0-9]{5,12}\\b\",\n",
    "    ]\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    # Process each region\n",
    "    for region in regions:\n",
    "        context = analyze_region_context(region[\"texts\"], region[\"type\"])\n",
    "\n",
    "        for pattern_idx, pattern in enumerate(patterns):\n",
    "            for bbox, text, confidence in region[\"texts\"]:\n",
    "                for match in re.findall(pattern, text):\n",
    "                    validation = is_valid_serial_number(match)\n",
    "\n",
    "                    if validation == True or validation == \"potential_ocr_error\":\n",
    "                        context_score = calculate_context_score(\n",
    "                            match, text, bbox, context\n",
    "                        )\n",
    "                        region_bonus = (\n",
    "                            0.3 if context[\"type\"] in [\"firearm\", \"isolated\"] else 0.0\n",
    "                        )\n",
    "\n",
    "                        score = (\n",
    "                            confidence * 0.2\n",
    "                            + context_score * 0.7\n",
    "                            + (len(patterns) - pattern_idx) * 0.02\n",
    "                            + region_bonus\n",
    "                        )\n",
    "\n",
    "                        # Determine status and serial number to report\n",
    "                        if validation == \"potential_ocr_error\":\n",
    "                            score *= 0.5\n",
    "                            status = \"Potential OCR Error\"\n",
    "                            reported_serial = f\"{match}_potential_error\"  # Add suffix for potential errors\n",
    "                        else:\n",
    "                            status = \"Valid\"\n",
    "                            reported_serial = match\n",
    "\n",
    "                        candidates.append(\n",
    "                            (\n",
    "                                reported_serial,\n",
    "                                score,\n",
    "                                f\"Pattern_{pattern_idx}\",\n",
    "                                f\"From OCR text: '{text}'\",\n",
    "                                confidence,\n",
    "                                pattern_idx + 1,\n",
    "                                text,\n",
    "                                status,\n",
    "                                context_score,\n",
    "                                region[\"region_id\"],\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "    # Process combined text\n",
    "    if all_text_combined:\n",
    "        for pattern_idx, pattern in enumerate(patterns):\n",
    "            for match in re.findall(pattern, all_text_combined):\n",
    "                validation = is_valid_serial_number(match)\n",
    "                if validation == True:\n",
    "                    score = 0.3 * 0.2 + 0.3 * 0.7 + (len(patterns) - pattern_idx) * 0.01\n",
    "                    candidates.append(\n",
    "                        (\n",
    "                            match,\n",
    "                            score,\n",
    "                            f\"Combined_Pattern_{pattern_idx}\",\n",
    "                            \"From combined text\",\n",
    "                            0.3,\n",
    "                            pattern_idx + 1,\n",
    "                            \"Combined OCR text\",\n",
    "                            \"Valid\",\n",
    "                            0.3,\n",
    "                            -1,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    # Remove duplicates, keeping highest score\n",
    "    unique_candidates = {}\n",
    "    for candidate in candidates:\n",
    "        serial = candidate[0]\n",
    "        if (\n",
    "            serial not in unique_candidates\n",
    "            or candidate[1] > unique_candidates[serial][1]\n",
    "        ):\n",
    "            unique_candidates[serial] = candidate\n",
    "\n",
    "    final_candidates = sorted(\n",
    "        unique_candidates.values(), key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    if debug and final_candidates:\n",
    "        print(f\"Serial number candidates (ALL {len(final_candidates)}):\")\n",
    "        for i, (\n",
    "            serial,\n",
    "            score,\n",
    "            method,\n",
    "            source,\n",
    "            conf,\n",
    "            priority,\n",
    "            source_text,\n",
    "            status,\n",
    "            context_score,\n",
    "            region_id,\n",
    "        ) in enumerate(final_candidates):\n",
    "            method_desc = get_method_description(method)\n",
    "            region_info = (\n",
    "                f\" [Region {region_id+1}]\" if region_id >= 0 else \" [Combined]\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  {i+1}. '{serial}' (score: {score:.3f}, method: {method}, priority: {priority}, conf: {conf:.3f}, status: {status}){region_info}\"\n",
    "            )\n",
    "            print(f\"      Method: {method_desc}\")\n",
    "            print(f\"      Source text: '{source_text}'\")\n",
    "\n",
    "    return final_candidates[0] if final_candidates else None, final_candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db32850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: Serial Number Extraction - Main Execution\n",
    "\"\"\"\n",
    "Main execution pipeline for serial number detection.\n",
    "This cell processes images using the functions defined in the previous cell.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from paddleocr import PaddleOCR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup OCR engine and file discovery\n",
    "test_folder = os.path.join(os.getcwd(), \"test\")\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang=\"en\", show_log=False)\n",
    "\n",
    "image_extensions = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".tif\"]\n",
    "file_list = [\n",
    "    f\n",
    "    for f in os.listdir(test_folder)\n",
    "    if any(f.lower().endswith(ext) for ext in image_extensions)\n",
    "]\n",
    "print(f\"Found {len(file_list)} image files in test folder\")\n",
    "\n",
    "# Process each file\n",
    "csv_results = []\n",
    "\n",
    "for filename in file_list:\n",
    "    file_path = os.path.join(test_folder, filename)\n",
    "\n",
    "    print(f\"\\nProcessing: {filename}\")\n",
    "\n",
    "    # Extract file metadata\n",
    "    file_metadata = get_file_metadata(file_path)\n",
    "\n",
    "    # Load and validate image\n",
    "    img = cv2.imread(file_path)\n",
    "    if img is None:\n",
    "        print(f\"Could not read image: {filename}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"  Dimensions: {img.shape[1]}x{img.shape[0]}\")\n",
    "\n",
    "    # Perform OCR\n",
    "    ocr_results = ocr.ocr(img, cls=True)\n",
    "    if not ocr_results or not ocr_results[0]:\n",
    "        print(f\"  No text detected\")\n",
    "        continue\n",
    "\n",
    "    # Process OCR results\n",
    "    processed_results = []\n",
    "    all_text = []\n",
    "\n",
    "    for line in ocr_results[0]:\n",
    "        bbox, (text, confidence) = line\n",
    "        text = combine_spaced_alphanumeric(text.strip())\n",
    "        processed_results.append((bbox, text, confidence))\n",
    "        all_text.append(text)\n",
    "\n",
    "    all_text_combined = \" \".join(all_text)\n",
    "    print(f\"  OCR extracted {len(all_text_combined)} characters\")\n",
    "\n",
    "    # Find serial numbers using multi-region analysis\n",
    "    selected_candidate, all_candidates = find_serial_number(\n",
    "        processed_results, all_text_combined, (img.shape[1], img.shape[0]), debug=True\n",
    "    )\n",
    "\n",
    "    candidate_serials = (\n",
    "        [candidate[0] for candidate in all_candidates] if all_candidates else []\n",
    "    )\n",
    "\n",
    "    # Display image with colored bounding boxes\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img_rgb)\n",
    "\n",
    "    for bbox, text, confidence in processed_results:\n",
    "        # Determine box color based on serial number detection\n",
    "        if (\n",
    "            selected_candidate\n",
    "            and selected_candidate[0].replace(\"_potential_error\", \"\") in text\n",
    "        ):\n",
    "            color, linewidth = \"red\", 3  # Best candidate\n",
    "        elif any(\n",
    "            candidate.replace(\"_potential_error\", \"\") in text\n",
    "            for candidate in candidate_serials\n",
    "        ):\n",
    "            color, linewidth = \"blue\", 2  # Other candidates\n",
    "        else:\n",
    "            color, linewidth = \"gray\", 1  # Regular text\n",
    "\n",
    "        # Draw bounding box\n",
    "        points = np.array(bbox, dtype=np.int32)\n",
    "        plt.plot(\n",
    "            [points[0][0], points[1][0], points[2][0], points[3][0], points[0][0]],\n",
    "            [points[0][1], points[1][1], points[2][1], points[3][1], points[0][1]],\n",
    "            color=color,\n",
    "            linewidth=linewidth,\n",
    "        )\n",
    "\n",
    "        # Add text label\n",
    "        plt.text(\n",
    "            points[0][0],\n",
    "            points[0][1] - 5,\n",
    "            f\"{text} ({confidence:.2f})\",\n",
    "            fontsize=8,\n",
    "            color=color,\n",
    "            weight=\"bold\",\n",
    "        )\n",
    "\n",
    "    plt.title(f\"Multi-Region Serial Number Detection: {filename}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Output results and store for CSV\n",
    "    if selected_candidate:\n",
    "        (\n",
    "            serial_number,\n",
    "            _,\n",
    "            method,\n",
    "            _,\n",
    "            _,\n",
    "            priority,\n",
    "            source_text,\n",
    "            status,\n",
    "            context_score,\n",
    "            region_id,\n",
    "        ) = selected_candidate\n",
    "        method_desc = get_method_description(method)\n",
    "\n",
    "        print(f\"Found 1 serial numbers\")\n",
    "        print(f\"Serial numbers: {serial_number}\")\n",
    "        print(f\"Method: {method} (Priority: {priority}) - {method_desc}\")\n",
    "        print(f\"Status: {status}\")\n",
    "        print(f\"Context Score: {context_score:.3f}\")\n",
    "        if region_id >= 0:\n",
    "            print(f\"Region: {region_id+1}\")\n",
    "        print(f\"Extracted from: '{source_text}'\")\n",
    "\n",
    "        # Store result for CSV output\n",
    "        csv_results.append(\n",
    "            [\n",
    "                serial_number,\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                filename,\n",
    "                file_metadata.get(\"file_creation_date\", \"Unknown\"),\n",
    "                file_metadata.get(\"file_modification_date\", \"Unknown\"),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Found 0 serial numbers\")\n",
    "\n",
    "# Display final summary\n",
    "print(f\"\\n📋 Processed {len(csv_results)} files with serial numbers\")\n",
    "\n",
    "# Display results in CSV format\n",
    "print(\"\\nEXTRACTION RESULTS (CSV FORMAT)\")\n",
    "print(\n",
    "    \"serial_number,event_type,event_date,associated_name,associated_address,source_file,file_created,file_modified\"\n",
    ")\n",
    "for row in csv_results:\n",
    "    print(\n",
    "        \",\".join([f'\"{str(item)}\"' if \",\" in str(item) else str(item) for item in row])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2dd44c0e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# CELL 6: Advanced Image Processing Pipeline for Document Analysis\n",
    "\n",
    "\"\"\"\n",
    "CELL 5: Advanced Image Processing Pipeline for Document Analysis\n",
    "==============================================================\n",
    "\n",
    "This cell implements a comprehensive OCR-based document processing system that:\n",
    "1. Processes multiple image formats (.jpg, .jpeg, .png, .bmp, .tiff, .tif)\n",
    "2. Extracts structured data including serial numbers, event types, names, dates, and addresses\n",
    "3. Uses contextual proximity analysis for improved field detection\n",
    "4. Provides visual feedback with color-coded bounding boxes\n",
    "5. Generates CSV output with extracted metadata\n",
    "\n",
    "Key Features:\n",
    "- Enhanced serial number detection with context-aware scoring\n",
    "- Multi-strategy text extraction and validation\n",
    "- Spatial relationship analysis between detected text elements\n",
    "- NER (Named Entity Recognition) for person and location identification\n",
    "- Comprehensive error handling and debugging capabilities\n",
    "\n",
    "Dependencies:\n",
    "- PaddleOCR: Primary OCR engine\n",
    "- spaCy: Natural Language Processing for NER\n",
    "- OpenCV: Image processing and visualization\n",
    "- pandas: Data manipulation and CSV output\n",
    "- matplotlib: Plotting and visualization\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from paddleocr import PaddleOCR\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import zipcodes\n",
    "import spacy\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================================\n",
    "# INITIALIZATION AND SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Load spaCy NER model for person and location detection\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Installing spaCy English model...\")\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Ensure DataFrame columns are properly typed for consistent processing\n",
    "df[\"filename\"] = df[\"filename\"].astype(str)\n",
    "df[\"file_creation_date\"] = df[\"file_creation_date\"].astype(str)\n",
    "df[\"file_modification_date\"] = df[\"file_modification_date\"].astype(str)\n",
    "df[\"file_location\"] = df[\"file_location\"].astype(str)\n",
    "\n",
    "# Initialize OCR engine with optimized settings\n",
    "test_folder = os.path.join(os.getcwd(), \"test\")\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang=\"en\", show_log=False)\n",
    "\n",
    "# Define supported image formats\n",
    "image_extensions = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".tif\"]\n",
    "\n",
    "# Scan for image files in the test directory\n",
    "file_list = [\n",
    "    f\n",
    "    for f in os.listdir(test_folder)\n",
    "    if any(f.lower().endswith(ext) for ext in image_extensions)\n",
    "]\n",
    "\n",
    "print(f\"Found {len(file_list)} image files in test folder\")\n",
    "\n",
    "# Initialize results storage for CSV export\n",
    "csv_results = []\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_file_metadata(file_path):\n",
    "    \"\"\"\n",
    "    Extract comprehensive file metadata including timestamps and size.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Full path to the file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing file creation date, modification date, and size\n",
    "        \n",
    "    Note:\n",
    "        Creation time handling is platform-dependent:\n",
    "        - Windows: Uses st_ctime\n",
    "        - macOS/Linux: Uses st_birthtime with fallback to modification time\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stat = os.stat(file_path)\n",
    "\n",
    "        # Get modification time in standardized format\n",
    "        modification_time = datetime.fromtimestamp(stat.st_mtime).strftime(\n",
    "            \"%m/%d/%y %H:%M\"\n",
    "        )\n",
    "\n",
    "        # Platform-specific creation time extraction\n",
    "        if platform.system() == \"Windows\":\n",
    "            creation_time = datetime.fromtimestamp(stat.st_ctime).strftime(\n",
    "                \"%m/%d/%y %H:%M\"\n",
    "            )\n",
    "        else:  # macOS and Linux\n",
    "            try:\n",
    "                creation_time = datetime.fromtimestamp(stat.st_birthtime).strftime(\n",
    "                    \"%m/%d/%y %H:%M\"\n",
    "                )\n",
    "            except AttributeError:\n",
    "                # Fallback for systems without birthtime support\n",
    "                creation_time = modification_time\n",
    "\n",
    "        return {\n",
    "            \"file_creation_date\": creation_time,\n",
    "            \"file_modification_date\": modification_time,\n",
    "            \"file_size\": stat.st_size,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting file metadata for {file_path}: {e}\")\n",
    "        return {\"file_creation_date\": \"\", \"file_modification_date\": \"\", \"file_size\": 0}\n",
    "\n",
    "\n",
    "def combine_spaced_alphanumeric(text):\n",
    "    \"\"\"\n",
    "    Reconstruct alphanumeric sequences that were incorrectly split by OCR.\n",
    "    \n",
    "    This function addresses common OCR issues where serial numbers or IDs\n",
    "    are detected as separate single characters instead of continuous strings.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text potentially containing spaced characters\n",
    "        \n",
    "    Returns:\n",
    "        str: Combined text if it forms a valid alphanumeric sequence, \n",
    "             otherwise returns original text\n",
    "             \n",
    "    Example:\n",
    "        \"A B C 1 2 3\" -> \"ABC123\" (if conditions are met)\n",
    "        \"Hello World\" -> \"Hello World\" (unchanged)\n",
    "    \"\"\"\n",
    "    parts = [part for part in text.split() if part]\n",
    "    \n",
    "    # Only process if we have multiple parts\n",
    "    if len(parts) >= 3:\n",
    "        # Count single character parts that are alphanumeric\n",
    "        single_chars = [\n",
    "            part\n",
    "            for part in parts\n",
    "            if len(part) == 1 and (part.isalpha() or part.isdigit())\n",
    "        ]\n",
    "        \n",
    "        # If we have enough single characters, try combining\n",
    "        if len(single_chars) >= 2:\n",
    "            combined = \"\".join(parts)\n",
    "            # Validate the combined result meets serial number criteria\n",
    "            if 5 <= len(combined) <= 20 and combined.isalnum():\n",
    "                return combined\n",
    "    return text\n",
    "\n",
    "\n",
    "def calculate_distance(bbox1, bbox2):\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance between centers of two bounding boxes.\n",
    "    \n",
    "    Used for spatial relationship analysis to determine which text elements\n",
    "    are likely related based on their proximity in the document.\n",
    "    \n",
    "    Args:\n",
    "        bbox1 (list): First bounding box as list of [x, y] coordinates\n",
    "        bbox2 (list): Second bounding box as list of [x, y] coordinates\n",
    "        \n",
    "    Returns:\n",
    "        float: Euclidean distance between bounding box centers\n",
    "    \"\"\"\n",
    "    # Calculate center points for both bounding boxes\n",
    "    center1_x = sum([point[0] for point in bbox1]) / len(bbox1)\n",
    "    center1_y = sum([point[1] for point in bbox1]) / len(bbox1)\n",
    "    center2_x = sum([point[0] for point in bbox2]) / len(bbox2)\n",
    "    center2_y = sum([point[1] for point in bbox2]) / len(bbox2)\n",
    "\n",
    "    # Return Euclidean distance\n",
    "    return ((center1_x - center2_x) ** 2 + (center1_y - center2_y) ** 2) ** 0.5\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SERIAL NUMBER DETECTION ENGINE\n",
    "# ============================================================================\n",
    "\n",
    "def find_serial(processed_results, all_text_combined, original_results, debug=True):\n",
    "    \"\"\"\n",
    "    Advanced serial number detection using multiple strategies and scoring.\n",
    "    \n",
    "    This function implements a sophisticated approach to serial number detection\n",
    "    that combines pattern matching, contextual analysis, and confidence scoring\n",
    "    to identify the most likely serial number from OCR results.\n",
    "    \n",
    "    Args:\n",
    "        processed_results (list): Processed OCR results as (bbox, text, confidence) tuples\n",
    "        all_text_combined (str): All detected text combined into a single string\n",
    "        original_results (list): Raw OCR results from PaddleOCR\n",
    "        debug (bool): Enable detailed debug output\n",
    "        \n",
    "    Returns:\n",
    "        str: Most likely serial number, or empty string if none found\n",
    "        \n",
    "    Strategies Used:\n",
    "        1. Pattern-based matching with multiple regex patterns\n",
    "        2. Contextual proximity analysis (near \"Serial\", \"S/N\", etc.)\n",
    "        3. Confidence-based scoring from OCR results\n",
    "        4. Length and character composition validation\n",
    "        5. Position-based scoring (preference for certain document areas)\n",
    "    \"\"\"\n",
    "    # Define serial number patterns in order of preference\n",
    "    # More specific patterns are tried first\n",
    "    patterns = [\n",
    "        r\"\\b[A-Z0-9]{7,12}\\b\",      # Mixed alphanumeric, 7-12 chars (most common)\n",
    "        r\"\\b[A-Z0-9]{4,20}\\b\",      # Mixed alphanumeric, broader range\n",
    "        r\"\\b[A-Z]{1,2}[0-9]{4,10}\\b\", # Letter prefix + numbers\n",
    "        r\"\\b[0-9]{4,15}\\b\",         # Numeric only, medium length\n",
    "        r\"\\b[A-Z]{2,10}[0-9]{2,10}\\b\", # Letters + numbers\n",
    "        r\"\\b[A-Z]{3,15}\\b\",         # Letters only, longer sequences\n",
    "        r\"\\b[0-9]{5,15}\\b\",         # Numeric only, longer sequences\n",
    "    ]\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    # ========================================================================\n",
    "    # STRATEGY 1: Pattern matching on processed results\n",
    "    # ========================================================================\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        for bbox, text, confidence in processed_results:\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                candidate = match.group()\n",
    "                \n",
    "                # Calculate base score based on pattern specificity\n",
    "                base_score = 100 - (pattern_idx * 10)  # Earlier patterns get higher scores\n",
    "                \n",
    "                # Confidence bonus (OCR confidence affects final score)\n",
    "                confidence_bonus = confidence * 20\n",
    "                \n",
    "                # Length bonus (prefer certain lengths for serial numbers)\n",
    "                length_bonus = 0\n",
    "                if 6 <= len(candidate) <= 12:\n",
    "                    length_bonus = 15\n",
    "                elif 4 <= len(candidate) <= 15:\n",
    "                    length_bonus = 10\n",
    "                \n",
    "                total_score = base_score + confidence_bonus + length_bonus\n",
    "                \n",
    "                candidates.append({\n",
    "                    'text': candidate,\n",
    "                    'score': total_score,\n",
    "                    'source': f'processed_pattern_{pattern_idx}',\n",
    "                    'bbox': bbox,\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "\n",
    "    # ========================================================================\n",
    "    # STRATEGY 2: Pattern matching on combined text\n",
    "    # ========================================================================\n",
    "    for pattern_idx, pattern in enumerate(patterns):\n",
    "        matches = re.finditer(pattern, all_text_combined)\n",
    "        for match in matches:\n",
    "            candidate = match.group()\n",
    "            \n",
    "            # Base score slightly lower than processed results\n",
    "            base_score = 90 - (pattern_idx * 10)\n",
    "            \n",
    "            # Length bonus\n",
    "            length_bonus = 0\n",
    "            if 6 <= len(candidate) <= 12:\n",
    "                length_bonus = 15\n",
    "            elif 4 <= len(candidate) <= 15:\n",
    "                length_bonus = 10\n",
    "            \n",
    "            total_score = base_score + length_bonus\n",
    "            \n",
    "            candidates.append({\n",
    "                'text': candidate,\n",
    "                'score': total_score,\n",
    "                'source': f'combined_pattern_{pattern_idx}',\n",
    "                'bbox': None,\n",
    "                'confidence': 0.8  # Default confidence for combined text\n",
    "            })\n",
    "\n",
    "    # ========================================================================\n",
    "    # STRATEGY 3: Contextual proximity analysis\n",
    "    # ========================================================================\n",
    "    # Look for text near serial number indicators\n",
    "    serial_indicators = ['serial', 's/n', 'sn', 'serial number', 'serial no']\n",
    "    \n",
    "    for bbox, text, confidence in processed_results:\n",
    "        text_lower = text.lower().strip()\n",
    "        \n",
    "        # Check if this text contains a serial indicator\n",
    "        if any(indicator in text_lower for indicator in serial_indicators):\n",
    "            # Look for nearby text that could be the actual serial number\n",
    "            for other_bbox, other_text, other_conf in processed_results:\n",
    "                if other_bbox != bbox:  # Don't compare with self\n",
    "                    distance = calculate_distance(bbox, other_bbox)\n",
    "                    \n",
    "                    # If nearby (within reasonable distance)\n",
    "                    if distance < 200:  # Adjust threshold as needed\n",
    "                        # Check if the nearby text matches serial patterns\n",
    "                        for pattern_idx, pattern in enumerate(patterns):\n",
    "                            if re.match(pattern, other_text.strip()):\n",
    "                                # High score for contextually relevant serials\n",
    "                                context_score = 120 - (pattern_idx * 5)\n",
    "                                proximity_bonus = max(0, 50 - (distance / 4))\n",
    "                                confidence_bonus = other_conf * 15\n",
    "                                \n",
    "                                total_score = context_score + proximity_bonus + confidence_bonus\n",
    "                                \n",
    "                                candidates.append({\n",
    "                                    'text': other_text.strip(),\n",
    "                                    'score': total_score,\n",
    "                                    'source': f'contextual_pattern_{pattern_idx}',\n",
    "                                    'bbox': other_bbox,\n",
    "                                    'confidence': other_conf,\n",
    "                                    'context_distance': distance\n",
    "                                })\n",
    "\n",
    "    # ========================================================================\n",
    "    # CANDIDATE FILTERING AND SELECTION\n",
    "    # ========================================================================\n",
    "    \n",
    "    if not candidates:\n",
    "        if debug:\n",
    "            print(\"No serial number candidates found\")\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove duplicates while keeping highest scoring version\n",
    "    unique_candidates = {}\n",
    "    for candidate in candidates:\n",
    "        text = candidate['text']\n",
    "        if text not in unique_candidates or candidate['score'] > unique_candidates[text]['score']:\n",
    "            unique_candidates[text] = candidate\n",
    "    \n",
    "    # Convert back to list and sort by score\n",
    "    final_candidates = list(unique_candidates.values())\n",
    "    final_candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Debug output\n",
    "    if debug and final_candidates:\n",
    "        print(f\"\\nSerial number candidates (top 5):\")\n",
    "        for i, candidate in enumerate(final_candidates[:5]):\n",
    "            print(f\"  {i+1}. '{candidate['text']}' - Score: {candidate['score']:.1f} \"\n",
    "                  f\"(Source: {candidate['source']}, Confidence: {candidate['confidence']:.2f})\")\n",
    "    \n",
    "    # Return the highest scoring candidate\n",
    "    return final_candidates[0]['text'] if final_candidates else \"\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TEXT CLASSIFICATION SYSTEM\n",
    "# ============================================================================\n",
    "\n",
    "def classify_token(text_lower):\n",
    "    \"\"\"\n",
    "    Classify text tokens into categories for visualization and processing.\n",
    "    \n",
    "    This lightweight classification system helps identify different types\n",
    "    of content in documents for color-coded visualization and improved\n",
    "    field extraction accuracy.\n",
    "    \n",
    "    Args:\n",
    "        text_lower (str): Lowercase text to classify\n",
    "        \n",
    "    Returns:\n",
    "        str: Classification category ('event', 'name', 'address', 'date', 'other')\n",
    "        \n",
    "    Categories:\n",
    "        - event: Event type indicators (inventory, missing, theft, etc.)\n",
    "        - name: Person name indicators (officer, reported, by, etc.)\n",
    "        - address: Address components (street, ave, rd, etc.)\n",
    "        - date: Date-related text (month names, date patterns)\n",
    "        - other: Default category for unclassified text\n",
    "    \"\"\"\n",
    "    # Event type keywords - indicators of incident/event classification\n",
    "    event_keywords = [\n",
    "        \"inventory\", \"missing\", \"theft\", \"stolen\", \"lost\", \"found\",\n",
    "        \"incident\", \"report\", \"case\", \"event\", \"occurrence\"\n",
    "    ]\n",
    "    \n",
    "    # Name/person indicators - suggest associated person information\n",
    "    name_keywords = [\n",
    "        \"officer\", \"reported\", \"by\", \"name\", \"person\", \"individual\",\n",
    "        \"witness\", \"complainant\", \"victim\", \"suspect\"\n",
    "    ]\n",
    "    \n",
    "    # Address components - geographical/location indicators\n",
    "    address_keywords = [\n",
    "        \"street\", \"st\", \"avenue\", \"ave\", \"road\", \"rd\", \"drive\", \"dr\",\n",
    "        \"lane\", \"ln\", \"court\", \"ct\", \"place\", \"pl\", \"way\", \"blvd\",\n",
    "        \"boulevard\", \"circle\", \"cir\", \"apt\", \"apartment\", \"suite\", \"unit\"\n",
    "    ]\n",
    "    \n",
    "    # Date indicators - temporal information\n",
    "    date_keywords = [\n",
    "        \"january\", \"february\", \"march\", \"april\", \"may\", \"june\",\n",
    "        \"july\", \"august\", \"september\", \"october\", \"november\", \"december\",\n",
    "        \"jan\", \"feb\", \"mar\", \"apr\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\",\n",
    "        \"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\",\n",
    "        \"date\", \"time\", \"am\", \"pm\"\n",
    "    ]\n",
    "    \n",
    "    # Classify based on keyword matching\n",
    "    if any(keyword in text_lower for keyword in event_keywords):\n",
    "        return \"event\"\n",
    "    elif any(keyword in text_lower for keyword in name_keywords):\n",
    "        return \"name\"\n",
    "    elif any(keyword in text_lower for keyword in address_keywords):\n",
    "        return \"address\"\n",
    "    elif any(keyword in text_lower for keyword in date_keywords):\n",
    "        return \"date\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PROCESSING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "# Color scheme for visualization - maps categories to RGB colors\n",
    "CATEGORY_COLORS = {\n",
    "    \"serial\": (0, 255, 0),      # Green - Serial numbers\n",
    "    \"event\": (255, 0, 0),       # Red - Event types\n",
    "    \"name\": (0, 0, 255),        # Blue - Person names\n",
    "    \"address\": (255, 165, 0),   # Orange - Addresses\n",
    "    \"date\": (128, 0, 128),      # Purple - Dates\n",
    "    \"other\": (128, 128, 128)    # Gray - Other text\n",
    "}\n",
    "\n",
    "# Process each image file in the test folder\n",
    "for filename in file_list:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {filename}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Construct full file path\n",
    "    file_path = os.path.join(test_folder, filename)\n",
    "    \n",
    "    try:\n",
    "        # ====================================================================\n",
    "        # IMAGE LOADING AND PREPROCESSING\n",
    "        # ====================================================================\n",
    "        \n",
    "        # Load image using OpenCV\n",
    "        image = cv2.imread(file_path)\n",
    "        if image is None:\n",
    "            print(f\"Error: Could not load image {filename}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Image dimensions: {image.shape}\")\n",
    "        \n",
    "        # Get file metadata for record keeping\n",
    "        metadata = get_file_metadata(file_path)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # OCR PROCESSING\n",
    "        # ====================================================================\n",
    "        \n",
    "        print(\"Running OCR analysis...\")\n",
    "        \n",
    "        # Execute OCR on the image\n",
    "        ocr_results = ocr.ocr(image, cls=True)\n",
    "        \n",
    "        if not ocr_results or not ocr_results[0]:\n",
    "            print(f\"No text detected in {filename}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract and process OCR results\n",
    "        results = ocr_results[0]\n",
    "        processed_results = []\n",
    "        all_text_parts = []\n",
    "        \n",
    "        print(f\"OCR detected {len(results)} text elements\")\n",
    "        \n",
    "        # Process each detected text element\n",
    "        for bbox, (text, confidence) in results:\n",
    "            if confidence > 0.5:  # Filter low-confidence detections\n",
    "                # Clean and process the text\n",
    "                cleaned_text = text.strip()\n",
    "                \n",
    "                # Apply spaced character combination for potential serial numbers\n",
    "                combined_text = combine_spaced_alphanumeric(cleaned_text)\n",
    "                \n",
    "                processed_results.append((bbox, combined_text, confidence))\n",
    "                all_text_parts.append(combined_text)\n",
    "                \n",
    "                print(f\"  Text: '{cleaned_text}' -> '{combined_text}' (Confidence: {confidence:.2f})\")\n",
    "        \n",
    "        # Combine all detected text for comprehensive analysis\n",
    "        all_text_combined = \" \".join(all_text_parts)\n",
    "        print(f\"\\nCombined text: {all_text_combined}\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # FIELD EXTRACTION\n",
    "        # ====================================================================\n",
    "        \n",
    "        print(\"\\nExtracting structured data...\")\n",
    "        \n",
    "        # Extract serial number using advanced detection\n",
    "        serial_number = find_serial(processed_results, all_text_combined, results, debug=True)\n",
    "        print(f\"Serial Number: '{serial_number}'\")\n",
    "        \n",
    "        # Initialize extraction results\n",
    "        extracted_data = {\n",
    "            'serial_number': serial_number,\n",
    "            'event_type': '',\n",
    "            'associated_name': '',\n",
    "            'event_date': '',\n",
    "            'associated_address': ''\n",
    "        }\n",
    "        \n",
    "        # ====================================================================\n",
    "        # EVENT TYPE EXTRACTION\n",
    "        # ====================================================================\n",
    "        \n",
    "        # Look for event type indicators in the text\n",
    "        event_patterns = [\n",
    "            r'\\b(?:missing|theft|stolen|lost|found|inventory)\\b',\n",
    "            r'\\b(?:incident|report|case|event|occurrence)\\b'\n",
    "        ]\n",
    "        \n",
    "        for pattern in event_patterns:\n",
    "            matches = re.finditer(pattern, all_text_combined, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                extracted_data['event_type'] = match.group()\n",
    "                break\n",
    "            if extracted_data['event_type']:\n",
    "                break\n",
    "        \n",
    "        print(f\"Event Type: '{extracted_data['event_type']}'\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # NAMED ENTITY RECOGNITION (NER)\n",
    "        # ====================================================================\n",
    "        \n",
    "        # Use spaCy NER to identify persons and locations\n",
    "        doc = nlp(all_text_combined)\n",
    "        \n",
    "        persons = []\n",
    "        locations = []\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                persons.append(ent.text)\n",
    "            elif ent.label_ in [\"GPE\", \"LOC\"]:  # Geopolitical entities and locations\n",
    "                locations.append(ent.text)\n",
    "        \n",
    "        # Select most likely associated name (exclude generic terms)\n",
    "        if persons:\n",
    "            # Filter out common non-name terms that might be misclassified\n",
    "            filtered_persons = [p for p in persons if p.lower() not in ['missing', 'inventory', 'report']]\n",
    "            if filtered_persons:\n",
    "                extracted_data['associated_name'] = filtered_persons[0]\n",
    "        \n",
    "        print(f\"Associated Name: '{extracted_data['associated_name']}'\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # DATE EXTRACTION\n",
    "        # ====================================================================\n",
    "        \n",
    "        # Look for date patterns in the text\n",
    "        date_patterns = [\n",
    "            r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b',  # MM/DD/YYYY or MM-DD-YYYY\n",
    "            r'\\b\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{2,4}\\b',\n",
    "            r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},?\\s+\\d{2,4}\\b'\n",
    "        ]\n",
    "        \n",
    "        for pattern in date_patterns:\n",
    "            match = re.search(pattern, all_text_combined, re.IGNORECASE)\n",
    "            if match:\n",
    "                extracted_data['event_date'] = match.group()\n",
    "                break\n",
    "        \n",
    "        print(f\"Event Date: '{extracted_data['event_date']}'\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # ADDRESS EXTRACTION\n",
    "        # ====================================================================\n",
    "        \n",
    "        # Use NER locations as potential addresses\n",
    "        if locations:\n",
    "            extracted_data['associated_address'] = locations[0]\n",
    "        \n",
    "        # Also look for address patterns\n",
    "        address_patterns = [\n",
    "            r'\\b\\d+\\s+[A-Za-z\\s]+(?:Street|St|Avenue|Ave|Road|Rd|Drive|Dr|Lane|Ln|Court|Ct|Place|Pl|Way|Boulevard|Blvd)\\b'\n",
    "        ]\n",
    "        \n",
    "        for pattern in address_patterns:\n",
    "            match = re.search(pattern, all_text_combined, re.IGNORECASE)\n",
    "            if match:\n",
    "                extracted_data['associated_address'] = match.group()\n",
    "                break\n",
    "        \n",
    "        print(f\"Associated Address: '{extracted_data['associated_address']}'\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # VISUALIZATION\n",
    "        # ====================================================================\n",
    "        \n",
    "        print(\"\\nGenerating visualization...\")\n",
    "        \n",
    "        # Create a copy of the image for visualization\n",
    "        vis_image = image.copy()\n",
    "        \n",
    "        # Draw bounding boxes with color coding\n",
    "        for bbox, text, confidence in processed_results:\n",
    "            # Determine color based on content classification\n",
    "            t_lower = text.lower()\n",
    "            \n",
    "            # Check for specific field matches first\n",
    "            if text == serial_number:\n",
    "                color_key = \"serial\"\n",
    "            elif text in extracted_data['associated_name']:\n",
    "                color_key = \"name\"\n",
    "            elif text in extracted_data['associated_address']:\n",
    "                color_key = \"address\"\n",
    "            elif text in extracted_data['event_date']:\n",
    "                color_key = \"date\"\n",
    "            elif text in extracted_data['event_type']:\n",
    "                color_key = \"event\"\n",
    "            else:\n",
    "                # Use classification function for general categorization\n",
    "                color_key = classify_token(t_lower)\n",
    "            \n",
    "            # Get color for the category\n",
    "            color = CATEGORY_COLORS.get(color_key, CATEGORY_COLORS[\"other\"])\n",
    "            \n",
    "            # Draw bounding box\n",
    "            pts = np.array(bbox, np.int32).reshape((-1, 1, 2))\n",
    "            cv2.polylines(vis_image, [pts], True, color, 2)\n",
    "            \n",
    "            # Add text label\n",
    "            cv2.putText(vis_image, f\"{text} ({confidence:.2f})\", \n",
    "                       (int(bbox[0][0]), int(bbox[0][1]) - 10),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "        \n",
    "        # Display the visualization\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.imshow(cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"OCR Results for {filename}\\n\"\n",
    "                 f\"Serial: {serial_number} | Event: {extracted_data['event_type']} | \"\n",
    "                 f\"Name: {extracted_data['associated_name']}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Add legend\n",
    "        legend_elements = []\n",
    "        for category, color in CATEGORY_COLORS.items():\n",
    "            legend_elements.append(plt.Rectangle((0,0),1,1, facecolor=[c/255 for c in color], \n",
    "                                               edgecolor='black', label=category.title()))\n",
    "        plt.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1, 1))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # ====================================================================\n",
    "        # RECORD RESULTS\n",
    "        # ====================================================================\n",
    "        \n",
    "        # Prepare CSV record\n",
    "        csv_record = {\n",
    "            'filename': filename,\n",
    "            'file_creation_date': metadata['file_creation_date'],\n",
    "            'file_modification_date': metadata['file_modification_date'],\n",
    "            'file_location': file_path,\n",
    "            'serial_number': extracted_data['serial_number'],\n",
    "            'event_type': extracted_data['event_type'],\n",
    "            'associated_name': extracted_data['associated_name'],\n",
    "            'event_date': extracted_data['event_date'],\n",
    "            'associated_address': extracted_data['associated_address'],\n",
    "            'processing_timestamp': datetime.now().strftime(\"%m/%d/%y %H:%M\")\n",
    "        }\n",
    "        \n",
    "        csv_results.append(csv_record)\n",
    "        \n",
    "        print(f\"\\nProcessing completed for {filename}\")\n",
    "        print(f\"Results recorded: {len(csv_results)} files processed so far\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL OUTPUT GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PROCESSING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Successfully processed {len(csv_results)} files\")\n",
    "\n",
    "if csv_results:\n",
    "    # Create DataFrame from results\n",
    "    results_df = pd.DataFrame(csv_results)\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nExtraction Summary:\")\n",
    "    print(f\"- Files with serial numbers: {sum(1 for r in csv_results if r['serial_number'])}\")\n",
    "    print(f\"- Files with event types: {sum(1 for r in csv_results if r['event_type'])}\")\n",
    "    print(f\"- Files with associated names: {sum(1 for r in csv_results if r['associated_name'])}\")\n",
    "    print(f\"- Files with dates: {sum(1 for r in csv_results if r['event_date'])}\")\n",
    "    print(f\"- Files with addresses: {sum(1 for r in csv_results if r['associated_address'])}\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = \"extracted_data_detailed.csv\"\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nResults saved to: {output_file}\")\n",
    "    \n",
    "    # Display first few results\n",
    "    print(f\"\\nSample Results:\")\n",
    "    print(results_df.head().to_string(index=False))\n",
    "else:\n",
    "    print(\"No files were successfully processed.\")\n",
    "\n",
    "print(f\"\\nProcessing pipeline completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45f55189",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# CELL 6.1: Initialization and Function Definitions\n",
    "\n",
    "\"\"\"\n",
    "This cell initializes the environment and defines all the functions needed for field extraction.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from paddleocr import PaddleOCR\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import platform\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "\n",
    "# Initialize OCR engine\n",
    "try:\n",
    "    ocr\n",
    "except NameError:\n",
    "    ocr = PaddleOCR(use_angle_cls=True, lang=\"en\", show_log=False)\n",
    "\n",
    "# Load spaCy NER model\n",
    "try:\n",
    "    nlp\n",
    "except NameError:\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        print(\"Installing spaCy English model...\")\n",
    "        os.system(\"python -m spacy download en_core_web_sm\")\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Setup test folder and file list\n",
    "test_folder = os.path.join(os.getcwd(), \"test\")\n",
    "if not os.path.exists(test_folder):\n",
    "    os.makedirs(test_folder)\n",
    "    print(f\"Created test folder: {test_folder}\")\n",
    "\n",
    "# Define image extensions and get file list\n",
    "image_extensions = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".tif\"]\n",
    "file_list = [\n",
    "    f for f in os.listdir(test_folder)\n",
    "    if any(f.lower().endswith(ext) for ext in image_extensions)\n",
    "]\n",
    "\n",
    "print(f\"Found {len(file_list)} image files in test folder\")\n",
    "\n",
    "# Color scheme for visualization\n",
    "CATEGORY_COLORS = {\n",
    "    \"serial_number\": (255, 0, 0),      # Red - Serial numbers\n",
    "    \"event_type\": (255, 140, 0),       # Dark Orange - Event types\n",
    "    \"associated_name\": (30, 144, 255), # Dodger Blue - Person names\n",
    "    \"event_date\": (255, 215, 0),       # Gold - Dates\n",
    "    \"associated_address\": (148, 0, 211), # Dark Violet - Addresses\n",
    "    \"other\": (128, 128, 128)           # Gray - Other text\n",
    "}\n",
    "\n",
    "# Utility functions\n",
    "def get_file_metadata(file_path):\n",
    "    \"\"\"Get file metadata including creation and modification times\"\"\"\n",
    "    try:\n",
    "        stat = os.stat(file_path)\n",
    "        modification_time = datetime.fromtimestamp(stat.st_mtime).strftime(\"%m/%d/%y %H:%M\")\n",
    "        \n",
    "        if platform.system() == \"Windows\":\n",
    "            creation_time = datetime.fromtimestamp(stat.st_ctime).strftime(\"%m/%d/%y %H:%M\")\n",
    "        else:  # macOS and Linux\n",
    "            try:\n",
    "                creation_time = datetime.fromtimestamp(stat.st_birthtime).strftime(\"%m/%d/%y %H:%M\")\n",
    "            except AttributeError:\n",
    "                creation_time = modification_time\n",
    "\n",
    "        return {\n",
    "            \"file_creation_date\": creation_time,\n",
    "            \"file_modification_date\": modification_time,\n",
    "            \"file_size\": stat.st_size,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting file metadata for {file_path}: {e}\")\n",
    "        return {\"file_creation_date\": \"\", \"file_modification_date\": \"\", \"file_size\": 0}\n",
    "\n",
    "def calculate_distance(bbox1, bbox2):\n",
    "    \"\"\"Calculate distance between two bounding boxes\"\"\"\n",
    "    center1_x = sum([point[0] for point in bbox1]) / len(bbox1)\n",
    "    center1_y = sum([point[1] for point in bbox1]) / len(bbox1)\n",
    "    center2_x = sum([point[0] for point in bbox2]) / len(bbox2)\n",
    "    center2_y = sum([point[1] for point in bbox2]) / len(bbox2)\n",
    "    return ((center1_x - center2_x) ** 2 + (center1_y - center2_y) ** 2) ** 0.5\n",
    "\n",
    "def combine_spaced_alphanumeric(text):\n",
    "    \"\"\"Combine spaced alphanumeric characters that form a sequence\"\"\"\n",
    "    parts = [part for part in text.split() if part]\n",
    "    if len(parts) >= 3:\n",
    "        single_chars = [\n",
    "            part for part in parts\n",
    "            if len(part) == 1 and (part.isalpha() or part.isdigit())\n",
    "        ]\n",
    "        if len(single_chars) >= 2:\n",
    "            combined = \"\".join(parts)\n",
    "            if 5 <= len(combined) <= 20 and combined.isalnum():\n",
    "                return combined\n",
    "    return text\n",
    "\n",
    "def classify_token(text_lower):\n",
    "    \"\"\"Classify text tokens into categories for visualization\"\"\"\n",
    "    # Event type keywords\n",
    "    event_keywords = [\n",
    "        \"inventory\", \"missing\", \"theft\", \"stolen\", \"lost\", \"found\",\n",
    "        \"incident\", \"report\", \"case\", \"event\", \"occurrence\", \"inspection\"\n",
    "    ]\n",
    "    \n",
    "    # Name/person indicators\n",
    "    name_keywords = [\n",
    "        \"officer\", \"reported\", \"by\", \"name\", \"person\", \"individual\",\n",
    "        \"witness\", \"complainant\", \"victim\", \"suspect\"\n",
    "    ]\n",
    "    \n",
    "    # Address components\n",
    "    address_keywords = [\n",
    "        \"street\", \"st\", \"avenue\", \"ave\", \"road\", \"rd\", \"drive\", \"dr\",\n",
    "        \"lane\", \"ln\", \"court\", \"ct\", \"place\", \"pl\", \"way\", \"blvd\",\n",
    "        \"boulevard\", \"circle\", \"cir\", \"apt\", \"apartment\", \"suite\", \"unit\"\n",
    "    ]\n",
    "    \n",
    "    # Date indicators\n",
    "    date_keywords = [\n",
    "        \"january\", \"february\", \"march\", \"april\", \"may\", \"june\",\n",
    "        \"july\", \"august\", \"september\", \"october\", \"november\", \"december\",\n",
    "        \"jan\", \"feb\", \"mar\", \"apr\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\",\n",
    "        \"date\", \"time\", \"am\", \"pm\"\n",
    "    ]\n",
    "    \n",
    "    # Classify based on keyword matching\n",
    "    if any(keyword in text_lower for keyword in event_keywords):\n",
    "        return \"event_type\"\n",
    "    elif any(keyword in text_lower for keyword in name_keywords):\n",
    "        return \"associated_name\"\n",
    "    elif any(keyword in text_lower for keyword in address_keywords):\n",
    "        return \"associated_address\"\n",
    "    elif any(keyword in text_lower for keyword in date_keywords):\n",
    "        return \"event_date\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "# Helper function to process a single image for testing\n",
    "def process_image_ocr(file_path):\n",
    "    \"\"\"Process a single image with OCR and return the results\"\"\"\n",
    "    try:\n",
    "        # Load image using OpenCV\n",
    "        image = cv2.imread(file_path)\n",
    "        if image is None:\n",
    "            print(f\"Error: Could not load image {file_path}\")\n",
    "            return None, None, None\n",
    "            \n",
    "        print(f\"Image dimensions: {image.shape}\")\n",
    "        \n",
    "        # Execute OCR on the image\n",
    "        ocr_results = ocr.ocr(image, cls=True)\n",
    "        \n",
    "        if not ocr_results or not ocr_results[0]:\n",
    "            print(f\"No text detected in {file_path}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # Extract and process OCR results\n",
    "        results = ocr_results[0]\n",
    "        processed_results = []\n",
    "        all_text_parts = []\n",
    "        \n",
    "        print(f\"OCR detected {len(results)} text elements\")\n",
    "        \n",
    "        # Process each detected text element\n",
    "        for bbox, (text, confidence) in results:\n",
    "            if confidence > 0.5:  # Filter low-confidence detections\n",
    "                # Clean and process the text\n",
    "                cleaned_text = text.strip()\n",
    "                \n",
    "                # Apply spaced character combination for potential serial numbers\n",
    "                combined_text = combine_spaced_alphanumeric(cleaned_text)\n",
    "                \n",
    "                processed_results.append((bbox, combined_text, confidence))\n",
    "                all_text_parts.append(combined_text)\n",
    "                \n",
    "                print(f\"  Text: '{cleaned_text}' -> '{combined_text}' (Confidence: {confidence:.2f})\")\n",
    "        \n",
    "        # Combine all detected text for comprehensive analysis\n",
    "        all_text_combined = \" \".join(all_text_parts)\n",
    "        print(f\"\\nCombined text: {all_text_combined}\")\n",
    "        \n",
    "        return image, processed_results, all_text_combined\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "# Field extraction function declarations - implementations in separate cells\n",
    "def find_serial_number(processed_results, all_text_combined, image_dimensions, debug=True):\n",
    "    \"\"\"Main serial number detection function with multi-region analysis.\"\"\"\n",
    "    # Implementation in Cell 6.2\n",
    "    pass\n",
    "\n",
    "def find_event_type(processed_results, all_text_combined, debug=True):\n",
    "    \"\"\"Extract event type using pattern matching and contextual analysis.\"\"\"\n",
    "    # Implementation in Cell 6.3\n",
    "    pass\n",
    "\n",
    "def find_associated_name(processed_results, all_text_combined, debug=True):\n",
    "    \"\"\"Extract associated name using NER and contextual analysis.\"\"\"\n",
    "    # Implementation in Cell 6.4\n",
    "    pass\n",
    "\n",
    "def find_event_date(processed_results, all_text_combined, debug=True):\n",
    "    \"\"\"Extract event date using pattern matching and validation.\"\"\"\n",
    "    # Implementation in Cell 6.5\n",
    "    pass\n",
    "\n",
    "def find_associated_address(processed_results, all_text_combined, debug=True):\n",
    "    \"\"\"Extract associated address using NER and pattern matching.\"\"\"\n",
    "    # Implementation in Cell 6.6\n",
    "    pass"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e83db9a9",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# CELL 6.2: Serial Number Extraction Implementation\n",
    "\n",
    "\n",
    "image_extensions = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".tif\"]\n",
    "file_list = [\n",
    "    f\n",
    "    for f in os.listdir(test_folder)\n",
    "    if any(f.lower().endswith(ext) for ext in image_extensions)\n",
    "]\n",
    "print(f\"Found {len(file_list)} image files in test folder\")\n",
    "\"\"\"\n",
    "Implementation of the serial number extraction function.\n",
    "This uses the robust method from Cell 5.\n",
    "\"\"\"\n",
    "\n",
    "def is_valid_serial_number(candidate):\n",
    "    \"\"\"Validate serial number format and detect potential OCR errors.\"\"\"\n",
    "    clean = candidate.replace(\"-\", \"\")\n",
    "\n",
    "    if (\n",
    "        not (5 <= len(clean) <= 12)\n",
    "        or candidate.count(\"-\") > 1\n",
    "        or not re.match(r\"^[A-Z0-9-]+$\", candidate)\n",
    "    ):\n",
    "        return False\n",
    "\n",
    "    has_letters = bool(re.search(r\"[A-Z]\", clean))\n",
    "    has_digits = bool(re.search(r\"[0-9]\", clean))\n",
    "\n",
    "    # Flag digits-only sequences as potential OCR errors\n",
    "    if not has_letters and has_digits:\n",
    "        return \"potential_ocr_error\"\n",
    "\n",
    "    return has_letters and has_digits\n",
    "\n",
    "\n",
    "def calculate_context_score(candidate, source_text, bbox, context):\n",
    "    \"\"\"Calculate context-aware confidence score for serial number candidates.\"\"\"\n",
    "    score = 0.5\n",
    "    candidate_upper = candidate.upper()\n",
    "    is_standalone = source_text.strip() == candidate\n",
    "\n",
    "    if context[\"type\"] == \"document\":\n",
    "        # Extreme Document Label Penalties\n",
    "        extreme_penalties = [\n",
    "            (\"POBOX\", -1.5),\n",
    "            (\"PO\", -1.5),\n",
    "            (r\"DOB\\d+\", -1.5),\n",
    "            (r\"LICENSE\\d+\", -1.4),\n",
    "            (r\"DATE\\d+\", -1.3),\n",
    "            (r\"EXPIRES\\d+\", -1.3),\n",
    "            (r\"ISSUED\\d+\", -1.2),\n",
    "        ]\n",
    "\n",
    "        for pattern, penalty in extreme_penalties:\n",
    "            if (\n",
    "                pattern.startswith(\"r\") and re.search(pattern[1:], candidate_upper)\n",
    "            ) or pattern in candidate_upper:\n",
    "                score += penalty\n",
    "                break\n",
    "        else:\n",
    "            # Strong Document Penalties\n",
    "            if any(\n",
    "                word in context[\"frequent_words\"]\n",
    "                for word in re.findall(r\"[A-Z]+\", candidate_upper)\n",
    "            ):\n",
    "                score -= 0.8\n",
    "            elif any(\n",
    "                addr in candidate_upper for addr in [\"ST\", \"AVE\", \"BLVD\", \"RD\", \"DR\"]\n",
    "            ):\n",
    "                score -= 0.8\n",
    "            elif any(name in candidate_upper for name in [\"MR\", \"MS\", \"DR\"]):\n",
    "                score -= 0.8\n",
    "\n",
    "        y_bucket = int((bbox[0][1] + bbox[2][1]) / 100) * 50\n",
    "        if y_bucket in context[\"label_zones\"]:\n",
    "            score -= 0.6\n",
    "\n",
    "        if is_standalone:\n",
    "            score += 0.2\n",
    "\n",
    "    elif context[\"type\"] in [\"firearm\", \"isolated\"]:\n",
    "        # Manufacturing penalties\n",
    "        if re.search(r\"\\d+MM|CAL\", candidate_upper):\n",
    "            score -= 0.5\n",
    "        elif candidate_upper in [\"USA\", \"AUSTRIA\", \"GERMANY\", \"ITALY\"]:\n",
    "            score -= 0.4\n",
    "        elif candidate in context.get(\"manufacturing_marks\", set()):\n",
    "            score -= 0.3\n",
    "\n",
    "        if is_standalone:\n",
    "            score += 0.8\n",
    "\n",
    "    else:  # mixed\n",
    "        if any(\n",
    "            label in candidate_upper\n",
    "            for label in [\"POBOX\", \"DOB\", \"DATE\", \"EXP\", \"ISSUED\", \"LIC\"]\n",
    "        ):\n",
    "            score += -1.0 if not is_standalone else -0.5\n",
    "\n",
    "        if is_standalone:\n",
    "            score += 0.4\n",
    "\n",
    "    # Common bonuses\n",
    "    if 6 <= len(candidate.replace(\"-\", \"\")) <= 10:\n",
    "        score += 0.2\n",
    "\n",
    "    letters = sum(1 for c in candidate if c.isalpha())\n",
    "    digits = sum(1 for c in candidate if c.isdigit())\n",
    "    if letters + digits > 0 and 0.3 <= letters / (letters + digits) <= 0.7:\n",
    "        score += 0.2\n",
    "\n",
    "    return max(0, min(2, score))\n",
    "\n",
    "\n",
    "def get_method_description(method):\n",
    "    \"\"\"Get human-readable description of pattern matching method.\"\"\"\n",
    "    descriptions = {\n",
    "        \"Pattern_0\": \"Mixed Letters+Digits 5-12 chars (Highest Priority)\",\n",
    "        \"Pattern_1\": \"Letters followed by Digits (High Priority)\",\n",
    "        \"Pattern_2\": \"Digits followed by Letters (Medium-High Priority)\",\n",
    "        \"Pattern_3\": \"Mixed alphanumeric with hyphen (Medium Priority)\",\n",
    "        \"Pattern_4\": \"Digits-only (potential OCR error) (Low Priority)\",\n",
    "    }\n",
    "    base_desc = descriptions.get(method.replace(\"Combined_\", \"\"), method)\n",
    "    return (\n",
    "        f\"Combined Text: {base_desc}\" if method.startswith(\"Combined_\") else base_desc\n",
    "    )\n",
    "\n",
    "\n",
    "def simple_clustering(text_positions, eps=150, min_samples=2):\n",
    "    \"\"\"Perform distance-based clustering of text positions without sklearn dependency.\"\"\"\n",
    "    if len(text_positions) < min_samples:\n",
    "        return [-1] * len(text_positions)\n",
    "\n",
    "    labels = [-1] * len(text_positions)\n",
    "    cluster_id = 0\n",
    "\n",
    "    for i, (x1, y1, _, _, _) in enumerate(text_positions):\n",
    "        if labels[i] != -1:\n",
    "            continue\n",
    "\n",
    "        cluster = [i]\n",
    "        labels[i] = cluster_id\n",
    "\n",
    "        for j, (x2, y2, _, _, _) in enumerate(text_positions):\n",
    "            if i == j or labels[j] != -1:\n",
    "                continue\n",
    "\n",
    "            if ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5 <= eps:\n",
    "                cluster.append(j)\n",
    "                labels[j] = cluster_id\n",
    "\n",
    "        if len(cluster) >= min_samples:\n",
    "            cluster_id += 1\n",
    "        else:\n",
    "            for idx in cluster:\n",
    "                labels[idx] = -1\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def detect_image_regions(processed_results, image_dimensions):\n",
    "    \"\"\"Identify and classify different regions within an image based on text clustering.\"\"\"\n",
    "    if len(processed_results) < 3:\n",
    "        return [{\"type\": \"unknown\", \"texts\": processed_results, \"region_id\": 0}]\n",
    "\n",
    "    text_positions = [\n",
    "        (\n",
    "            (bbox[0][0] + bbox[2][0]) / 2,\n",
    "            (bbox[0][1] + bbox[2][1]) / 2,\n",
    "            text,\n",
    "            bbox,\n",
    "            confidence,\n",
    "        )\n",
    "        for bbox, text, confidence in processed_results\n",
    "    ]\n",
    "\n",
    "    labels = simple_clustering(text_positions, eps=150, min_samples=2)\n",
    "\n",
    "    regions = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        regions.setdefault(label, []).append(processed_results[i])\n",
    "\n",
    "    classified_regions = []\n",
    "    for region_id, region_texts in regions.items():\n",
    "        region_type = (\n",
    "            \"isolated\" if region_id == -1 else classify_region_type(region_texts)\n",
    "        )\n",
    "        classified_regions.append(\n",
    "            {\n",
    "                \"type\": region_type,\n",
    "                \"texts\": region_texts,\n",
    "                \"region_id\": region_id if region_id != -1 else len(classified_regions),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return classified_regions\n",
    "\n",
    "\n",
    "def classify_region_type(region_texts):\n",
    "    \"\"\"Classify a region as document, firearm, or mixed based on content analysis.\"\"\"\n",
    "    all_text = \" \".join([text for _, text, _ in region_texts]).upper()\n",
    "    text_count = len(region_texts)\n",
    "\n",
    "    if region_texts:\n",
    "        xs = [bbox[0][0] for bbox, _, _ in region_texts] + [\n",
    "            bbox[2][0] for bbox, _, _ in region_texts\n",
    "        ]\n",
    "        ys = [bbox[0][1] for bbox, _, _ in region_texts] + [\n",
    "            bbox[2][1] for bbox, _, _ in region_texts\n",
    "        ]\n",
    "        area = (max(xs) - min(xs)) * (max(ys) - min(ys))\n",
    "        text_area = sum(\n",
    "            (bbox[2][0] - bbox[0][0]) * (bbox[2][1] - bbox[0][1])\n",
    "            for bbox, _, _ in region_texts\n",
    "        )\n",
    "        density = text_area / area if area > 0 else 0\n",
    "    else:\n",
    "        density = 0\n",
    "\n",
    "    doc_keywords = [\"LICENSE\", \"NAME\", \"ADDRESS\", \"DATE\", \"ISSUED\", \"EXPIRES\"]\n",
    "    firearm_keywords = [\"GLOCK\", \"SMITH\", \"COLT\", \"CAL\", \"MM\", \"MODEL\", \"MADE IN\"]\n",
    "\n",
    "    doc_score = (\n",
    "        (2 if density > 0.1 else 0)\n",
    "        + (2 if text_count > 8 else 0)\n",
    "        + sum(2 for kw in doc_keywords if kw in all_text)\n",
    "    )\n",
    "    firearm_score = (\n",
    "        (2 if density < 0.05 else 0)\n",
    "        + (2 if text_count < 6 else 0)\n",
    "        + sum(2 for kw in firearm_keywords if kw in all_text)\n",
    "    )\n",
    "\n",
    "    if doc_score > firearm_score + 2:\n",
    "        return \"document\"\n",
    "    elif firearm_score > doc_score + 2:\n",
    "        return \"firearm\"\n",
    "    else:\n",
    "        return \"mixed\"\n",
    "\n",
    "\n",
    "def analyze_region_context(region_texts, region_type):\n",
    "    \"\"\"Perform detailed context analysis for a classified region.\"\"\"\n",
    "    all_texts = [text for _, text, _ in region_texts]\n",
    "\n",
    "    if region_type == \"document\":\n",
    "        word_freq = {}\n",
    "        for text in all_texts:\n",
    "            for word in re.findall(r\"[A-Z]{3,}\", text.upper()):\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "        frequent_words = {word for word, count in word_freq.items() if count > 1}\n",
    "        document_labels = {\n",
    "            \"LICENSE\",\n",
    "            \"NAME\",\n",
    "            \"ADDRESS\",\n",
    "            \"DATE\",\n",
    "            \"DOB\",\n",
    "            \"EXPIRES\",\n",
    "            \"ISSUED\",\n",
    "            \"POBOX\",\n",
    "            \"PO\",\n",
    "            \"BOX\",\n",
    "        }\n",
    "\n",
    "        for text in all_texts:\n",
    "            text_upper = text.upper()\n",
    "            frequent_words.update(\n",
    "                label for label in document_labels if label in text_upper\n",
    "            )\n",
    "\n",
    "        y_groups = {}\n",
    "        for bbox, text, _ in region_texts:\n",
    "            y_bucket = int((bbox[0][1] + bbox[2][1]) / 100) * 50\n",
    "            y_groups.setdefault(y_bucket, []).append(text)\n",
    "\n",
    "        label_zones = {\n",
    "            y\n",
    "            for y, texts in y_groups.items()\n",
    "            if len(texts) > 2 and sum(len(t) for t in texts) / len(texts) < 15\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"type\": \"document\",\n",
    "            \"frequent_words\": frequent_words,\n",
    "            \"label_zones\": label_zones,\n",
    "        }\n",
    "\n",
    "    elif region_type == \"firearm\":\n",
    "        all_text = \" \".join(all_texts).upper()\n",
    "        manufacturers = [\"GLOCK\", \"SMITH\", \"COLT\", \"RUGER\", \"SIG\"]\n",
    "        manufacturer = next((mfg for mfg in manufacturers if mfg in all_text), None)\n",
    "\n",
    "        manufacturing_marks = set()\n",
    "        for text in all_texts:\n",
    "            text_upper = text.upper()\n",
    "            if (\n",
    "                re.search(r\"\\d+MM|CAL\", text_upper)\n",
    "                or any(\n",
    "                    country in text_upper for country in [\"USA\", \"AUSTRIA\", \"GERMANY\"]\n",
    "                )\n",
    "                or (\n",
    "                    manufacturer\n",
    "                    and text_upper.startswith(manufacturer[:4])\n",
    "                    and len(text) <= 8\n",
    "                )\n",
    "            ):\n",
    "                manufacturing_marks.add(text)\n",
    "\n",
    "        return {\n",
    "            \"type\": \"firearm\",\n",
    "            \"manufacturer\": manufacturer,\n",
    "            \"manufacturing_marks\": manufacturing_marks,\n",
    "        }\n",
    "\n",
    "    elif region_type == \"isolated\":\n",
    "        if len(region_texts) == 1:\n",
    "            text = region_texts[0][1]\n",
    "            if len(text) >= 5 and text.isalnum():\n",
    "                return {\n",
    "                    \"type\": \"firearm\",\n",
    "                    \"manufacturer\": None,\n",
    "                    \"manufacturing_marks\": set(),\n",
    "                }\n",
    "        return {\n",
    "            \"type\": \"mixed\",\n",
    "            \"frequent_words\": set(),\n",
    "            \"label_zones\": set(),\n",
    "            \"manufacturing_marks\": set(),\n",
    "        }\n",
    "\n",
    "    elif region_type == \"unknown\":\n",
    "        if (\n",
    "            len(region_texts) == 1\n",
    "            and len(region_texts[0][1]) >= 5\n",
    "            and region_texts[0][1].replace(\" \", \"\").isalnum()\n",
    "        ):\n",
    "            return {\n",
    "                \"type\": \"firearm\",\n",
    "                \"manufacturer\": None,\n",
    "                \"manufacturing_marks\": set(),\n",
    "            }\n",
    "        return {\n",
    "            \"type\": \"mixed\",\n",
    "            \"frequent_words\": set(),\n",
    "            \"label_zones\": set(),\n",
    "            \"manufacturing_marks\": set(),\n",
    "        }\n",
    "\n",
    "    else:  # mixed\n",
    "        return {\n",
    "            \"type\": \"mixed\",\n",
    "            \"frequent_words\": set(),\n",
    "            \"label_zones\": set(),\n",
    "            \"manufacturing_marks\": set(),\n",
    "        }\n",
    "\n",
    "\n",
    "def find_serial_number(\n",
    "    processed_results, all_text_combined, image_dimensions, debug=True\n",
    "):\n",
    "    \"\"\"Main serial number detection function with multi-region analysis.\"\"\"\n",
    "    regions = detect_image_regions(processed_results, image_dimensions)\n",
    "\n",
    "    if debug:\n",
    "        if len(regions) > 1:\n",
    "            print(\n",
    "                f\"  Image classified as: multi-region\"\n",
    "            )\n",
    "            print(f\"    {len(regions)} regions detected\")\n",
    "            for i, region in enumerate(regions):\n",
    "                print(\n",
    "                    f\"    Region {i+1}: {len(region['texts'])} texts\"\n",
    "                )\n",
    "        else:\n",
    "            print(f\"  Image classified as: {regions[0]['type']}\")\n",
    "\n",
    "    patterns = [\n",
    "        r\"\\b[A-Z0-9]{5,12}\\b\",\n",
    "        r\"\\b[A-Z]{1,6}[0-9]{1,11}\\b\",\n",
    "        r\"\\b[0-9]{1,11}[A-Z]{1,6}\\b\",\n",
    "        r\"\\b[A-Z0-9]{2,6}-[A-Z0-9]{2,6}\\b\",\n",
    "        r\"\\b[0-9]{5,12}\\b\",\n",
    "    ]\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    # Process each region\n",
    "    for region in regions:\n",
    "        context = analyze_region_context(region[\"texts\"], region[\"type\"])\n",
    "\n",
    "        for pattern_idx, pattern in enumerate(patterns):\n",
    "            for bbox, text, confidence in region[\"texts\"]:\n",
    "                for match in re.findall(pattern, text):\n",
    "                    validation = is_valid_serial_number(match)\n",
    "\n",
    "                    if validation == True or validation == \"potential_ocr_error\":\n",
    "                        context_score = calculate_context_score(\n",
    "                            match, text, bbox, context\n",
    "                        )\n",
    "                        region_bonus = (\n",
    "                            0.3 if context[\"type\"] in [\"firearm\", \"isolated\"] else 0.0\n",
    "                        )\n",
    "\n",
    "                        score = (\n",
    "                            confidence * 0.2\n",
    "                            + context_score * 0.7\n",
    "                            + (len(patterns) - pattern_idx) * 0.02\n",
    "                            + region_bonus\n",
    "                        )\n",
    "\n",
    "                        # Determine status and serial number to report\n",
    "                        if validation == \"potential_ocr_error\":\n",
    "                            score *= 0.5\n",
    "                            status = \"Potential OCR Error\"\n",
    "                            reported_serial = f\"{match}_potential_error\"  # Add suffix for potential errors\n",
    "                        else:\n",
    "                            status = \"Valid\"\n",
    "                            reported_serial = match\n",
    "\n",
    "                        candidates.append(\n",
    "                            (\n",
    "                                reported_serial,\n",
    "                                score,\n",
    "                                f\"Pattern_{pattern_idx}\",\n",
    "                                f\"From OCR text: '{text}'\",\n",
    "                                confidence,\n",
    "                                pattern_idx + 1,\n",
    "                                text,\n",
    "                                status,\n",
    "                                context_score,\n",
    "                                region[\"region_id\"],\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "    # Process combined text\n",
    "    if all_text_combined:\n",
    "        for pattern_idx, pattern in enumerate(patterns):\n",
    "            for match in re.findall(pattern, all_text_combined):\n",
    "                validation = is_valid_serial_number(match)\n",
    "                if validation == True:\n",
    "                    score = 0.3 * 0.2 + 0.3 * 0.7 + (len(patterns) - pattern_idx) * 0.01\n",
    "                    candidates.append(\n",
    "                        (\n",
    "                            match,\n",
    "                            score,\n",
    "                            f\"Combined_Pattern_{pattern_idx}\",\n",
    "                            \"From combined text\",\n",
    "                            0.3,\n",
    "                            pattern_idx + 1,\n",
    "                            \"Combined OCR text\",\n",
    "                            \"Valid\",\n",
    "                            0.3,\n",
    "                            -1,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    # Remove duplicates, keeping highest score\n",
    "    unique_candidates = {}\n",
    "    for candidate in candidates:\n",
    "        serial = candidate[0]\n",
    "        if (\n",
    "            serial not in unique_candidates\n",
    "            or candidate[1] > unique_candidates[serial][1]\n",
    "        ):\n",
    "            unique_candidates[serial] = candidate\n",
    "\n",
    "    final_candidates = sorted(\n",
    "        unique_candidates.values(), key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    if debug and final_candidates:\n",
    "        print(f\"Serial number candidates (ALL {len(final_candidates)}):\")\n",
    "        for i, (\n",
    "            serial,\n",
    "            score,\n",
    "            method,\n",
    "            source,\n",
    "            conf,\n",
    "            priority,\n",
    "            source_text,\n",
    "            status,\n",
    "            context_score,\n",
    "            region_id,\n",
    "        ) in enumerate(final_candidates[:5]):  # Show top 5\n",
    "            method_desc = get_method_description(method)\n",
    "            region_info = (\n",
    "                f\" [Region {region_id+1}]\" if region_id >= 0 else \" [Combined]\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  {i+1}. '{serial}' (score: {score:.3f}, method: {method}, priority: {priority}, conf: {conf:.3f}, status: {status}){region_info}\"\n",
    "            )\n",
    "\n",
    "    return final_candidates[0] if final_candidates else None, final_candidates\n",
    "\n",
    "\n",
    "# Test the serial number extraction on ALL images\n",
    "print(\"\\n===== TESTING SERIAL NUMBER EXTRACTION ON ALL IMAGES =====\")\n",
    "\n",
    "# Initialize results storage\n",
    "serial_results = []\n",
    "\n",
    "if file_list:\n",
    "    for test_file in file_list:\n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        print(f\"Processing: {test_file}\")\n",
    "        test_path = os.path.join(test_folder, test_file)\n",
    "        \n",
    "        # Load and process image\n",
    "        image = cv2.imread(test_path)\n",
    "        if image is None:\n",
    "            print(f\"Could not read image: {test_file}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Dimensions: {image.shape[1]}x{image.shape[0]}\")\n",
    "        \n",
    "        # Perform OCR\n",
    "        ocr_results = ocr.ocr(image, cls=True)\n",
    "        if not ocr_results or not ocr_results[0]:\n",
    "            print(f\"  No text detected\")\n",
    "            continue\n",
    "            \n",
    "        # Process OCR results\n",
    "        processed_results = []\n",
    "        all_text = []\n",
    "        \n",
    "        for line in ocr_results[0]:\n",
    "            bbox, (text, confidence) = line\n",
    "            text = combine_spaced_alphanumeric(text.strip())\n",
    "            processed_results.append((bbox, text, confidence))\n",
    "            all_text.append(text)\n",
    "            \n",
    "        all_text_combined = \" \".join(all_text)\n",
    "        print(f\"  OCR extracted {len(all_text_combined)} characters\")\n",
    "        \n",
    "        # Extract serial number\n",
    "        selected_candidate, all_candidates = find_serial_number(\n",
    "            processed_results, all_text_combined, (image.shape[1], image.shape[0]), debug=True\n",
    "        )\n",
    "        \n",
    "        candidate_serials = (\n",
    "            [candidate[0] for candidate in all_candidates] if all_candidates else []\n",
    "        )\n",
    "        \n",
    "        # Display image with colored bounding boxes\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img_rgb)\n",
    "        \n",
    "        for bbox, text, confidence in processed_results:\n",
    "            # Determine box color based on serial number detection\n",
    "            if (\n",
    "                selected_candidate\n",
    "                and selected_candidate[0].replace(\"_potential_error\", \"\") in text\n",
    "            ):\n",
    "                color, linewidth = \"red\", 3  # Best candidate\n",
    "            elif any(\n",
    "                candidate.replace(\"_potential_error\", \"\") in text\n",
    "                for candidate in candidate_serials\n",
    "            ):\n",
    "                color, linewidth = \"blue\", 2  # Other candidates\n",
    "            else:\n",
    "                color, linewidth = \"gray\", 1  # Regular text\n",
    "                \n",
    "            # Draw bounding box\n",
    "            points = np.array(bbox, dtype=np.int32)\n",
    "            plt.plot(\n",
    "                [points[0][0], points[1][0], points[2][0], points[3][0], points[0][0]],\n",
    "                [points[0][1], points[1][1], points[2][1], points[3][1], points[0][1]],\n",
    "                color=color,\n",
    "                linewidth=linewidth,\n",
    "            )\n",
    "            \n",
    "            # Add text label\n",
    "            plt.text(\n",
    "                points[0][0],\n",
    "                points[0][1] - 5,\n",
    "                f\"{text} ({confidence:.2f})\",\n",
    "                fontsize=8,\n",
    "                color=color,\n",
    "                weight=\"bold\",\n",
    "            )\n",
    "            \n",
    "        plt.title(f\"Multi-Region Serial Number Detection: {test_file}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Output results and store for CSV\n",
    "        if selected_candidate:\n",
    "            (\n",
    "                serial_number,\n",
    "                _,\n",
    "                method,\n",
    "                _,\n",
    "                confidence,\n",
    "                priority,\n",
    "                source_text,\n",
    "                status,\n",
    "                context_score,\n",
    "                region_id,\n",
    "            ) = selected_candidate\n",
    "            method_desc = get_method_description(method)\n",
    "            \n",
    "            print(f\"Found 1 serial number\")\n",
    "            print(f\"Serial number: {serial_number}\")\n",
    "            print(f\"Method: {method} (Priority: {priority}) - {method_desc}\")\n",
    "            print(f\"Status: {status}\")\n",
    "            print(f\"Context Score: {context_score:.3f}\")\n",
    "            if region_id >= 0:\n",
    "                print(f\"Region: {region_id+1}\")\n",
    "            print(f\"Extracted from: '{source_text}'\")\n",
    "            \n",
    "            # Store result for CSV output\n",
    "            serial_results.append({\n",
    "                'filename': test_file,\n",
    "                'serial_number': serial_number,\n",
    "                'confidence': confidence,\n",
    "                'method': method\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Found 0 serial numbers\")\n",
    "    \n",
    "    # Display summary of results\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"SERIAL NUMBER EXTRACTION SUMMARY\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    print(f\"Processed {len(serial_results)} images\")\n",
    "    print(f\"Found serial numbers in {sum(1 for r in serial_results if r['serial_number'])} images\")\n",
    "    \n",
    "    # Display table of results\n",
    "    if serial_results:\n",
    "        results_df = pd.DataFrame(serial_results)\n",
    "        print(\"\\nExtracted Serial Numbers:\")\n",
    "        print(results_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No test images available for serial number extraction testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863788ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6.3: Event Type Extraction Implementation with Checkbox Detection\n",
    "\n",
    "\"\"\"\n",
    "Implementation of the event type extraction function using TF-IDF and cosine similarity.\n",
    "Includes checkbox detection to identify selected event types.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def detect_checkboxes(image, processed_results, debug=False):\n",
    "    \"\"\"\n",
    "    Detect checkboxes in the image and identify which ones are checked.\n",
    "    \n",
    "    Args:\n",
    "        image: The input image\n",
    "        processed_results: List of (bbox, text, confidence) tuples from OCR\n",
    "        debug: Whether to print debug information\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping text to checkbox status (True if checked, False otherwise)\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Threshold the image\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Filter contours to find potential checkboxes\n",
    "    checkboxes = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Filter by aspect ratio and size - more strict filtering\n",
    "        aspect_ratio = float(w) / h\n",
    "        if 0.8 <= aspect_ratio <= 1.2 and 15 <= w <= 25 and 15 <= h <= 25:\n",
    "            # Check if it's a checkbox by looking at the shape\n",
    "            perimeter = cv2.arcLength(contour, True)\n",
    "            approx = cv2.approxPolyDP(contour, 0.04 * perimeter, True)\n",
    "            \n",
    "            # Checkboxes typically have 4 corners\n",
    "            if len(approx) == 4:\n",
    "                checkboxes.append((x, y, w, h))\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Found {len(checkboxes)} potential checkboxes\")\n",
    "    \n",
    "    # Define event type options we're specifically looking for\n",
    "    event_types = [\"Burglary\", \"Robbery\", \"Larceny\", \"Missing Inventory\"]\n",
    "    \n",
    "    # For each checkbox, check if it's filled\n",
    "    checkbox_status = {}\n",
    "    for x, y, w, h in checkboxes:\n",
    "        # Extract the checkbox region\n",
    "        checkbox_roi = thresh[y:y+h, x:x+w]\n",
    "        \n",
    "        # Calculate the percentage of filled pixels\n",
    "        filled_ratio = np.sum(checkbox_roi > 0) / (w * h)\n",
    "        \n",
    "        # Consider it checked if more than 20% is filled\n",
    "        is_checked = filled_ratio > 0.2\n",
    "        \n",
    "        # Only look for text near the checkbox that matches event types\n",
    "        if is_checked:\n",
    "            min_distance = float('inf')\n",
    "            nearest_text = None\n",
    "            \n",
    "            for bbox, text, _ in processed_results:\n",
    "                # Only consider text that matches our event types\n",
    "                if text in event_types:\n",
    "                    # Calculate center of text bbox\n",
    "                    text_center_x = sum([point[0] for point in bbox]) / len(bbox)\n",
    "                    text_center_y = sum([point[1] for point in bbox]) / len(bbox)\n",
    "                    \n",
    "                    # Calculate center of checkbox\n",
    "                    checkbox_center_x = x + w/2\n",
    "                    checkbox_center_y = y + h/2\n",
    "                    \n",
    "                    # Calculate distance\n",
    "                    distance = ((text_center_x - checkbox_center_x) ** 2 + \n",
    "                                (text_center_y - checkbox_center_y) ** 2) ** 0.5\n",
    "                    \n",
    "                    # Check if this text is closer than previous nearest\n",
    "                    if distance < min_distance and distance < 50:  # Within 50 pixels\n",
    "                        min_distance = distance\n",
    "                        nearest_text = text\n",
    "            \n",
    "            if nearest_text:\n",
    "                checkbox_status[nearest_text] = is_checked\n",
    "                if debug:\n",
    "                    print(f\"Checkbox near '{nearest_text}' is checked (Distance: {min_distance:.1f})\")\n",
    "    \n",
    "    return checkbox_status\n",
    "\n",
    "def calculate_position_score(bbox):\n",
    "    \"\"\"Calculate position score based on location in document.\"\"\"\n",
    "    # Get y-coordinate (vertical position)\n",
    "    y_position = bbox[0][1]\n",
    "    \n",
    "    # Normalize to 0-1 range (assuming document height of 3000 pixels)\n",
    "    normalized_position = min(1.0, max(0.0, y_position / 3000))\n",
    "    \n",
    "    # Higher score for elements closer to the top\n",
    "    return 1.0 - normalized_position\n",
    "\n",
    "def calculate_contextual_relevance(text, bbox, processed_results):\n",
    "    \"\"\"Calculate contextual relevance score based on position and surrounding text.\"\"\"\n",
    "    # Position-based score (prefer text near the top of the document)\n",
    "    position_score = calculate_position_score(bbox)\n",
    "    \n",
    "    # Look for event type indicators nearby\n",
    "    event_indicators = [\"event\", \"incident\", \"type\", \"description\", \"category\"]\n",
    "    proximity_score = 0\n",
    "    \n",
    "    for other_bbox, other_text, _ in processed_results:\n",
    "        if bbox != other_bbox:\n",
    "            other_text_lower = other_text.lower()\n",
    "            if any(indicator in other_text_lower for indicator in event_indicators):\n",
    "                # Calculate distance between bboxes\n",
    "                center1_x = sum([point[0] for point in bbox]) / len(bbox)\n",
    "                center1_y = sum([point[1] for point in bbox]) / len(bbox)\n",
    "                center2_x = sum([point[0] for point in other_bbox]) / len(other_bbox)\n",
    "                center2_y = sum([point[1] for point in other_bbox]) / len(other_bbox)\n",
    "                distance = ((center1_x - center2_x) ** 2 + (center1_y - center2_y) ** 2) ** 0.5\n",
    "                \n",
    "                if distance < 100:\n",
    "                    proximity_score = 1.0\n",
    "                elif distance < 200:\n",
    "                    proximity_score = 0.8\n",
    "                elif distance < 300:\n",
    "                    proximity_score = 0.6\n",
    "                elif distance < 400:\n",
    "                    proximity_score = 0.4\n",
    "                else:\n",
    "                    proximity_score = max(proximity_score, 0.2)\n",
    "    \n",
    "    # Combine scores\n",
    "    return 0.4 * position_score + 0.6 * proximity_score\n",
    "\n",
    "def find_event_type(processed_results, all_text_combined, image=None, serial_number=None, debug=True):\n",
    "    \"\"\"\n",
    "    Extract event type using TF-IDF and cosine similarity with checkbox detection.\n",
    "    \n",
    "    Args:\n",
    "        processed_results: List of (bbox, text, confidence) tuples from OCR\n",
    "        all_text_combined: Combined text from all OCR results\n",
    "        image: Original image for checkbox detection\n",
    "        serial_number: Optional serial number for context\n",
    "        debug: Whether to print debug information\n",
    "        \n",
    "    Returns:\n",
    "        Extracted event type string, or empty string if not found\n",
    "    \"\"\"\n",
    "    # Minimum semantic score threshold\n",
    "    SEMANTIC_THRESHOLD = 0.3\n",
    "    \n",
    "    # Check if there's enough text to extract an event type\n",
    "    if len(all_text_combined) < 20 or len(processed_results) < 3:\n",
    "        if debug:\n",
    "            print(\"Not enough text to extract event type\")\n",
    "        return \"\", []\n",
    "    \n",
    "    # Define event type categories with example phrases\n",
    "    event_categories = {\n",
    "        \"theft\": [\"theft\", \"stolen\", \"burglary\", \"robbery\", \"shoplifting\", \"larceny\"],\n",
    "        \"loss\": [\"loss\", \"lost\", \"misplaced\", \"missing\", \"inventory loss\", \"missing inventory\"],\n",
    "        \"damage\": [\"damage\", \"damaged\", \"destruction\", \"vandalism\", \"defaced\"],\n",
    "        \"other\": [\"other\", \"unknown\", \"miscellaneous\", \"unspecified\"]\n",
    "    }\n",
    "    \n",
    "    # Detect checkboxes if image is provided\n",
    "    checkbox_status = {}\n",
    "    if image is not None:\n",
    "        checkbox_status = detect_checkboxes(image, processed_results, debug)\n",
    "        \n",
    "        # Check if any checkboxes are detected as checked\n",
    "        checked_items = [text for text, is_checked in checkbox_status.items() if is_checked]\n",
    "        if checked_items and debug:\n",
    "            print(f\"Detected checked items: {checked_items}\")\n",
    "            \n",
    "        # If we have checked items that match common event types, prioritize them\n",
    "        common_event_types = [\"Burglary\", \"Robbery\", \"Larceny\", \"Missing Inventory\"]\n",
    "        for item in checked_items:\n",
    "            if item in common_event_types:\n",
    "                if debug:\n",
    "                    print(f\"Selected event type from checkbox: {item}\")\n",
    "                return item, []\n",
    "    \n",
    "    # Flatten categories for vectorization\n",
    "    all_examples = []\n",
    "    category_indices = {}\n",
    "    current_idx = 0\n",
    "    \n",
    "    for category, examples in event_categories.items():\n",
    "        category_indices[category] = (current_idx, current_idx + len(examples))\n",
    "        all_examples.extend(examples)\n",
    "        current_idx += len(examples)\n",
    "    \n",
    "    # Extract text from processed results\n",
    "    texts = [text for _, text, _ in processed_results]\n",
    "    \n",
    "    # Skip if no text found\n",
    "    if not texts:\n",
    "        if debug:\n",
    "            print(\"No text found in processed results\")\n",
    "        return \"\", []\n",
    "    \n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    # Combine examples and texts for vectorization\n",
    "    all_texts = all_examples + texts\n",
    "    \n",
    "    try:\n",
    "        # Create TF-IDF matrix\n",
    "        tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "        \n",
    "        # Split matrix into examples and texts\n",
    "        examples_matrix = tfidf_matrix[:len(all_examples)]\n",
    "        texts_matrix = tfidf_matrix[len(all_examples):]\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error in TF-IDF vectorization: {e}\")\n",
    "        return \"\", []\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    # Process each text element\n",
    "    for i, (bbox, text, confidence) in enumerate(processed_results):\n",
    "        # Skip if text is too short\n",
    "        if len(text) < 3:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Get text vector\n",
    "            text_vector = texts_matrix[i]\n",
    "            \n",
    "            # Compare with category examples\n",
    "            scores = {}\n",
    "            for category, (start_idx, end_idx) in category_indices.items():\n",
    "                # Calculate cosine similarity with each example in the category\n",
    "                category_vectors = examples_matrix[start_idx:end_idx]\n",
    "                similarities = cosine_similarity(text_vector, category_vectors)[0]\n",
    "                scores[category] = np.max(similarities) if len(similarities) > 0 else 0\n",
    "            \n",
    "            # Find best matching category\n",
    "            best_category = max(scores.items(), key=lambda x: x[1]) if scores else (\"unknown\", 0)\n",
    "            \n",
    "            # Calculate contextual relevance\n",
    "            context_score = calculate_contextual_relevance(text, bbox, processed_results)\n",
    "            \n",
    "            # Calculate checkbox bonus\n",
    "            checkbox_bonus = 2.0 if checkbox_status.get(text, False) else 1.0\n",
    "            \n",
    "            # Final score combines semantic similarity, contextual relevance, and checkbox status\n",
    "            total_score = (best_category[1] * 0.5 + context_score * 0.5) * confidence * checkbox_bonus\n",
    "            \n",
    "            candidates.append({\n",
    "                'text': text,\n",
    "                'score': total_score,\n",
    "                'category': best_category[0],\n",
    "                'semantic_score': best_category[1],\n",
    "                'context_score': context_score,\n",
    "                'bbox': bbox,\n",
    "                'confidence': confidence,\n",
    "                'is_checked': checkbox_status.get(text, False)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"Error processing text '{text}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Filter and rank candidates\n",
    "    if not candidates:\n",
    "        if debug:\n",
    "            print(\"No event type candidates found\")\n",
    "        return \"\", []\n",
    "        \n",
    "    # Filter by semantic score threshold\n",
    "    valid_candidates = [c for c in candidates if c['semantic_score'] >= SEMANTIC_THRESHOLD]\n",
    "    \n",
    "    # Prioritize checked checkboxes\n",
    "    checked_candidates = [c for c in valid_candidates if c['is_checked']]\n",
    "    if checked_candidates:\n",
    "        valid_candidates = checked_candidates\n",
    "    \n",
    "    # Sort by score\n",
    "    valid_candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Debug output\n",
    "    if debug and valid_candidates:\n",
    "        print(f\"\\nEvent type candidates ({len(valid_candidates)}):\")\n",
    "        for i, candidate in enumerate(valid_candidates[:5]):  # Show top 5\n",
    "            checked = \", Checked: Yes\" if candidate['is_checked'] else \"\"\n",
    "            print(f\"  {i+1}. '{candidate['text']}' - Score: {candidate['score']:.3f} \"\n",
    "                  f\"(Category: {candidate['category']}, Semantic: {candidate['semantic_score']:.3f}, \"\n",
    "                  f\"Context: {candidate['context_score']:.3f}{checked})\")\n",
    "    elif debug:\n",
    "        print(\"No valid event type candidates found\")\n",
    "    \n",
    "    # Return best candidate or empty string if none found\n",
    "    return valid_candidates[0]['text'] if valid_candidates else \"\", valid_candidates\n",
    "\n",
    "\n",
    "# Test the event type extraction on ALL images\n",
    "print(\"\\n===== TESTING EVENT TYPE EXTRACTION ON ALL IMAGES =====\")\n",
    "\n",
    "# Initialize results storage\n",
    "event_type_results = []\n",
    "\n",
    "if file_list:\n",
    "    for test_file in file_list:\n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        print(f\"Processing: {test_file}\")\n",
    "        test_path = os.path.join(test_folder, test_file)\n",
    "        \n",
    "        # Load and process image\n",
    "        image = cv2.imread(test_path)\n",
    "        if image is None:\n",
    "            print(f\"Could not read image: {test_file}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Dimensions: {image.shape[1]}x{image.shape[0]}\")\n",
    "        \n",
    "        # Perform OCR\n",
    "        ocr_results = ocr.ocr(image, cls=True)\n",
    "        if not ocr_results or not ocr_results[0]:\n",
    "            print(f\"  No text detected\")\n",
    "            continue\n",
    "            \n",
    "        # Process OCR results\n",
    "        processed_results = []\n",
    "        all_text = []\n",
    "        \n",
    "        for line in ocr_results[0]:\n",
    "            bbox, (text, confidence) = line\n",
    "            text = combine_spaced_alphanumeric(text.strip())\n",
    "            processed_results.append((bbox, text, confidence))\n",
    "            all_text.append(text)\n",
    "            \n",
    "        all_text_combined = \" \".join(all_text)\n",
    "        print(f\"  OCR extracted {len(all_text_combined)} characters\")\n",
    "        \n",
    "        # Get image dimensions for serial number extraction\n",
    "        image_dimensions = (image.shape[1], image.shape[0])  # width, height\n",
    "        \n",
    "        # Extract serial number first to use as context\n",
    "        serial_number, serial_candidates = find_serial_number(processed_results, all_text_combined, image_dimensions, debug=False)\n",
    "        if serial_number:\n",
    "            print(f\"Serial Number: '{serial_number}'\")\n",
    "        \n",
    "        # Extract event type\n",
    "        event_type, all_candidates = find_event_type(processed_results, all_text_combined, image, serial_number, debug=True)\n",
    "        print(f\"\\nExtracted Event Type: '{event_type}'\")\n",
    "        \n",
    "        # Store result\n",
    "        event_type_results.append({\n",
    "            'filename': test_file,\n",
    "            'event_type': event_type\n",
    "        })\n",
    "        \n",
    "        # Visualize the result with colored bounding boxes\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img_rgb)\n",
    "        \n",
    "        # Get all candidate texts for visualization\n",
    "        candidate_texts = [candidate['text'] for candidate in all_candidates if candidate['bbox'] is not None]\n",
    "        \n",
    "        for bbox, text, confidence in processed_results:\n",
    "            # Determine box color based on detection\n",
    "            if event_type and text.strip() == event_type:\n",
    "                color, linewidth = \"red\", 3  # Event type\n",
    "            elif any(text.strip() == candidate_text for candidate_text in candidate_texts):\n",
    "                color, linewidth = \"blue\", 2  # Other candidates\n",
    "            else:\n",
    "                color, linewidth = \"gray\", 1  # Regular text\n",
    "                \n",
    "            # Draw bounding box\n",
    "            points = np.array(bbox, dtype=np.int32)\n",
    "            plt.plot(\n",
    "                [points[0][0], points[1][0], points[2][0], points[3][0], points[0][0]],\n",
    "                [points[0][1], points[1][1], points[2][1], points[3][1], points[0][1]],\n",
    "                color=color,\n",
    "                linewidth=linewidth,\n",
    "            )\n",
    "            \n",
    "            # Add text label\n",
    "            plt.text(\n",
    "                points[0][0],\n",
    "                points[0][1] - 5,\n",
    "                f\"{text} ({confidence:.2f})\",\n",
    "                fontsize=8,\n",
    "                color=color,\n",
    "                weight=\"bold\",\n",
    "            )\n",
    "            \n",
    "        plt.title(f\"Event Type Detection: {test_file}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Display summary of results\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"EVENT TYPE EXTRACTION SUMMARY\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    print(f\"Processed {len(event_type_results)} images\")\n",
    "    print(f\"Found event types in {sum(1 for r in event_type_results if r['event_type'])} images\")\n",
    "    \n",
    "    # Display table of results\n",
    "    if event_type_results:\n",
    "        results_df = pd.DataFrame(event_type_results)\n",
    "        print(\"\\nExtracted Event Types:\")\n",
    "        print(results_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No test images available for event type extraction testing\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0c2fa46",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# CELL 6.3: Event Type Extraction Implementation - Improved Approach\n",
    "\n",
    "\"\"\"\n",
    "Implementation of the event type extraction function using TF-IDF and cosine similarity.\n",
    "Includes improved handling for images without event types.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_distance(bbox1, bbox2):\n",
    "    \"\"\"Calculate distance between two bounding boxes\"\"\"\n",
    "    center1_x = sum([point[0] for point in bbox1]) / len(bbox1)\n",
    "    center1_y = sum([point[1] for point in bbox1]) / len(bbox1)\n",
    "    center2_x = sum([point[0] for point in bbox2]) / len(bbox2)\n",
    "    center2_y = sum([point[1] for point in bbox2]) / len(bbox2)\n",
    "    return ((center1_x - center2_x) ** 2 + (center1_y - center2_y) ** 2) ** 0.5\n",
    "\n",
    "def identify_form_fields(processed_results):\n",
    "    \"\"\"Identify form fields based on text content and position\"\"\"\n",
    "    form_field_indicators = [\"name\", \"date\", \"address\", \"phone\", \"description\", \"report\", \"signature\"]\n",
    "    form_fields = []\n",
    "    \n",
    "    for bbox, text, confidence in processed_results:\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check if text contains form field indicators\n",
    "        if any(indicator in text_lower for indicator in form_field_indicators):\n",
    "            # Check if text ends with a colon or contains form-like patterns\n",
    "            if text_lower.endswith(\":\") or \":\" in text_lower or \"_____\" in text or \"____\" in text:\n",
    "                form_fields.append((bbox, text, confidence))\n",
    "                \n",
    "    return form_fields\n",
    "\n",
    "def is_form_label(text):\n",
    "    \"\"\"Check if text is likely a form label\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Common form label patterns\n",
    "    if text_lower.endswith(\":\") or \":\" in text_lower:\n",
    "        return True\n",
    "        \n",
    "    # Check if text contains common form field terms\n",
    "    form_terms = [\"name\", \"date\", \"address\", \"phone\", \"description\", \"report\", \"signature\", \n",
    "                 \"enter\", \"fill\", \"complete\", \"provide\", \"include\"]\n",
    "    if any(term in text_lower for term in form_terms) and len(text) > 10:\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def is_legal_text(text):\n",
    "    \"\"\"Check if text is likely legal text or disclaimer\"\"\"\n",
    "    legal_indicators = [\"u.s.c\", \"section\", \"violation\", \"law\", \"penalty\", \"punish\", \"certify\", \"hereby\"]\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Check for legal indicators\n",
    "    if any(indicator in text_lower for indicator in legal_indicators):\n",
    "        return True\n",
    "        \n",
    "    # Check for sentence fragments that start with prepositions\n",
    "    if re.match(r'^(of|in|by|for|with|under|to|from)\\b', text_lower):\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def calculate_contextual_relevance(text, bbox, processed_results):\n",
    "    \"\"\"Calculate contextual relevance score based on position and surrounding text\"\"\"\n",
    "    # Position-based score (prefer text near the top of the document)\n",
    "    y_position = bbox[0][1]\n",
    "    position_score = max(0, 30 - (y_position / 50)) / 30  # Normalize to 0-1\n",
    "    \n",
    "    # Look for event type indicators nearby\n",
    "    event_indicators = [\"event\", \"incident\", \"type\", \"description\", \"report\"]\n",
    "    proximity_score = 0\n",
    "    \n",
    "    for other_bbox, other_text, _ in processed_results:\n",
    "        if bbox != other_bbox:\n",
    "            other_text_lower = other_text.lower()\n",
    "            if any(indicator in other_text_lower for indicator in event_indicators):\n",
    "                distance = calculate_distance(bbox, other_bbox)\n",
    "                if distance < 100:\n",
    "                    proximity_score = 1.0\n",
    "                elif distance < 200:\n",
    "                    proximity_score = 0.8\n",
    "                elif distance < 300:\n",
    "                    proximity_score = 0.6\n",
    "                elif distance < 400:\n",
    "                    proximity_score = 0.4\n",
    "                else:\n",
    "                    proximity_score = max(proximity_score, 0.2)\n",
    "    \n",
    "    # Combine scores\n",
    "    return 0.4 * position_score + 0.6 * proximity_score\n",
    "\n",
    "def find_event_type(processed_results, all_text_combined, serial_number=None, debug=True):\n",
    "    \"\"\"Extract event type using TF-IDF and cosine similarity.\"\"\"\n",
    "    # Minimum semantic score threshold\n",
    "    SEMANTIC_THRESHOLD = 0.3\n",
    "    \n",
    "    # Check if there's enough text to extract an event type\n",
    "    if len(all_text_combined) < 20 or len(processed_results) < 3:\n",
    "        if debug:\n",
    "            print(\"Not enough text to extract event type\")\n",
    "        return \"\", []\n",
    "    \n",
    "    # Define event type categories with example phrases\n",
    "    event_categories = {\n",
    "        \"theft\": [\"theft\", \"stolen\", \"burglary\", \"robbery\", \"larceny\"],\n",
    "        \"missing\": [\"missing inventory\", \"lost items\", \"misplaced equipment\", \"missing property\"],\n",
    "        \"damage\": [\"damage\", \"vandalism\", \"destruction\", \"broken\"],\n",
    "        \"audit\": [\"inventory check\", \"audit\", \"inspection\", \"review\"]\n",
    "    }\n",
    "    \n",
    "    # Flatten categories for vectorization\n",
    "    all_examples = []\n",
    "    category_indices = {}\n",
    "    current_idx = 0\n",
    "    \n",
    "    for category, examples in event_categories.items():\n",
    "        category_indices[category] = (current_idx, current_idx + len(examples))\n",
    "        all_examples.extend(examples)\n",
    "        current_idx += len(examples)\n",
    "    \n",
    "    # Extract text from processed results\n",
    "    texts = [text for _, text, _ in processed_results]\n",
    "    \n",
    "    # Skip if no text found\n",
    "    if not texts:\n",
    "        if debug:\n",
    "            print(\"No text found in processed results\")\n",
    "        return \"\", []\n",
    "    \n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    # Combine examples and texts for vectorization\n",
    "    all_texts = all_examples + texts\n",
    "    \n",
    "    try:\n",
    "        # Create TF-IDF matrix\n",
    "        tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "        \n",
    "        # Split matrix into examples and texts\n",
    "        examples_matrix = tfidf_matrix[:len(all_examples)]\n",
    "        texts_matrix = tfidf_matrix[len(all_examples):]\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error in TF-IDF vectorization: {e}\")\n",
    "        return \"\", []\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    # Process each text element\n",
    "    for i, (bbox, text, confidence) in enumerate(processed_results):\n",
    "        # Skip if text is too short\n",
    "        if len(text) < 3:\n",
    "            continue\n",
    "            \n",
    "        # Skip if text is likely a form label\n",
    "        if is_form_label(text):\n",
    "            form_label_penalty = 0.5\n",
    "        else:\n",
    "            form_label_penalty = 1.0\n",
    "            \n",
    "        # Skip if text is likely legal text\n",
    "        if is_legal_text(text):\n",
    "            legal_text_penalty = 0.3\n",
    "        else:\n",
    "            legal_text_penalty = 1.0\n",
    "        \n",
    "        try:\n",
    "            # Get text vector\n",
    "            text_vector = texts_matrix[i]\n",
    "            \n",
    "            # Compare with category examples\n",
    "            scores = {}\n",
    "            for category, (start_idx, end_idx) in category_indices.items():\n",
    "                # Calculate cosine similarity with each example in the category\n",
    "                category_vectors = examples_matrix[start_idx:end_idx]\n",
    "                similarities = cosine_similarity(text_vector, category_vectors)[0]\n",
    "                scores[category] = np.max(similarities) if len(similarities) > 0 else 0\n",
    "            \n",
    "            # Find best matching category\n",
    "            best_category = max(scores.items(), key=lambda x: x[1]) if scores else (\"unknown\", 0)\n",
    "            \n",
    "            # Calculate contextual relevance\n",
    "            context_score = calculate_contextual_relevance(text, bbox, processed_results)\n",
    "            \n",
    "            # Final score combines similarity, contextual relevance, and penalties\n",
    "            total_score = (best_category[1] * 0.6 + context_score * 0.4) * form_label_penalty * legal_text_penalty * confidence\n",
    "            \n",
    "            candidates.append({\n",
    "                'text': text,\n",
    "                'score': total_score,\n",
    "                'category': best_category[0],\n",
    "                'semantic_score': best_category[1],\n",
    "                'context_score': context_score,\n",
    "                'bbox': bbox,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"Error processing text '{text}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Filter and rank candidates\n",
    "    if not candidates:\n",
    "        if debug:\n",
    "            print(\"No event type candidates found\")\n",
    "        return \"\", []\n",
    "        \n",
    "    # Remove duplicates, keeping highest scoring version\n",
    "    unique_candidates = {}\n",
    "    for candidate in candidates:\n",
    "        text = candidate['text'].lower()\n",
    "        if text not in unique_candidates or candidate['score'] > unique_candidates[text]['score']:\n",
    "            unique_candidates[text] = candidate\n",
    "            \n",
    "    # Convert back to list and sort by score\n",
    "    final_candidates = list(unique_candidates.values())\n",
    "    final_candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Apply minimum threshold and validation for actual selection\n",
    "    valid_candidates = []\n",
    "    for candidate in final_candidates:\n",
    "        # Skip if score is too low\n",
    "        if candidate['score'] < 0.2:\n",
    "            continue\n",
    "            \n",
    "        # Skip if semantic score is too low\n",
    "        if candidate['semantic_score'] < SEMANTIC_THRESHOLD:\n",
    "            continue\n",
    "            \n",
    "        # Skip if this is the serial number\n",
    "        if serial_number and candidate['text'] == serial_number:\n",
    "            continue\n",
    "            \n",
    "        # Skip if it looks like a serial number (alphanumeric with no spaces)\n",
    "        if re.match(r'^[A-Z0-9]+$', candidate['text']) and len(candidate['text']) <= 10:\n",
    "            continue\n",
    "            \n",
    "        # Skip if text is too long to be an event type\n",
    "        if len(candidate['text']) > 50:\n",
    "            continue\n",
    "            \n",
    "        valid_candidates.append(candidate)\n",
    "    \n",
    "    # Debug output - filter out candidates with semantic score below threshold for display\n",
    "    if debug:\n",
    "        # Filter candidates for display\n",
    "        display_candidates = [c for c in final_candidates if c['semantic_score'] >= SEMANTIC_THRESHOLD]\n",
    "        \n",
    "        if display_candidates:\n",
    "            print(f\"\\nAll event type candidates ({len(display_candidates)}):\")\n",
    "            for i, candidate in enumerate(display_candidates):\n",
    "                print(f\"  {i+1}. '{candidate['text']}' - Score: {candidate['score']:.3f} \"\n",
    "                      f\"(Category: {candidate['category']}, Semantic: {candidate['semantic_score']:.3f}, \"\n",
    "                      f\"Context: {candidate['context_score']:.3f}, Confidence: {candidate['confidence']:.2f})\")\n",
    "        else:\n",
    "            print(\"No valid event type candidates found\")\n",
    "    \n",
    "    # Return best candidate or empty string if none found\n",
    "    return valid_candidates[0]['text'] if valid_candidates else \"\", final_candidates\n",
    "\n",
    "\n",
    "# Test the event type extraction on ALL images\n",
    "print(\"\\n===== TESTING EVENT TYPE EXTRACTION ON ALL IMAGES =====\")\n",
    "\n",
    "# Initialize results storage\n",
    "event_type_results = []\n",
    "\n",
    "if file_list:\n",
    "    for test_file in file_list:\n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        print(f\"Processing: {test_file}\")\n",
    "        test_path = os.path.join(test_folder, test_file)\n",
    "        \n",
    "        # Load and process image\n",
    "        image = cv2.imread(test_path)\n",
    "        if image is None:\n",
    "            print(f\"Could not read image: {test_file}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Dimensions: {image.shape[1]}x{image.shape[0]}\")\n",
    "        \n",
    "        # Perform OCR\n",
    "        ocr_results = ocr.ocr(image, cls=True)\n",
    "        if not ocr_results or not ocr_results[0]:\n",
    "            print(f\"  No text detected\")\n",
    "            continue\n",
    "            \n",
    "        # Process OCR results\n",
    "        processed_results = []\n",
    "        all_text = []\n",
    "        \n",
    "        for line in ocr_results[0]:\n",
    "            bbox, (text, confidence) = line\n",
    "            text = combine_spaced_alphanumeric(text.strip())\n",
    "            processed_results.append((bbox, text, confidence))\n",
    "            all_text.append(text)\n",
    "            \n",
    "        all_text_combined = \" \".join(all_text)\n",
    "        print(f\"  OCR extracted {len(all_text_combined)} characters\")\n",
    "        \n",
    "        # Get serial number if it was previously extracted\n",
    "        serial_number = None\n",
    "        for result in csv_results:\n",
    "            if result[5] == test_file:  # Check if filename matches\n",
    "                serial_number = result[0]  # Get serial number\n",
    "                break\n",
    "        \n",
    "        # Extract event type\n",
    "        event_type, all_candidates = find_event_type(processed_results, all_text_combined, serial_number, debug=True)\n",
    "        print(f\"\\nExtracted Event Type: '{event_type}'\")\n",
    "        \n",
    "        # Store result\n",
    "        event_type_results.append({\n",
    "            'filename': test_file,\n",
    "            'event_type': event_type\n",
    "        })\n",
    "        \n",
    "        # Visualize the result with colored bounding boxes\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img_rgb)\n",
    "        \n",
    "        # Get all candidate texts for visualization\n",
    "        candidate_texts = [candidate['text'] for candidate in all_candidates if candidate['bbox'] is not None and candidate['semantic_score'] >= 0.3]\n",
    "        \n",
    "        for bbox, text, confidence in processed_results:\n",
    "            # Determine box color based on event type detection - FIXED to use exact match only\n",
    "            if event_type and (text.strip() == event_type or text.strip().lower() == event_type.lower()):\n",
    "                color, linewidth = \"red\", 3  # Best candidate\n",
    "            elif any(candidate == text.strip() for candidate in candidate_texts):\n",
    "                color, linewidth = \"blue\", 2  # Other candidates\n",
    "            else:\n",
    "                color, linewidth = \"gray\", 1  # Regular text\n",
    "                \n",
    "            # Draw bounding box\n",
    "            points = np.array(bbox, dtype=np.int32)\n",
    "            plt.plot(\n",
    "                [points[0][0], points[1][0], points[2][0], points[3][0], points[0][0]],\n",
    "                [points[0][1], points[1][1], points[2][1], points[3][1], points[0][1]],\n",
    "                color=color,\n",
    "                linewidth=linewidth,\n",
    "            )\n",
    "            \n",
    "            # Add text label\n",
    "            plt.text(\n",
    "                points[0][0],\n",
    "                points[0][1] - 5,\n",
    "                f\"{text} ({confidence:.2f})\",\n",
    "                fontsize=8,\n",
    "                color=color,\n",
    "                weight=\"bold\",\n",
    "            )\n",
    "            \n",
    "        plt.title(f\"Event Type Detection: {test_file}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Display summary of results\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"EVENT TYPE EXTRACTION SUMMARY\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    print(f\"Processed {len(event_type_results)} images\")\n",
    "    print(f\"Found event types in {sum(1 for r in event_type_results if r['event_type'])} images\")\n",
    "    \n",
    "    # Display table of results\n",
    "    if event_type_results:\n",
    "        results_df = pd.DataFrame(event_type_results)\n",
    "        print(\"\\nExtracted Event Types:\")\n",
    "        print(results_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No test images available for event type extraction testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f0e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6.4: Event Date Extraction Implementation\n",
    "\n",
    "\"\"\"\n",
    "Implementation of the event date extraction function.\n",
    "Uses proximity to event_type and date pattern recognition.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import dateutil.parser\n",
    "from datetime import datetime\n",
    "\n",
    "def is_valid_date_format(text):\n",
    "    \"\"\"Check if text matches common date formats.\"\"\"\n",
    "    # Common date patterns\n",
    "    date_patterns = [\n",
    "        r'\\d{1,2}[-/\\.]\\d{1,2}[-/\\.]\\d{2,4}',  # MM/DD/YYYY, DD/MM/YYYY, etc.\n",
    "        r'\\d{1,2}[-/\\.]\\d{2,4}',  # MM/YY, MM/YYYY\n",
    "        r'\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\.?\\s+\\d{1,2},?\\s+\\d{2,4}\\b',  # January 1, 2020\n",
    "        r'\\b\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\.?\\s+\\d{2,4}\\b',  # 1 January 2020\n",
    "        r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},?\\s+\\d{2,4}\\b',\n",
    "        r'\\b\\d{1,2}\\s+(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{2,4}\\b',\n",
    "    ]\n",
    "    \n",
    "    for pattern in date_patterns:\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def extract_date_from_text(text):\n",
    "    \"\"\"Extract date from text using dateutil parser.\"\"\"\n",
    "    try:\n",
    "        # Try to parse the date\n",
    "        date = dateutil.parser.parse(text, fuzzy=True)\n",
    "        \n",
    "        # Check if year is reasonable (not future date)\n",
    "        current_year = datetime.now().year\n",
    "        if date.year > current_year + 1:\n",
    "            # Adjust 2-digit years that were interpreted as future\n",
    "            if date.year - 2000 > current_year:\n",
    "                date = date.replace(year=date.year - 100)\n",
    "        \n",
    "        # Format date consistently\n",
    "        return date.strftime(\"%m/%d/%Y\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def find_date_labels(processed_results):\n",
    "    \"\"\"Find text elements that are likely date labels.\"\"\"\n",
    "    date_labels = []\n",
    "    date_keywords = [\"date\", \"occurred\", \"reported\", \"discovered\", \"incident\", \"event\"]\n",
    "    \n",
    "    for bbox, text, confidence in processed_results:\n",
    "        text_lower = text.lower()\n",
    "        if any(keyword in text_lower for keyword in date_keywords):\n",
    "            if \"date\" in text_lower or \":\" in text:\n",
    "                date_labels.append((bbox, text, confidence))\n",
    "    \n",
    "    return date_labels\n",
    "\n",
    "def calculate_distance(bbox1, bbox2):\n",
    "    \"\"\"Calculate distance between two bounding boxes.\"\"\"\n",
    "    center1_x = sum([point[0] for point in bbox1]) / len(bbox1)\n",
    "    center1_y = sum([point[1] for point in bbox1]) / len(bbox1)\n",
    "    center2_x = sum([point[0] for point in bbox2]) / len(bbox2)\n",
    "    center2_y = sum([point[1] for point in bbox2]) / len(bbox2)\n",
    "    return ((center1_x - center2_x) ** 2 + (center1_y - center2_y) ** 2) ** 0.5\n",
    "\n",
    "def find_event_date(processed_results, event_type_bbox=None, debug=True):\n",
    "    \"\"\"\n",
    "    Extract event date using proximity to event_type and date pattern recognition.\n",
    "    \n",
    "    Args:\n",
    "        processed_results: List of (bbox, text, confidence) tuples from OCR\n",
    "        event_type_bbox: Bounding box of the detected event type\n",
    "        debug: Whether to print debug information\n",
    "        \n",
    "    Returns:\n",
    "        Extracted date string in MM/DD/YYYY format, or empty string if not found\n",
    "    \"\"\"\n",
    "    # Skip if no event_type was found\n",
    "    if event_type_bbox is None:\n",
    "        if debug:\n",
    "            print(\"No event_type found, skipping event_date extraction\")\n",
    "        return \"\", []\n",
    "    \n",
    "    # Find date labels\n",
    "    date_labels = find_date_labels(processed_results)\n",
    "    \n",
    "    # Find text elements that match date patterns\n",
    "    date_candidates = []\n",
    "    \n",
    "    # Distance threshold for severe penalty\n",
    "    DISTANCE_THRESHOLD = 2000\n",
    "    \n",
    "    # First pass: find explicit dates\n",
    "    for bbox, text, confidence in processed_results:\n",
    "        # Skip very short text\n",
    "        if len(text) < 4:\n",
    "            continue\n",
    "            \n",
    "        # Check if text matches date pattern\n",
    "        if is_valid_date_format(text):\n",
    "            # Extract date\n",
    "            date_str = extract_date_from_text(text)\n",
    "            if date_str:\n",
    "                # Calculate distance to event_type\n",
    "                distance_to_event = calculate_distance(bbox, event_type_bbox)\n",
    "                \n",
    "                # Calculate distance to nearest date label\n",
    "                min_label_distance = float('inf')\n",
    "                nearest_label = None\n",
    "                for label_bbox, label_text, _ in date_labels:\n",
    "                    dist = calculate_distance(bbox, label_bbox)\n",
    "                    if dist < min_label_distance:\n",
    "                        min_label_distance = dist\n",
    "                        nearest_label = label_text\n",
    "                \n",
    "                # Calculate score based on distances with threshold penalty\n",
    "                if distance_to_event < DISTANCE_THRESHOLD:\n",
    "                    # Normal scoring for dates within threshold\n",
    "                    event_proximity_score = max(0, 1 - (distance_to_event / DISTANCE_THRESHOLD))\n",
    "                else:\n",
    "                    # Severe penalty for dates beyond threshold\n",
    "                    event_proximity_score = max(0, 0.1 - (distance_to_event - DISTANCE_THRESHOLD) / 1000)\n",
    "                \n",
    "                # Label proximity is secondary\n",
    "                label_proximity_score = max(0, 1 - (min_label_distance / 500)) if nearest_label else 0\n",
    "                \n",
    "                # Combine scores with much higher weight (0.9) to event proximity\n",
    "                total_score = (event_proximity_score * 0.9 + label_proximity_score * 0.1) * confidence\n",
    "                \n",
    "                date_candidates.append({\n",
    "                    'date': date_str,\n",
    "                    'text': text,\n",
    "                    'score': total_score,\n",
    "                    'bbox': bbox,\n",
    "                    'confidence': confidence,\n",
    "                    'distance_to_event': distance_to_event,\n",
    "                    'nearest_label': nearest_label,\n",
    "                    'label_distance': min_label_distance\n",
    "                })\n",
    "    \n",
    "    # Second pass: look for date fields near date labels\n",
    "    for label_bbox, label_text, _ in date_labels:\n",
    "        for bbox, text, confidence in processed_results:\n",
    "            # Skip if already processed as a date\n",
    "            if any(c['text'] == text for c in date_candidates):\n",
    "                continue\n",
    "                \n",
    "            # Check proximity to label\n",
    "            distance = calculate_distance(bbox, label_bbox)\n",
    "            if distance < 200:  # Close to label\n",
    "                # Try to extract date\n",
    "                date_str = extract_date_from_text(text)\n",
    "                if date_str:\n",
    "                    # Calculate distance to event_type\n",
    "                    distance_to_event = calculate_distance(bbox, event_type_bbox)\n",
    "                    \n",
    "                    # Calculate score based on distances with threshold penalty\n",
    "                    if distance_to_event < DISTANCE_THRESHOLD:\n",
    "                        # Normal scoring for dates within threshold\n",
    "                        event_proximity_score = max(0, 1 - (distance_to_event / DISTANCE_THRESHOLD))\n",
    "                    else:\n",
    "                        # Severe penalty for dates beyond threshold\n",
    "                        event_proximity_score = max(0, 0.1 - (distance_to_event - DISTANCE_THRESHOLD) / 1000)\n",
    "                    \n",
    "                    label_proximity_score = max(0, 1 - (distance / 500))\n",
    "                    \n",
    "                    # Combine scores with much higher weight (0.9) to event proximity\n",
    "                    total_score = (event_proximity_score * 0.9 + label_proximity_score * 0.1) * confidence\n",
    "                    \n",
    "                    date_candidates.append({\n",
    "                        'date': date_str,\n",
    "                        'text': text,\n",
    "                        'score': total_score,\n",
    "                        'bbox': bbox,\n",
    "                        'confidence': confidence,\n",
    "                        'distance_to_event': distance_to_event,\n",
    "                        'nearest_label': label_text,\n",
    "                        'label_distance': distance\n",
    "                    })\n",
    "    \n",
    "    # Sort candidates by score\n",
    "    date_candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Debug output\n",
    "    if debug and date_candidates:\n",
    "        print(f\"\\nEvent date candidates ({len(date_candidates)}):\")\n",
    "        for i, candidate in enumerate(date_candidates[:5]):  # Show top 5\n",
    "            print(f\"  {i+1}. '{candidate['date']}' (from '{candidate['text']}') - Score: {candidate['score']:.3f}, \"\n",
    "                  f\"Distance to event: {candidate['distance_to_event']:.1f}, \"\n",
    "                  f\"Label: '{candidate['nearest_label']}' ({candidate['label_distance']:.1f})\")\n",
    "    \n",
    "    # Return best candidate\n",
    "    return date_candidates[0]['date'] if date_candidates else \"\", date_candidates\n",
    "\n",
    "\n",
    "# Test the event date extraction on ALL images\n",
    "print(\"\\n===== TESTING EVENT DATE EXTRACTION ON ALL IMAGES =====\")\n",
    "\n",
    "# Initialize results storage\n",
    "event_date_results = []\n",
    "\n",
    "if file_list:\n",
    "    for test_file in file_list:\n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        print(f\"Processing: {test_file}\")\n",
    "        test_path = os.path.join(test_folder, test_file)\n",
    "        \n",
    "        # Load and process image\n",
    "        image = cv2.imread(test_path)\n",
    "        if image is None:\n",
    "            print(f\"Could not read image: {test_file}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Dimensions: {image.shape[1]}x{image.shape[0]}\")\n",
    "        \n",
    "        # Perform OCR\n",
    "        ocr_results = ocr.ocr(image, cls=True)\n",
    "        if not ocr_results or not ocr_results[0]:\n",
    "            print(f\"  No text detected\")\n",
    "            continue\n",
    "            \n",
    "        # Process OCR results\n",
    "        processed_results = []\n",
    "        all_text = []\n",
    "        \n",
    "        for line in ocr_results[0]:\n",
    "            bbox, (text, confidence) = line\n",
    "            text = combine_spaced_alphanumeric(text.strip())\n",
    "            processed_results.append((bbox, text, confidence))\n",
    "            all_text.append(text)\n",
    "            \n",
    "        all_text_combined = \" \".join(all_text)\n",
    "        print(f\"  OCR extracted {len(all_text_combined)} characters\")\n",
    "        \n",
    "        # First extract event type\n",
    "        event_type, event_type_candidates = find_event_type(processed_results, all_text_combined, debug=False)\n",
    "        \n",
    "        # Get event type bbox if found\n",
    "        event_type_bbox = None\n",
    "        if event_type:\n",
    "            print(f\"Event Type: '{event_type}'\")\n",
    "            # Find the bbox for the event type\n",
    "            for bbox, text, _ in processed_results:\n",
    "                if text.strip() == event_type:\n",
    "                    event_type_bbox = bbox\n",
    "                    break\n",
    "        else:\n",
    "            print(\"No event type found\")\n",
    "        \n",
    "        # Extract event date only if event type was found\n",
    "        if event_type_bbox:\n",
    "            event_date, date_candidates = find_event_date(processed_results, event_type_bbox, debug=True)\n",
    "            print(f\"\\nExtracted Event Date: '{event_date}'\")\n",
    "        else:\n",
    "            event_date = \"\"\n",
    "            date_candidates = []\n",
    "            print(\"\\nSkipping event date extraction (no event type found)\")\n",
    "        \n",
    "        # Store result\n",
    "        event_date_results.append({\n",
    "            'filename': test_file,\n",
    "            'event_type': event_type,\n",
    "            'event_date': event_date\n",
    "        })\n",
    "        \n",
    "        # Visualize the result with colored bounding boxes\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img_rgb)\n",
    "        \n",
    "        # Get all candidate texts for visualization\n",
    "        date_texts = [candidate['text'] for candidate in date_candidates if candidate['bbox'] is not None]\n",
    "        \n",
    "        for bbox, text, confidence in processed_results:\n",
    "            # Determine box color based on detection\n",
    "            if event_type and text.strip() == event_type:\n",
    "                color, linewidth = \"red\", 3  # Event type\n",
    "            elif event_date and text.strip() == event_date:\n",
    "                color, linewidth = \"green\", 3  # Event date\n",
    "            elif any(text.strip() == date_text for date_text in date_texts):\n",
    "                color, linewidth = \"blue\", 2  # Other date candidates\n",
    "            else:\n",
    "                color, linewidth = \"gray\", 1  # Regular text\n",
    "                \n",
    "            # Draw bounding box\n",
    "            points = np.array(bbox, dtype=np.int32)\n",
    "            plt.plot(\n",
    "                [points[0][0], points[1][0], points[2][0], points[3][0], points[0][0]],\n",
    "                [points[0][1], points[1][1], points[2][1], points[3][1], points[0][1]],\n",
    "                color=color,\n",
    "                linewidth=linewidth,\n",
    "            )\n",
    "            \n",
    "            # Add text label\n",
    "            plt.text(\n",
    "                points[0][0],\n",
    "                points[0][1] - 5,\n",
    "                f\"{text} ({confidence:.2f})\",\n",
    "                fontsize=8,\n",
    "                color=color,\n",
    "                weight=\"bold\",\n",
    "            )\n",
    "            \n",
    "        plt.title(f\"Event Date Detection: {test_file}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Display summary of results\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"EVENT DATE EXTRACTION SUMMARY\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    print(f\"Processed {len(event_date_results)} images\")\n",
    "    print(f\"Found event dates in {sum(1 for r in event_date_results if r['event_date'])} images\")\n",
    "    \n",
    "    # Display table of results\n",
    "    if event_date_results:\n",
    "        results_df = pd.DataFrame(event_date_results)\n",
    "        print(\"\\nExtracted Event Dates:\")\n",
    "        print(results_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No test images available for event date extraction testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6.5: Associated Name Extraction Implementation\n",
    "\n",
    "\"\"\"\n",
    "Implementation of the associated name extraction function.\n",
    "Identifies person names associated with the event (reporter or owner).\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def is_person_name(text):\n",
    "    \"\"\"Check if text is likely a person's name.\"\"\"\n",
    "    # Common name patterns\n",
    "    name_patterns = [\n",
    "        r'^[A-Z][a-z]+\\s+[A-Z][a-z]+$',  # First Last\n",
    "        r'^[A-Z][a-z]+\\s+[A-Z]\\.\\s*[A-Z][a-z]+$',  # First M. Last\n",
    "        r'^[A-Z][a-z]+\\s+[A-Z][a-z]+\\s+[A-Z][a-z]+$',  # First Middle Last\n",
    "        r'^[A-Z][a-z]+,\\s*[A-Z][a-z]+$',  # Last, First\n",
    "        r'^[A-Z][a-z]+,\\s*[A-Z][a-z]+\\s+[A-Z][a-z]+$',  # Last, First Middle\n",
    "        r'^[A-Z][a-z]+,\\s*[A-Z][a-z]+\\s+[A-Z]\\.$',  # Last, First M.\n",
    "        r'^[A-Z]+\\s+[A-Z]+$',  # FIRST LAST (all caps)\n",
    "        r'^[A-Z]+,\\s*[A-Z]+$',  # LAST, FIRST (all caps)\n",
    "        r'^[A-Z]+,\\s*[A-Z]+\\s+[A-Z]+$',  # LAST, FIRST MIDDLE (all caps)\n",
    "        r'^[A-Z]+,\\s*[A-Z]+\\s+[A-Z]\\.?$',  # LAST, FIRST M. (all caps)\n",
    "    ]\n",
    "    \n",
    "    for pattern in name_patterns:\n",
    "        if re.match(pattern, text):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def contains_name_indicator(text):\n",
    "    \"\"\"Check if text contains indicators of a name field.\"\"\"\n",
    "    name_indicators = [\"name\", \"reported by\", \"owner\", \"licensee\", \"person\", \"signature\", \"signed\"]\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for indicator in name_indicators:\n",
    "        if indicator in text_lower:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def find_name_labels(processed_results):\n",
    "    \"\"\"Find text elements that are likely name labels.\"\"\"\n",
    "    name_labels = []\n",
    "    name_indicators = [\"full name\", \"person making report\", \"reported by\", \"owner\", \n",
    "                      \"licensee\", \"person\", \"signature of\", \"signed by\"]\n",
    "    \n",
    "    for bbox, text, confidence in processed_results:\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check if text contains name indicators\n",
    "        if any(indicator in text_lower for indicator in name_indicators):\n",
    "            name_labels.append((bbox, text, confidence))\n",
    "    \n",
    "    return name_labels\n",
    "\n",
    "def calculate_distance(bbox1, bbox2):\n",
    "    \"\"\"Calculate distance between two bounding boxes.\"\"\"\n",
    "    center1_x = sum([point[0] for point in bbox1]) / len(bbox1)\n",
    "    center1_y = sum([point[1] for point in bbox1]) / len(bbox1)\n",
    "    center2_x = sum([point[0] for point in bbox2]) / len(bbox2)\n",
    "    center2_y = sum([point[1] for point in bbox2]) / len(bbox2)\n",
    "    return ((center1_x - center2_x) ** 2 + (center1_y - center2_y) ** 2) ** 0.5\n",
    "\n",
    "def is_likely_not_a_name(text):\n",
    "    \"\"\"Check if text is likely not a person name.\"\"\"\n",
    "    # Common non-name patterns\n",
    "    non_name_indicators = [\n",
    "        \"missing\", \"inventory\", \"theft\", \"loss\", \"report\", \"form\", \"license\", \n",
    "        \"serial\", \"number\", \"date\", \"address\", \"phone\", \"fax\", \"email\",\n",
    "        \"llc\", \"inc\", \"corporation\", \"company\", \"department\", \"bureau\",\n",
    "        \"zip\", \"code\", \"city\", \"state\", \"street\", \"tower\", \"bridge\", \"services\",\n",
    "        \"security\", \"universal\", \"police\", \"notification\", \"signature\", \"felony\"\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Check for non-name indicators\n",
    "    for indicator in non_name_indicators:\n",
    "        if indicator in text_lower:\n",
    "            return True\n",
    "    \n",
    "    # Check for numeric content\n",
    "    if re.search(r'\\d', text):\n",
    "        return True\n",
    "    \n",
    "    # Check for very short text\n",
    "    if len(text.split()) < 2:\n",
    "        return True\n",
    "    \n",
    "    # Check for very long text\n",
    "    if len(text.split()) > 5:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def is_form_field_label(text):\n",
    "    \"\"\"Check if text is likely a form field label.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Common form field label indicators\n",
    "    form_field_indicators = [\"name:\", \"address:\", \"code:\", \"number:\", \"telephone\", \n",
    "                           \"signature\", \"street\", \"city\", \"state\", \"zip\", \"date\"]\n",
    "    \n",
    "    # Check for form field label indicators\n",
    "    for indicator in form_field_indicators:\n",
    "        if indicator in text_lower:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def find_associated_name(processed_results, event_type_bbox=None, debug=True):\n",
    "    \"\"\"\n",
    "    Extract associated name (person reporting the event or owner).\n",
    "    \n",
    "    Args:\n",
    "        processed_results: List of (bbox, text, confidence) tuples from OCR\n",
    "        event_type_bbox: Bounding box of the detected event type\n",
    "        debug: Whether to print debug information\n",
    "        \n",
    "    Returns:\n",
    "        Extracted name string, or empty string if not found\n",
    "    \"\"\"\n",
    "    # Find name labels\n",
    "    name_labels = find_name_labels(processed_results)\n",
    "    \n",
    "    # Find text elements that are likely names\n",
    "    name_candidates = []\n",
    "    person_name_candidates = []\n",
    "    \n",
    "    # First pass: find explicit names\n",
    "    for bbox, text, confidence in processed_results:\n",
    "        # Skip very short text\n",
    "        if len(text) < 4:\n",
    "            continue\n",
    "            \n",
    "        # Skip if likely not a name\n",
    "        if is_likely_not_a_name(text):\n",
    "            continue\n",
    "            \n",
    "        # Skip if it's a form field label\n",
    "        if is_form_field_label(text):\n",
    "            continue\n",
    "            \n",
    "        # Check if text matches name pattern\n",
    "        is_name = is_person_name(text)\n",
    "        \n",
    "        # Calculate distance to nearest name label\n",
    "        min_label_distance = float('inf')\n",
    "        nearest_label = None\n",
    "        for label_bbox, label_text, _ in name_labels:\n",
    "            dist = calculate_distance(bbox, label_bbox)\n",
    "            if dist < min_label_distance:\n",
    "                min_label_distance = dist\n",
    "                nearest_label = label_text\n",
    "        \n",
    "        # Calculate distance to event type if available\n",
    "        if event_type_bbox:\n",
    "            distance_to_event = calculate_distance(bbox, event_type_bbox)\n",
    "        else:\n",
    "            distance_to_event = float('inf')\n",
    "        \n",
    "        # Calculate score based on distances\n",
    "        label_proximity_score = max(0, 1 - (min_label_distance / 500)) if nearest_label else 0\n",
    "        \n",
    "        # Event type proximity is less important for names\n",
    "        if event_type_bbox:\n",
    "            event_proximity_score = max(0, 1 - (distance_to_event / 2000))\n",
    "        else:\n",
    "            event_proximity_score = 0\n",
    "        \n",
    "        # Apply person name bonus - this is the key change\n",
    "        person_name_bonus = 2.0 if is_name else 0.3\n",
    "        \n",
    "        # Combine scores with person name bonus\n",
    "        total_score = (label_proximity_score * 0.8 + event_proximity_score * 0.2) * confidence * person_name_bonus\n",
    "        \n",
    "        candidate = {\n",
    "            'name': text,\n",
    "            'score': total_score,\n",
    "            'bbox': bbox,\n",
    "            'confidence': confidence,\n",
    "            'nearest_label': nearest_label,\n",
    "            'label_distance': min_label_distance,\n",
    "            'distance_to_event': distance_to_event if event_type_bbox else None,\n",
    "            'is_person_name': is_name\n",
    "        }\n",
    "        \n",
    "        name_candidates.append(candidate)\n",
    "        \n",
    "        # Keep track of person name candidates separately\n",
    "        if is_name:\n",
    "            person_name_candidates.append(candidate)\n",
    "    \n",
    "    # Second pass: look for names near name labels\n",
    "    for label_bbox, label_text, _ in name_labels:\n",
    "        for bbox, text, confidence in processed_results:\n",
    "            # Skip if already processed as a name\n",
    "            if any(c['name'] == text for c in name_candidates):\n",
    "                continue\n",
    "                \n",
    "            # Skip if likely not a name\n",
    "            if is_likely_not_a_name(text):\n",
    "                continue\n",
    "                \n",
    "            # Skip if it's a form field label\n",
    "            if is_form_field_label(text):\n",
    "                continue\n",
    "                \n",
    "            # Check proximity to label\n",
    "            distance = calculate_distance(bbox, label_bbox)\n",
    "            if distance < 200:  # Close to label\n",
    "                # Check if text matches name pattern\n",
    "                is_name = is_person_name(text)\n",
    "                \n",
    "                # Calculate distance to event type if available\n",
    "                if event_type_bbox:\n",
    "                    distance_to_event = calculate_distance(bbox, event_type_bbox)\n",
    "                else:\n",
    "                    distance_to_event = float('inf')\n",
    "                \n",
    "                # Calculate score based on distances\n",
    "                label_proximity_score = max(0, 1 - (distance / 500))\n",
    "                \n",
    "                # Event type proximity is less important for names\n",
    "                if event_type_bbox:\n",
    "                    event_proximity_score = max(0, 1 - (distance_to_event / 2000))\n",
    "                else:\n",
    "                    event_proximity_score = 0\n",
    "                \n",
    "                # Apply person name bonus - this is the key change\n",
    "                person_name_bonus = 2.0 if is_name else 0.3\n",
    "                \n",
    "                # Combine scores with person name bonus\n",
    "                total_score = (label_proximity_score * 0.8 + event_proximity_score * 0.2) * confidence * person_name_bonus\n",
    "                \n",
    "                candidate = {\n",
    "                    'name': text,\n",
    "                    'score': total_score,\n",
    "                    'bbox': bbox,\n",
    "                    'confidence': confidence,\n",
    "                    'nearest_label': label_text,\n",
    "                    'label_distance': distance,\n",
    "                    'distance_to_event': distance_to_event if event_type_bbox else None,\n",
    "                    'is_person_name': is_name\n",
    "                }\n",
    "                \n",
    "                name_candidates.append(candidate)\n",
    "                \n",
    "                # Keep track of person name candidates separately\n",
    "                if is_name:\n",
    "                    person_name_candidates.append(candidate)\n",
    "    \n",
    "    # Sort candidates by score\n",
    "    name_candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Sort person name candidates by score\n",
    "    person_name_candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Debug output\n",
    "    if debug and name_candidates:\n",
    "        print(f\"\\nAssociated name candidates ({len(name_candidates)}):\")\n",
    "        for i, candidate in enumerate(name_candidates[:5]):  # Show top 5\n",
    "            event_dist = f\", Distance to event: {candidate['distance_to_event']:.1f}\" if candidate['distance_to_event'] else \"\"\n",
    "            person_name = \", Is person name: Yes\" if candidate['is_person_name'] else \", Is person name: No\"\n",
    "            print(f\"  {i+1}. '{candidate['name']}' - Score: {candidate['score']:.3f}{event_dist}, \"\n",
    "                  f\"Label: '{candidate['nearest_label']}' ({candidate['label_distance']:.1f}){person_name}\")\n",
    "    elif debug:\n",
    "        print(\"No associated name candidates found\")\n",
    "    \n",
    "    # IMPORTANT: Prioritize person name candidates if available\n",
    "    if person_name_candidates:\n",
    "        return person_name_candidates[0]['name'], name_candidates\n",
    "    elif name_candidates:\n",
    "        return name_candidates[0]['name'], name_candidates\n",
    "    else:\n",
    "        return \"\", name_candidates\n",
    "\n",
    "\n",
    "# Test the associated name extraction on ALL images\n",
    "print(\"\\n===== TESTING ASSOCIATED NAME EXTRACTION ON ALL IMAGES =====\")\n",
    "\n",
    "# Initialize results storage\n",
    "associated_name_results = []\n",
    "\n",
    "if file_list:\n",
    "    for test_file in file_list:\n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        print(f\"Processing: {test_file}\")\n",
    "        test_path = os.path.join(test_folder, test_file)\n",
    "        \n",
    "        # Load and process image\n",
    "        image = cv2.imread(test_path)\n",
    "        if image is None:\n",
    "            print(f\"Could not read image: {test_file}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Dimensions: {image.shape[1]}x{image.shape[0]}\")\n",
    "        \n",
    "        # Perform OCR\n",
    "        ocr_results = ocr.ocr(image, cls=True)\n",
    "        if not ocr_results or not ocr_results[0]:\n",
    "            print(f\"  No text detected\")\n",
    "            continue\n",
    "            \n",
    "        # Process OCR results\n",
    "        processed_results = []\n",
    "        all_text = []\n",
    "        \n",
    "        for line in ocr_results[0]:\n",
    "            bbox, (text, confidence) = line\n",
    "            text = combine_spaced_alphanumeric(text.strip())\n",
    "            processed_results.append((bbox, text, confidence))\n",
    "            all_text.append(text)\n",
    "            \n",
    "        all_text_combined = \" \".join(all_text)\n",
    "        print(f\"  OCR extracted {len(all_text_combined)} characters\")\n",
    "        \n",
    "        # First extract event type to use as reference\n",
    "        event_type, event_type_candidates = find_event_type(processed_results, all_text_combined, debug=False)\n",
    "        \n",
    "        # Get event type bbox if found\n",
    "        event_type_bbox = None\n",
    "        if event_type:\n",
    "            print(f\"Event Type: '{event_type}'\")\n",
    "            # Find the bbox for the event type\n",
    "            for bbox, text, _ in processed_results:\n",
    "                if text.strip() == event_type:\n",
    "                    event_type_bbox = bbox\n",
    "                    break\n",
    "        else:\n",
    "            print(\"No event type found\")\n",
    "        \n",
    "        # Extract associated name\n",
    "        associated_name, name_candidates = find_associated_name(processed_results, event_type_bbox, debug=True)\n",
    "        print(f\"\\nExtracted Associated Name: '{associated_name}'\")\n",
    "        \n",
    "        # Store result\n",
    "        associated_name_results.append({\n",
    "            'filename': test_file,\n",
    "            'event_type': event_type,\n",
    "            'associated_name': associated_name\n",
    "        })\n",
    "        \n",
    "        # Visualize the result with colored bounding boxes\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img_rgb)\n",
    "        \n",
    "        # Get all candidate texts for visualization\n",
    "        name_texts = [candidate['name'] for candidate in name_candidates if candidate['bbox'] is not None]\n",
    "        \n",
    "        for bbox, text, confidence in processed_results:\n",
    "            # Determine box color based on detection\n",
    "            if associated_name and text.strip() == associated_name:\n",
    "                color, linewidth = \"red\", 3  # Associated name (selected candidate)\n",
    "            elif any(text.strip() == name_text for name_text in name_texts):\n",
    "                color, linewidth = \"blue\", 2  # Other name candidates\n",
    "            else:\n",
    "                color, linewidth = \"gray\", 1  # Regular text\n",
    "                \n",
    "            # Draw bounding box\n",
    "            points = np.array(bbox, dtype=np.int32)\n",
    "            plt.plot(\n",
    "                [points[0][0], points[1][0], points[2][0], points[3][0], points[0][0]],\n",
    "                [points[0][1], points[1][1], points[2][1], points[3][1], points[0][1]],\n",
    "                color=color,\n",
    "                linewidth=linewidth,\n",
    "            )\n",
    "            \n",
    "            # Add text label\n",
    "            plt.text(\n",
    "                points[0][0],\n",
    "                points[0][1] - 5,\n",
    "                f\"{text} ({confidence:.2f})\",\n",
    "                fontsize=8,\n",
    "                color=color,\n",
    "                weight=\"bold\",\n",
    "            )\n",
    "            \n",
    "        plt.title(f\"Associated Name Detection: {test_file}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Display summary of results\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"ASSOCIATED NAME EXTRACTION SUMMARY\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    print(f\"Processed {len(associated_name_results)} images\")\n",
    "    print(f\"Found associated names in {sum(1 for r in associated_name_results if r['associated_name'])} images\")\n",
    "    \n",
    "    # Display table of results\n",
    "    if associated_name_results:\n",
    "        results_df = pd.DataFrame(associated_name_results)\n",
    "        print(\"\\nExtracted Associated Names:\")\n",
    "        print(results_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No test images available for associated name extraction testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a17e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6.6: Associated Address Extraction Implementation\n",
    "\n",
    "\"\"\"\n",
    "Implementation of the associated address extraction function.\n",
    "Identifies the full address (street, city, state, zip) associated with the person.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load spaCy NER model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    # If model not installed, download it\n",
    "    import sys\n",
    "    !{sys.executable} -m spacy download en_core_web_sm\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# [KEEP ALL ORIGINAL ASSOCIATED NAME FUNCTIONS FROM CELL 6.5 EXACTLY AS PROVIDED]\n",
    "# is_person_name, contains_name_indicator, find_name_labels, calculate_distance,\n",
    "# is_likely_not_a_name, is_form_field_label, find_associated_name\n",
    "\n",
    "# Address extraction functions\n",
    "def is_street_address(text):\n",
    "    \"\"\"Check if text is likely a street address.\"\"\"\n",
    "    # Common street address patterns\n",
    "    street_patterns = [\n",
    "        r'\\d+\\s+[A-Za-z]+\\s+[A-Za-z]+',  # 123 Main Street\n",
    "        r'\\d+\\s+[A-Za-z]+\\s+[A-Za-z]+\\s+[A-Za-z]+',  # 123 East Main Street\n",
    "        r'\\d+\\s+[A-Za-z]+',  # 123 Main\n",
    "        r'P\\.?O\\.?\\s+Box\\s+\\d+',  # P.O. Box 123\n",
    "    ]\n",
    "    \n",
    "    # Check for common street suffixes\n",
    "    street_suffixes = ['street', 'st', 'avenue', 'ave', 'road', 'rd', 'boulevard', 'blvd', \n",
    "                      'drive', 'dr', 'lane', 'ln', 'place', 'pl', 'court', 'ct', 'way']\n",
    "    \n",
    "    # Check for street patterns\n",
    "    for pattern in street_patterns:\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            return True\n",
    "    \n",
    "    # Check for street suffixes\n",
    "    for suffix in street_suffixes:\n",
    "        if re.search(r'\\b' + suffix + r'\\b', text, re.IGNORECASE):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def is_state(text):\n",
    "    \"\"\"Check if text is a US state or abbreviation.\"\"\"\n",
    "    # List of US state abbreviations\n",
    "    state_abbrs = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', \n",
    "                  'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', \n",
    "                  'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', \n",
    "                  'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', \n",
    "                  'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "    \n",
    "    # Check if text is a state abbreviation\n",
    "    if text.upper() in state_abbrs:\n",
    "        return True\n",
    "        \n",
    "    # List of US state names\n",
    "    state_names = ['alabama', 'alaska', 'arizona', 'arkansas', 'california', \n",
    "                  'colorado', 'connecticut', 'delaware', 'florida', 'georgia', \n",
    "                  'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', \n",
    "                  'kentucky', 'louisiana', 'maine', 'maryland', 'massachusetts', \n",
    "                  'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', \n",
    "                  'nebraska', 'nevada', 'new hampshire', 'new jersey', 'new mexico', \n",
    "                  'new york', 'north carolina', 'north dakota', 'ohio', 'oklahoma', \n",
    "                  'oregon', 'pennsylvania', 'rhode island', 'south carolina', \n",
    "                  'south dakota', 'tennessee', 'texas', 'utah', 'vermont', \n",
    "                  'virginia', 'washington', 'west virginia', 'wisconsin', 'wyoming']\n",
    "    \n",
    "    # Check if text is a state name\n",
    "    if text.lower() in state_names:\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def is_zipcode(text):\n",
    "    \"\"\"Check if text is a US ZIP code.\"\"\"\n",
    "    # 5-digit ZIP code\n",
    "    if re.match(r'^\\d{5}$', text):\n",
    "        return True\n",
    "        \n",
    "    # 9-digit ZIP+4 code\n",
    "    if re.match(r'^\\d{5}-\\d{4}$', text):\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def find_address_blocks(processed_results):\n",
    "    \"\"\"Group text elements into potential address blocks based on spatial proximity.\"\"\"\n",
    "    # If too few elements, return a single block\n",
    "    if len(processed_results) < 5:\n",
    "        return [processed_results]\n",
    "    \n",
    "    # Initialize address blocks\n",
    "    address_blocks = []\n",
    "    \n",
    "    # Sort by vertical position (y-coordinate)\n",
    "    sorted_results = sorted(processed_results, \n",
    "                           key=lambda x: sum([point[1] for point in x[0]]) / len(x[0]))\n",
    "    \n",
    "    current_block = []\n",
    "    prev_y = None\n",
    "    \n",
    "    # Group by vertical proximity\n",
    "    for bbox, text, confidence in sorted_results:\n",
    "        # Calculate center y-coordinate\n",
    "        center_y = sum([point[1] for point in bbox]) / len(bbox)\n",
    "        \n",
    "        # Start a new block if vertical gap is large\n",
    "        if prev_y is not None and abs(center_y - prev_y) > 100:\n",
    "            if current_block:\n",
    "                address_blocks.append(current_block)\n",
    "                current_block = []\n",
    "        \n",
    "        current_block.append((bbox, text, confidence))\n",
    "        prev_y = center_y\n",
    "    \n",
    "    # Add the last block\n",
    "    if current_block:\n",
    "        address_blocks.append(current_block)\n",
    "    \n",
    "    return address_blocks\n",
    "\n",
    "def find_city_contextual(processed_results, state_candidates, zip_candidates, associated_name_bbox=None, debug=False):\n",
    "    \"\"\"\n",
    "    Extract city name using contextual clues from form structure and spatial relationships.\n",
    "    \n",
    "    Args:\n",
    "        processed_results: List of (bbox, text, confidence) tuples from OCR\n",
    "        state_candidates: List of state candidates\n",
    "        zip_candidates: List of ZIP code candidates\n",
    "        associated_name_bbox: Bounding box of the associated name (optional)\n",
    "        debug: Whether to print debug information\n",
    "        \n",
    "    Returns:\n",
    "        List of city candidates with scores\n",
    "    \"\"\"\n",
    "    city_candidates = []\n",
    "    \n",
    "    # Find city labels\n",
    "    city_labels = []\n",
    "    for bbox, text, confidence in processed_results:\n",
    "        text_lower = text.lower()\n",
    "        if text_lower == 'city' or text_lower == 'city:':\n",
    "            city_labels.append((bbox, text, confidence))\n",
    "            if debug:\n",
    "                print(f\"Found city label: '{text}'\")\n",
    "    \n",
    "    # Group text elements into potential address blocks\n",
    "    address_blocks = find_address_blocks(processed_results)\n",
    "    \n",
    "    # Process each text element\n",
    "    for bbox, text, confidence in processed_results:\n",
    "        # Skip very short text or text with numbers\n",
    "        if len(text) < 3 or re.search(r'\\d', text):\n",
    "            continue\n",
    "            \n",
    "        # Skip form field labels and text with colons\n",
    "        if text.lower() in ['city', 'city:', 'state', 'state:', 'zip', 'zip code', 'zip:'] or ':' in text:\n",
    "            continue\n",
    "        \n",
    "        # Initialize score and reasons\n",
    "        score = 0.0\n",
    "        reasons = []\n",
    "        \n",
    "        # 1. Check proximity to city labels\n",
    "        for label_bbox, label_text, _ in city_labels:\n",
    "            distance = calculate_distance(bbox, label_bbox)\n",
    "            if distance < 200:\n",
    "                # Text near a city label is likely a city\n",
    "                label_score = 5.0 * confidence\n",
    "                score += label_score\n",
    "                reasons.append(f\"Near city label '{label_text}' (distance: {distance:.1f}): +{label_score:.2f} points\")\n",
    "                break  # Only count once\n",
    "        \n",
    "        # 2. Check for city-state-zip pattern within the same address block\n",
    "        if state_candidates or zip_candidates:\n",
    "            # Find which address block this text belongs to\n",
    "            text_block = None\n",
    "            for block in address_blocks:\n",
    "                if any(t == text for _, t, _ in block):\n",
    "                    text_block = block\n",
    "                    break\n",
    "            \n",
    "            if text_block:\n",
    "                # Check if any state candidate is in the same block\n",
    "                block_texts = [t for _, t, _ in text_block]\n",
    "                state_in_block = False\n",
    "                zip_in_block = False\n",
    "                \n",
    "                for state_candidate in state_candidates:\n",
    "                    if state_candidate['text'] in block_texts:\n",
    "                        state_in_block = True\n",
    "                        \n",
    "                        # Find positions to check if city is before state\n",
    "                        state_bbox = state_candidate['bbox']\n",
    "                        state_x = sum([point[0] for point in state_bbox]) / len(state_bbox)\n",
    "                        state_y = sum([point[1] for point in state_bbox]) / len(state_bbox)\n",
    "                        text_x = sum([point[0] for point in bbox]) / len(bbox)\n",
    "                        text_y = sum([point[1] for point in bbox]) / len(bbox)\n",
    "                        \n",
    "                        # City typically appears before state on same line\n",
    "                        if abs(text_y - state_y) < 30 and text_x < state_x:\n",
    "                            pattern_score = 6.0 * confidence\n",
    "                            score += pattern_score\n",
    "                            reasons.append(f\"Appears before state '{state_candidate['text']}' on same line: +{pattern_score:.2f} points\")\n",
    "                        \n",
    "                        # City typically appears before state in address block\n",
    "                        elif text_y < state_y:\n",
    "                            pattern_score = 3.0 * confidence\n",
    "                            score += pattern_score\n",
    "                            reasons.append(f\"Appears before state '{state_candidate['text']}' in address block: +{pattern_score:.2f} points\")\n",
    "                        \n",
    "                        break  # Only count once\n",
    "                \n",
    "                for zip_candidate in zip_candidates:\n",
    "                    if zip_candidate['text'] in block_texts:\n",
    "                        zip_in_block = True\n",
    "                        break\n",
    "                \n",
    "                if state_in_block and zip_in_block:\n",
    "                    # Strong indicator: city, state, and ZIP in same block\n",
    "                    block_score = 4.0 * confidence\n",
    "                    score += block_score\n",
    "                    reasons.append(f\"In same address block as state and ZIP: +{block_score:.2f} points\")\n",
    "                elif state_in_block:\n",
    "                    # Medium indicator: city and state in same block\n",
    "                    block_score = 2.0 * confidence\n",
    "                    score += block_score\n",
    "                    reasons.append(f\"In same address block as state: +{block_score:.2f} points\")\n",
    "        \n",
    "        # 3. Check proximity to associated name (if provided)\n",
    "        if associated_name_bbox:\n",
    "            distance_to_name = calculate_distance(bbox, associated_name_bbox)\n",
    "            # Addresses are typically near the associated name\n",
    "            if distance_to_name < 500:\n",
    "                name_proximity_score = max(0, 1 - (distance_to_name / 500)) * 2.0 * confidence\n",
    "                score += name_proximity_score\n",
    "                reasons.append(f\"Near associated name (distance: {distance_to_name:.1f}): +{name_proximity_score:.2f} points\")\n",
    "        \n",
    "        # 4. Check capitalization pattern (proper nouns)\n",
    "        if text[0].isupper() and not text.isupper() and len(text.split()) <= 2:\n",
    "            cap_score = 1.0 * confidence\n",
    "            score += cap_score\n",
    "            reasons.append(f\"Has city-like capitalization pattern: +{cap_score:.2f} points\")\n",
    "        \n",
    "        # Add to candidates if score is positive\n",
    "        if score > 0:\n",
    "            city_candidates.append({\n",
    "                'text': text,\n",
    "                'score': score,\n",
    "                'bbox': bbox,\n",
    "                'confidence': confidence,\n",
    "                'reasons': reasons\n",
    "            })\n",
    "    \n",
    "    # Sort candidates by score\n",
    "    city_candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    if debug and city_candidates:\n",
    "        print(f\"\\nContextual city candidates ({len(city_candidates)}):\")\n",
    "        for i, candidate in enumerate(city_candidates[:3]):  # Show top 3\n",
    "            print(f\"  {i+1}. '{candidate['text']}' - Score: {candidate['score']:.3f}\")\n",
    "            for reason in candidate['reasons']:\n",
    "                print(f\"     - {reason}\")\n",
    "    \n",
    "    return city_candidates\n",
    "\n",
    "def find_associated_address(processed_results, associated_name_bbox, debug=True):\n",
    "    \"\"\"\n",
    "    Extract the full address associated with a person.\n",
    "    \n",
    "    Args:\n",
    "        processed_results: List of (bbox, text, confidence) tuples from OCR\n",
    "        associated_name_bbox: Bounding box of the associated name\n",
    "        debug: Whether to print debug information\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (full_address, address_components)\n",
    "    \"\"\"\n",
    "    # Handle case where no associated name is found\n",
    "    if not associated_name_bbox:\n",
    "        if debug:\n",
    "            print(\"No associated name bounding box provided\")\n",
    "        return \"\", {}\n",
    "    \n",
    "    # Initialize address components\n",
    "    address_components = {\n",
    "        'street': None,\n",
    "        'city': None,\n",
    "        'state': None,\n",
    "        'zip': None\n",
    "    }\n",
    "    \n",
    "    # First identify state and ZIP candidates\n",
    "    state_candidates = []\n",
    "    zip_candidates = []\n",
    "    \n",
    "    for bbox, text, confidence in processed_results:\n",
    "        # Skip very short text\n",
    "        if len(text) < 2:\n",
    "            continue\n",
    "            \n",
    "        # Check for state\n",
    "        if is_state(text):\n",
    "            state_candidates.append({\n",
    "                'text': text,\n",
    "                'bbox': bbox,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "            \n",
    "        # Check for ZIP code\n",
    "        if is_zipcode(text):\n",
    "            zip_candidates.append({\n",
    "                'text': text,\n",
    "                'bbox': bbox,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "    \n",
    "    # Sort state and ZIP candidates by confidence\n",
    "    if state_candidates:\n",
    "        state_candidates.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    if zip_candidates:\n",
    "        zip_candidates.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    # Initialize candidates for street\n",
    "    street_candidates = []\n",
    "    \n",
    "    # First pass: identify potential street address\n",
    "    for bbox, text, confidence in processed_results:\n",
    "        # Skip very short text\n",
    "        if len(text) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Check for street address\n",
    "        if is_street_address(text):\n",
    "            street_candidates.append({\n",
    "                'text': text,\n",
    "                'bbox': bbox,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "    \n",
    "    # Use contextual approach for city detection\n",
    "    city_candidates = find_city_contextual(processed_results, state_candidates, zip_candidates, associated_name_bbox, debug=debug)\n",
    "    \n",
    "    # Sort candidates by score/confidence\n",
    "    if street_candidates:\n",
    "        street_candidates.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "        address_components['street'] = street_candidates[0]['text']\n",
    "        \n",
    "    if city_candidates:\n",
    "        address_components['city'] = city_candidates[0]['text']\n",
    "        \n",
    "    if state_candidates:\n",
    "        address_components['state'] = state_candidates[0]['text']\n",
    "        \n",
    "    if zip_candidates:\n",
    "        address_components['zip'] = zip_candidates[0]['text']\n",
    "    \n",
    "    # Debug output\n",
    "    if debug:\n",
    "        print(\"\\nAddress component candidates:\")\n",
    "        if street_candidates:\n",
    "            print(f\"  Street candidates ({len(street_candidates)}):\")\n",
    "            for i, candidate in enumerate(street_candidates[:3]):  # Show top 3\n",
    "                print(f\"    {i+1}. '{candidate['text']}' - Confidence: {candidate['confidence']:.3f}\")\n",
    "        else:\n",
    "            print(\"  No street candidates found\")\n",
    "            \n",
    "        if city_candidates:\n",
    "            print(f\"  City candidates ({len(city_candidates)}):\")\n",
    "            for i, candidate in enumerate(city_candidates[:3]):  # Show top 3\n",
    "                print(f\"    {i+1}. '{candidate['text']}' - Score: {candidate['score']:.3f}\")\n",
    "        else:\n",
    "            print(\"  No city candidates found\")\n",
    "            \n",
    "        if state_candidates:\n",
    "            print(f\"  State candidates ({len(state_candidates)}):\")\n",
    "            for i, candidate in enumerate(state_candidates[:3]):  # Show top 3\n",
    "                print(f\"    {i+1}. '{candidate['text']}' - Confidence: {candidate['confidence']:.3f}\")\n",
    "        else:\n",
    "            print(\"  No state candidates found\")\n",
    "            \n",
    "        if zip_candidates:\n",
    "            print(f\"  ZIP candidates ({len(zip_candidates)}):\")\n",
    "            for i, candidate in enumerate(zip_candidates[:3]):  # Show top 3\n",
    "                print(f\"    {i+1}. '{candidate['text']}' - Confidence: {candidate['confidence']:.3f}\")\n",
    "        else:\n",
    "            print(\"  No ZIP candidates found\")\n",
    "    \n",
    "    # Format full address\n",
    "    full_address = \"\"\n",
    "    if address_components['street']:\n",
    "        full_address += address_components['street']\n",
    "    \n",
    "    if address_components['city']:\n",
    "        if full_address:\n",
    "            full_address += \", \"\n",
    "        full_address += address_components['city']\n",
    "    \n",
    "    if address_components['state']:\n",
    "        if full_address:\n",
    "            full_address += \", \"\n",
    "        full_address += address_components['state']\n",
    "    \n",
    "    if address_components['zip']:\n",
    "        if full_address and address_components['state']:\n",
    "            full_address += \" \"\n",
    "        elif full_address:\n",
    "            full_address += \", \"\n",
    "        full_address += address_components['zip']\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\nExtracted Address: '{full_address}'\")\n",
    "    \n",
    "    return full_address, address_components\n",
    "\n",
    "\n",
    "# Test the associated address extraction on ALL images\n",
    "print(\"\\n===== TESTING ASSOCIATED ADDRESS EXTRACTION ON ALL IMAGES =====\")\n",
    "\n",
    "# Initialize results storage\n",
    "address_results = []\n",
    "\n",
    "if file_list:\n",
    "    for test_file in file_list:\n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        print(f\"Processing: {test_file}\")\n",
    "        test_path = os.path.join(test_folder, test_file)\n",
    "        \n",
    "        # Load and process image\n",
    "        image = cv2.imread(test_path)\n",
    "        if image is None:\n",
    "            print(f\"Could not read image: {test_file}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Dimensions: {image.shape[1]}x{image.shape[0]}\")\n",
    "        \n",
    "        # Perform OCR\n",
    "        ocr_results = ocr.ocr(image, cls=True)\n",
    "        if not ocr_results or not ocr_results[0]:\n",
    "            print(f\"  No text detected\")\n",
    "            continue\n",
    "            \n",
    "        # Process OCR results\n",
    "        processed_results = []\n",
    "        all_text = []\n",
    "        \n",
    "        for line in ocr_results[0]:\n",
    "            bbox, (text, confidence) = line\n",
    "            text = combine_spaced_alphanumeric(text.strip())\n",
    "            processed_results.append((bbox, text, confidence))\n",
    "            all_text.append(text)\n",
    "            \n",
    "        all_text_combined = \" \".join(all_text)\n",
    "        print(f\"  OCR extracted {len(all_text_combined)} characters\")\n",
    "        \n",
    "        # First extract event type to use as reference for associated_name\n",
    "        event_type, event_type_candidates = find_event_type(processed_results, all_text_combined, debug=False)\n",
    "        \n",
    "        # Get event type bbox if found\n",
    "        event_type_bbox = None\n",
    "        if event_type:\n",
    "            # Find the bbox for the event type\n",
    "            for bbox, text, _ in processed_results:\n",
    "                if text.strip() == event_type:\n",
    "                    event_type_bbox = bbox\n",
    "                    break\n",
    "        \n",
    "        # Extract associated name using the exact same function from the first cell\n",
    "        associated_name, name_candidates = find_associated_name(processed_results, event_type_bbox, debug=False)\n",
    "        \n",
    "        # Get associated name bbox if found\n",
    "        associated_name_bbox = None\n",
    "        if associated_name:\n",
    "            print(f\"Associated Name: '{associated_name}'\")\n",
    "            # Find the bbox for the associated name\n",
    "            for bbox, text, _ in processed_results:\n",
    "                if text.strip() == associated_name:\n",
    "                    associated_name_bbox = bbox\n",
    "                    break\n",
    "        else:\n",
    "            print(\"No associated name found\")\n",
    "        \n",
    "        # Extract associated address only if we have an associated name\n",
    "        if associated_name_bbox:\n",
    "            associated_address, address_components = find_associated_address(processed_results, associated_name_bbox, debug=True)\n",
    "            print(f\"\\nExtracted Associated Address: '{associated_address}'\")\n",
    "        else:\n",
    "            associated_address = \"\"\n",
    "            address_components = {}\n",
    "            print(\"No associated address extracted (no associated name found)\")\n",
    "        \n",
    "        # Store result\n",
    "        address_results.append({\n",
    "            'filename': test_file,\n",
    "            'associated_name': associated_name if associated_name else \"\",\n",
    "            'associated_address': associated_address\n",
    "        })\n",
    "        \n",
    "        # Visualize the result with colored bounding boxes\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img_rgb)\n",
    "        \n",
    "        # Draw bounding boxes for address components\n",
    "        for bbox, text, confidence in processed_results:\n",
    "            # Determine box color based on detection\n",
    "            color, linewidth = \"gray\", 1  # Default\n",
    "            \n",
    "            if associated_name and text.strip() == associated_name:\n",
    "                color, linewidth = \"red\", 3  # Associated name\n",
    "            elif address_components.get('street') and text.strip() == address_components['street']:\n",
    "                color, linewidth = \"blue\", 2  # Street\n",
    "            elif address_components.get('city') and text.strip() == address_components['city']:\n",
    "                color, linewidth = \"green\", 2  # City\n",
    "            elif address_components.get('state') and text.strip() == address_components['state']:\n",
    "                color, linewidth = \"purple\", 2  # State\n",
    "            elif address_components.get('zip') and text.strip() == address_components['zip']:\n",
    "                color, linewidth = \"orange\", 2  # ZIP\n",
    "                \n",
    "            # Draw bounding box\n",
    "            points = np.array(bbox, dtype=np.int32)\n",
    "            plt.plot(\n",
    "                [points[0][0], points[1][0], points[2][0], points[3][0], points[0][0]],\n",
    "                [points[0][1], points[1][1], points[2][1], points[3][1], points[0][1]],\n",
    "                color=color,\n",
    "                linewidth=linewidth,\n",
    "            )\n",
    "            \n",
    "            # Add text label\n",
    "            plt.text(\n",
    "                points[0][0],\n",
    "                points[0][1] - 5,\n",
    "                f\"{text} ({confidence:.2f})\",\n",
    "                fontsize=8,\n",
    "                color=color,\n",
    "                weight=\"bold\",\n",
    "            )\n",
    "            \n",
    "        plt.title(f\"Associated Address Detection: {test_file}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Display summary of results\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"ASSOCIATED ADDRESS EXTRACTION SUMMARY\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    print(f\"Processed {len(address_results)} images\")\n",
    "    print(f\"Found addresses in {sum(1 for r in address_results if r['associated_address'])} images\")\n",
    "    \n",
    "    # Display table of results\n",
    "    if address_results:\n",
    "        results_df = pd.DataFrame(address_results)\n",
    "        print(\"\\nExtracted Associated Addresses:\")\n",
    "        print(results_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No test images available for associated address extraction testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6.6: Associated Address Extraction Implementation\n",
    "\n",
    "\"\"\"\n",
    "Implementation of the associated address extraction function.\n",
    "Identifies the full address (street, city, state, zip) associated with the person.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load spaCy NER model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    # If model not installed, download it\n",
    "    import sys\n",
    "    !{sys.executable} -m spacy download en_core_web_sm\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# [KEEP ALL ORIGINAL ASSOCIATED NAME FUNCTIONS FROM CELL 6.5 EXACTLY AS PROVIDED]\n",
    "# is_person_name, contains_name_indicator, find_name_labels, calculate_distance,\n",
    "# is_likely_not_a_name, is_form_field_label, find_associated_name\n",
    "\n",
    "# Address extraction functions\n",
    "def is_street_address(text):\n",
    "    \"\"\"Check if text is likely a street address.\"\"\"\n",
    "    # Common street address patterns\n",
    "    street_patterns = [\n",
    "        r'\\d+\\s+[A-Za-z]+\\s+[A-Za-z]+',  # 123 Main Street\n",
    "        r'\\d+\\s+[A-Za-z]+\\s+[A-Za-z]+\\s+[A-Za-z]+',  # 123 East Main Street\n",
    "        r'\\d+\\s+[A-Za-z]+',  # 123 Main\n",
    "        r'P\\.?O\\.?\\s+Box\\s+\\d+',  # P.O. Box 123\n",
    "    ]\n",
    "    \n",
    "    # Check for common street suffixes\n",
    "    street_suffixes = ['street', 'st', 'avenue', 'ave', 'road', 'rd', 'boulevard', 'blvd', \n",
    "                      'drive', 'dr', 'lane', 'ln', 'place', 'pl', 'court', 'ct', 'way']\n",
    "    \n",
    "    # Check for street patterns\n",
    "    for pattern in street_patterns:\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            return True\n",
    "    \n",
    "    # Check for street suffixes\n",
    "    for suffix in street_suffixes:\n",
    "        if re.search(r'\\b' + suffix + r'\\b', text, re.IGNORECASE):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def is_state(text):\n",
    "    \"\"\"Check if text is a US state or abbreviation.\"\"\"\n",
    "    # List of US state abbreviations\n",
    "    state_abbrs = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', \n",
    "                  'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', \n",
    "                  'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', \n",
    "                  'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', \n",
    "                  'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "    \n",
    "    # Check if text is a state abbreviation\n",
    "    if text.upper() in state_abbrs:\n",
    "        return True\n",
    "        \n",
    "    # List of US state names\n",
    "    state_names = ['alabama', 'alaska', 'arizona', 'arkansas', 'california', \n",
    "                  'colorado', 'connecticut', 'delaware', 'florida', 'georgia', \n",
    "                  'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', \n",
    "                  'kentucky', 'louisiana', 'maine', 'maryland', 'massachusetts', \n",
    "                  'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', \n",
    "                  'nebraska', 'nevada', 'new hampshire', 'new jersey', 'new mexico', \n",
    "                  'new york', 'north carolina', 'north dakota', 'ohio', 'oklahoma', \n",
    "                  'oregon', 'pennsylvania', 'rhode island', 'south carolina', \n",
    "                  'south dakota', 'tennessee', 'texas', 'utah', 'vermont', \n",
    "                  'virginia', 'washington', 'west virginia', 'wisconsin', 'wyoming']\n",
    "    \n",
    "    # Check if text is a state name\n",
    "    if text.lower() in state_names:\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def is_zipcode(text):\n",
    "    \"\"\"Check if text is a US ZIP code.\"\"\"\n",
    "    # 5-digit ZIP code\n",
    "    if re.match(r'^\\d{5}$', text):\n",
    "        return True\n",
    "        \n",
    "    # 9-digit ZIP+4 code\n",
    "    if re.match(r'^\\d{5}-\\d{4}$', text):\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def find_address_blocks(processed_results):\n",
    "    \"\"\"Group text elements into potential address blocks based on spatial proximity.\"\"\"\n",
    "    # If too few elements, return a single block\n",
    "    if len(processed_results) < 5:\n",
    "        return [processed_results]\n",
    "    \n",
    "    # Initialize address blocks\n",
    "    address_blocks = []\n",
    "    \n",
    "    # Sort by vertical position (y-coordinate)\n",
    "    sorted_results = sorted(processed_results, \n",
    "                           key=lambda x: sum([point[1] for point in x[0]]) / len(x[0]))\n",
    "    \n",
    "    current_block = []\n",
    "    prev_y = None\n",
    "    \n",
    "    # Group by vertical proximity\n",
    "    for bbox, text, confidence in sorted_results:\n",
    "        # Calculate center y-coordinate\n",
    "        center_y = sum([point[1] for point in bbox]) / len(bbox)\n",
    "        \n",
    "        # Start a new block if vertical gap is large\n",
    "        if prev_y is not None and abs(center_y - prev_y) > 100:\n",
    "            if current_block:\n",
    "                address_blocks.append(current_block)\n",
    "                current_block = []\n",
    "        \n",
    "        current_block.append((bbox, text, confidence))\n",
    "        prev_y = center_y\n",
    "    \n",
    "    # Add the last block\n",
    "    if current_block:\n",
    "        address_blocks.append(current_block)\n",
    "    \n",
    "    return address_blocks\n",
    "\n",
    "def identify_city_with_ner(text, debug=False):\n",
    "    \"\"\"\n",
    "    Use NER to identify if text is likely a city.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to analyze\n",
    "        debug: Whether to print debug information\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (is_city, confidence_score)\n",
    "    \"\"\"\n",
    "    # Skip very short text\n",
    "    if len(text) < 3:\n",
    "        return False, 0.0\n",
    "    \n",
    "    # Skip text with numbers\n",
    "    if re.search(r'\\d', text):\n",
    "        return False, 0.0\n",
    "    \n",
    "    # Skip form field labels\n",
    "    if text.lower() in ['city', 'city:', 'state', 'state:', 'zip', 'zip code', 'zip:']:\n",
    "        return False, 0.0\n",
    "    \n",
    "    # Skip common form headers and organization names\n",
    "    non_city_indicators = [\n",
    "        'department', 'bureau', 'federal', 'firearms', 'license', 'report', 'form',\n",
    "        'section', 'universal', 'security', 'services', 'corporation', 'corporate',\n",
    "        'trade', 'full', 'name', 'person', 'making', 'street', 'address', 'theft',\n",
    "        'loss', 'inventory', 'notification', 'alcohol', 'tobacco', 'explosives',\n",
    "        'signature', 'justice', 'licensee', 'date', 'time', 'code', 'telephone',\n",
    "        'bridge', 'tower', 'building', 'plaza', 'center', 'complex', 'director',\n",
    "        'category', 'hair', 'eyes', 'sex', 'height', 'weight', 'expires', 'print'\n",
    "    ]\n",
    "    \n",
    "    # Check for non-city indicators\n",
    "    text_lower = text.lower()\n",
    "    for indicator in non_city_indicators:\n",
    "        if indicator in text_lower:\n",
    "            return False, 0.0\n",
    "    \n",
    "    # Check for labels/headers with colons\n",
    "    if ':' in text:\n",
    "        return False, 0.0\n",
    "    \n",
    "    # Use spaCy NER to check if it's recognized as a location\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Initialize confidence score\n",
    "    confidence = 0.0\n",
    "    \n",
    "    # Check for GPE (Geo-Political Entity) or LOC (Location) entities\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"GPE\":\n",
    "            confidence = 0.9  # High confidence for GPE (cities, states, countries)\n",
    "            if debug:\n",
    "                print(f\"  '{text}' recognized as GPE by NER: +0.9 confidence\")\n",
    "            break\n",
    "        elif ent.label_ == \"LOC\":\n",
    "            confidence = 0.7  # Medium confidence for LOC (non-GPE locations)\n",
    "            if debug:\n",
    "                print(f\"  '{text}' recognized as LOC by NER: +0.7 confidence\")\n",
    "            break\n",
    "    \n",
    "    # IMPORTANT: Only return True if NER actually identified it as a location\n",
    "    # Do NOT provide fallback confidence for non-location entities\n",
    "    if confidence > 0.0:\n",
    "        return True, confidence\n",
    "    else:\n",
    "        if debug:\n",
    "            print(f\"  '{text}' not recognized as a location by NER\")\n",
    "        return False, 0.0\n",
    "\n",
    "def find_city_with_contextual_validation(processed_results, state_candidates, zip_candidates, debug=False):\n",
    "    \"\"\"\n",
    "    Extract city name using NER and contextual validation.\n",
    "    \n",
    "    Args:\n",
    "        processed_results: List of (bbox, text, confidence) tuples from OCR\n",
    "        state_candidates: List of state candidates\n",
    "        zip_candidates: List of ZIP code candidates\n",
    "        debug: Whether to print debug information\n",
    "        \n",
    "    Returns:\n",
    "        List of city candidates with scores\n",
    "    \"\"\"\n",
    "    city_candidates = []\n",
    "    \n",
    "    # Find city labels\n",
    "    city_labels = []\n",
    "    for bbox, text, confidence in processed_results:\n",
    "        text_lower = text.lower()\n",
    "        if text_lower == 'city' or text_lower == 'city:':\n",
    "            city_labels.append((bbox, text, confidence))\n",
    "            if debug:\n",
    "                print(f\"Found city label: '{text}'\")\n",
    "    \n",
    "    # Group text elements into potential address blocks\n",
    "    address_blocks = find_address_blocks(processed_results)\n",
    "    \n",
    "    # Process each text element\n",
    "    for bbox, text, confidence in processed_results:\n",
    "        # Use NER to check if it's a city\n",
    "        is_city, ner_confidence = identify_city_with_ner(text, debug)\n",
    "        \n",
    "        if not is_city:\n",
    "            continue\n",
    "        \n",
    "        # Initialize score with NER confidence\n",
    "        score = ner_confidence * 5.0  # Scale up NER confidence\n",
    "        reasons = [f\"NER confidence: {ner_confidence:.2f} x 5.0 = {ner_confidence * 5.0:.2f} points\"]\n",
    "        \n",
    "        # Check proximity to city labels\n",
    "        for label_bbox, label_text, _ in city_labels:\n",
    "            distance = calculate_distance(bbox, label_bbox)\n",
    "            if distance < 200:\n",
    "                # Text near a city label is likely a city\n",
    "                label_score = 3.0 * confidence\n",
    "                score += label_score\n",
    "                reasons.append(f\"Near city label '{label_text}': +{label_score:.2f} points\")\n",
    "                break  # Only count once\n",
    "        \n",
    "        # Check for city-state-zip pattern within the same address block\n",
    "        if state_candidates and zip_candidates:\n",
    "            # Find which address block this text belongs to\n",
    "            text_block = None\n",
    "            for block in address_blocks:\n",
    "                if any(t == text for _, t, _ in block):\n",
    "                    text_block = block\n",
    "                    break\n",
    "            \n",
    "            if text_block:\n",
    "                # Check if any state candidate is in the same block\n",
    "                block_texts = [t for _, t, _ in text_block]\n",
    "                state_in_block = False\n",
    "                zip_in_block = False\n",
    "                \n",
    "                for state_candidate in state_candidates:\n",
    "                    if state_candidate['text'] in block_texts:\n",
    "                        state_in_block = True\n",
    "                        \n",
    "                        # Find positions to check if city is before state\n",
    "                        state_bbox = state_candidate['bbox']\n",
    "                        state_x = sum([point[0] for point in state_bbox]) / len(state_bbox)\n",
    "                        state_y = sum([point[1] for point in state_bbox]) / len(state_bbox)\n",
    "                        text_x = sum([point[0] for point in bbox]) / len(bbox)\n",
    "                        text_y = sum([point[1] for point in bbox]) / len(bbox)\n",
    "                        \n",
    "                        # City typically appears before state on same line\n",
    "                        if abs(text_y - state_y) < 30 and text_x < state_x:\n",
    "                            pattern_score = 4.0 * confidence\n",
    "                            score += pattern_score\n",
    "                            reasons.append(f\"Appears before state '{state_candidate['text']}' on same line: +{pattern_score:.2f} points\")\n",
    "                        \n",
    "                        # City typically appears before state in address block\n",
    "                        elif text_y < state_y:\n",
    "                            pattern_score = 2.0 * confidence\n",
    "                            score += pattern_score\n",
    "                            reasons.append(f\"Appears before state '{state_candidate['text']}' in address block: +{pattern_score:.2f} points\")\n",
    "                        \n",
    "                        break  # Only count once\n",
    "                \n",
    "                for zip_candidate in zip_candidates:\n",
    "                    if zip_candidate['text'] in block_texts:\n",
    "                        zip_in_block = True\n",
    "                        break\n",
    "                \n",
    "                if state_in_block and zip_in_block:\n",
    "                    # Strong indicator: city, state, and ZIP in same block\n",
    "                    block_score = 3.0 * confidence\n",
    "                    score += block_score\n",
    "                    reasons.append(f\"In same address block as state and ZIP: +{block_score:.2f} points\")\n",
    "                elif state_in_block:\n",
    "                    # Medium indicator: city and state in same block\n",
    "                    block_score = 1.5 * confidence\n",
    "                    score += block_score\n",
    "                    reasons.append(f\"In same address block as state: +{block_score:.2f} points\")\n",
    "        \n",
    "        # Add to candidates if score is positive\n",
    "        if score > 0:\n",
    "            city_candidates.append({\n",
    "                'text': text,\n",
    "                'score': score,\n",
    "                'bbox': bbox,\n",
    "                'confidence': confidence,\n",
    "                'ner_confidence': ner_confidence,\n",
    "                'reasons': reasons\n",
    "            })\n",
    "    \n",
    "    # Sort candidates by score\n",
    "    city_candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    if debug and city_candidates:\n",
    "        print(f\"\\nNER-based city candidates ({len(city_candidates)}):\")\n",
    "        for i, candidate in enumerate(city_candidates[:3]):  # Show top 3\n",
    "            print(f\"  {i+1}. '{candidate['text']}' - Score: {candidate['score']:.3f}, NER confidence: {candidate['ner_confidence']:.2f}\")\n",
    "            for reason in candidate['reasons']:\n",
    "                print(f\"     - {reason}\")\n",
    "    \n",
    "    return city_candidates\n",
    "\n",
    "def find_associated_address(processed_results, associated_name_bbox, debug=True):\n",
    "    \"\"\"\n",
    "    Extract the full address associated with a person.\n",
    "    \n",
    "    Args:\n",
    "        processed_results: List of (bbox, text, confidence) tuples from OCR\n",
    "        associated_name_bbox: Bounding box of the associated name\n",
    "        debug: Whether to print debug information\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (full_address, address_components)\n",
    "    \"\"\"\n",
    "    # Handle case where no associated name is found\n",
    "    if not associated_name_bbox:\n",
    "        if debug:\n",
    "            print(\"No associated name bounding box provided\")\n",
    "        return \"\", {}\n",
    "    \n",
    "    # Initialize address components\n",
    "    address_components = {\n",
    "        'street': None,\n",
    "        'city': None,\n",
    "        'state': None,\n",
    "        'zip': None\n",
    "    }\n",
    "    \n",
    "    # First identify state and ZIP candidates\n",
    "    state_candidates = []\n",
    "    zip_candidates = []\n",
    "    \n",
    "    for bbox, text, confidence in processed_results:\n",
    "        # Skip very short text\n",
    "        if len(text) < 2:\n",
    "            continue\n",
    "            \n",
    "        # Check for state\n",
    "        if is_state(text):\n",
    "            state_candidates.append({\n",
    "                'text': text,\n",
    "                'bbox': bbox,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "            \n",
    "        # Check for ZIP code\n",
    "        if is_zipcode(text):\n",
    "            zip_candidates.append({\n",
    "                'text': text,\n",
    "                'bbox': bbox,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "    \n",
    "    # Sort state and ZIP candidates by confidence\n",
    "    if state_candidates:\n",
    "        state_candidates.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    if zip_candidates:\n",
    "        zip_candidates.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    # Initialize candidates for street\n",
    "    street_candidates = []\n",
    "    \n",
    "    # First pass: identify potential street address\n",
    "    for bbox, text, confidence in processed_results:\n",
    "        # Skip very short text\n",
    "        if len(text) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Check for street address\n",
    "        if is_street_address(text):\n",
    "            street_candidates.append({\n",
    "                'text': text,\n",
    "                'bbox': bbox,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "    \n",
    "    # Use NER with contextual validation for city detection\n",
    "    city_candidates = find_city_with_contextual_validation(processed_results, state_candidates, zip_candidates, debug=debug)\n",
    "    \n",
    "    # Sort candidates by score/confidence\n",
    "    if street_candidates:\n",
    "        street_candidates.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "        address_components['street'] = street_candidates[0]['text']\n",
    "        \n",
    "    if city_candidates:\n",
    "        address_components['city'] = city_candidates[0]['text']\n",
    "        \n",
    "    if state_candidates:\n",
    "        address_components['state'] = state_candidates[0]['text']\n",
    "        \n",
    "    if zip_candidates:\n",
    "        address_components['zip'] = zip_candidates[0]['text']\n",
    "    \n",
    "    # Debug output\n",
    "    if debug:\n",
    "        print(\"\\nAddress component candidates:\")\n",
    "        if street_candidates:\n",
    "            print(f\"  Street candidates ({len(street_candidates)}):\")\n",
    "            for i, candidate in enumerate(street_candidates[:3]):  # Show top 3\n",
    "                print(f\"    {i+1}. '{candidate['text']}' - Confidence: {candidate['confidence']:.3f}\")\n",
    "        else:\n",
    "            print(\"  No street candidates found\")\n",
    "            \n",
    "        if city_candidates:\n",
    "            print(f\"  City candidates ({len(city_candidates)}):\")\n",
    "            for i, candidate in enumerate(city_candidates[:3]):  # Show top 3\n",
    "                print(f\"    {i+1}. '{candidate['text']}' - Score: {candidate['score']:.3f}\")\n",
    "        else:\n",
    "            print(\"  No city candidates found\")\n",
    "            \n",
    "        if state_candidates:\n",
    "            print(f\"  State candidates ({len(state_candidates)}):\")\n",
    "            for i, candidate in enumerate(state_candidates[:3]):  # Show top 3\n",
    "                print(f\"    {i+1}. '{candidate['text']}' - Confidence: {candidate['confidence']:.3f}\")\n",
    "        else:\n",
    "            print(\"  No state candidates found\")\n",
    "            \n",
    "        if zip_candidates:\n",
    "            print(f\"  ZIP candidates ({len(zip_candidates)}):\")\n",
    "            for i, candidate in enumerate(zip_candidates[:3]):  # Show top 3\n",
    "                print(f\"    {i+1}. '{candidate['text']}' - Confidence: {candidate['confidence']:.3f}\")\n",
    "        else:\n",
    "            print(\"  No ZIP candidates found\")\n",
    "    \n",
    "    # Format full address\n",
    "    full_address = \"\"\n",
    "    if address_components['street']:\n",
    "        full_address += address_components['street']\n",
    "    \n",
    "    if address_components['city']:\n",
    "        if full_address:\n",
    "            full_address += \", \"\n",
    "        full_address += address_components['city']\n",
    "    \n",
    "    if address_components['state']:\n",
    "        if full_address:\n",
    "            full_address += \", \"\n",
    "        full_address += address_components['state']\n",
    "    \n",
    "    if address_components['zip']:\n",
    "        if full_address and address_components['state']:\n",
    "            full_address += \" \"\n",
    "        elif full_address:\n",
    "            full_address += \", \"\n",
    "        full_address += address_components['zip']\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\nExtracted Address: '{full_address}'\")\n",
    "    \n",
    "    return full_address, address_components\n",
    "\n",
    "# Test the associated address extraction on ALL images\n",
    "print(\"\\n===== TESTING ASSOCIATED ADDRESS EXTRACTION ON ALL IMAGES =====\")\n",
    "\n",
    "# Initialize results storage\n",
    "address_results = []\n",
    "\n",
    "if file_list:\n",
    "    for test_file in file_list:\n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        print(f\"Processing: {test_file}\")\n",
    "        test_path = os.path.join(test_folder, test_file)\n",
    "        \n",
    "        # Load and process image\n",
    "        image = cv2.imread(test_path)\n",
    "        if image is None:\n",
    "            print(f\"Could not read image: {test_file}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Dimensions: {image.shape[1]}x{image.shape[0]}\")\n",
    "        \n",
    "        # Perform OCR\n",
    "        ocr_results = ocr.ocr(image, cls=True)\n",
    "        if not ocr_results or not ocr_results[0]:\n",
    "            print(f\"  No text detected\")\n",
    "            continue\n",
    "            \n",
    "        # Process OCR results\n",
    "        processed_results = []\n",
    "        all_text = []\n",
    "        \n",
    "        for line in ocr_results[0]:\n",
    "            bbox, (text, confidence) = line\n",
    "            text = combine_spaced_alphanumeric(text.strip())\n",
    "            processed_results.append((bbox, text, confidence))\n",
    "            all_text.append(text)\n",
    "            \n",
    "        all_text_combined = \" \".join(all_text)\n",
    "        print(f\"  OCR extracted {len(all_text_combined)} characters\")\n",
    "        \n",
    "        # First extract event type to use as reference for associated_name\n",
    "        event_type, event_type_candidates = find_event_type(processed_results, all_text_combined, debug=False)\n",
    "        \n",
    "        # Get event type bbox if found\n",
    "        event_type_bbox = None\n",
    "        if event_type:\n",
    "            # Find the bbox for the event type\n",
    "            for bbox, text, _ in processed_results:\n",
    "                if text.strip() == event_type:\n",
    "                    event_type_bbox = bbox\n",
    "                    break\n",
    "        \n",
    "        # Extract associated name using the exact same function from the first cell\n",
    "        associated_name, name_candidates = find_associated_name(processed_results, event_type_bbox, debug=False)\n",
    "        \n",
    "        # Get associated name bbox if found\n",
    "        associated_name_bbox = None\n",
    "        if associated_name:\n",
    "            print(f\"Associated Name: '{associated_name}'\")\n",
    "            # Find the bbox for the associated name\n",
    "            for bbox, text, _ in processed_results:\n",
    "                if text.strip() == associated_name:\n",
    "                    associated_name_bbox = bbox\n",
    "                    break\n",
    "        else:\n",
    "            print(\"No associated name found\")\n",
    "        \n",
    "        # Extract associated address only if we have an associated name\n",
    "        if associated_name_bbox:\n",
    "            associated_address, address_components = find_associated_address(processed_results, associated_name_bbox, debug=True)\n",
    "            print(f\"\\nExtracted Associated Address: '{associated_address}'\")\n",
    "        else:\n",
    "            associated_address = \"\"\n",
    "            address_components = {}\n",
    "            print(\"No associated address extracted (no associated name found)\")\n",
    "        \n",
    "        # Store result\n",
    "        address_results.append({\n",
    "            'filename': test_file,\n",
    "            'associated_name': associated_name if associated_name else \"\",\n",
    "            'associated_address': associated_address\n",
    "        })\n",
    "        \n",
    "        # Visualize the result with colored bounding boxes\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img_rgb)\n",
    "        \n",
    "        # Draw bounding boxes for address components\n",
    "        for bbox, text, confidence in processed_results:\n",
    "            # Determine box color based on detection\n",
    "            color, linewidth = \"gray\", 1  # Default\n",
    "            \n",
    "            if associated_name and text.strip() == associated_name:\n",
    "                color, linewidth = \"red\", 3  # Associated name\n",
    "            elif address_components.get('street') and text.strip() == address_components['street']:\n",
    "                color, linewidth = \"blue\", 2  # Street\n",
    "            elif address_components.get('city') and text.strip() == address_components['city']:\n",
    "                color, linewidth = \"green\", 2  # City\n",
    "            elif address_components.get('state') and text.strip() == address_components['state']:\n",
    "                color, linewidth = \"purple\", 2  # State\n",
    "            elif address_components.get('zip') and text.strip() == address_components['zip']:\n",
    "                color, linewidth = \"orange\", 2  # ZIP\n",
    "                \n",
    "            # Draw bounding box\n",
    "            points = np.array(bbox, dtype=np.int32)\n",
    "            plt.plot(\n",
    "                [points[0][0], points[1][0], points[2][0], points[3][0], points[0][0]],\n",
    "                [points[0][1], points[1][1], points[2][1], points[3][1], points[0][1]],\n",
    "                color=color,\n",
    "                linewidth=linewidth,\n",
    "            )\n",
    "            \n",
    "            # Add text label\n",
    "            plt.text(\n",
    "                points[0][0],\n",
    "                points[0][1] - 5,\n",
    "                f\"{text} ({confidence:.2f})\",\n",
    "                fontsize=8,\n",
    "                color=color,\n",
    "                weight=\"bold\",\n",
    "            )\n",
    "            \n",
    "        plt.title(f\"Associated Address Detection: {test_file}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Display summary of results\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"ASSOCIATED ADDRESS EXTRACTION SUMMARY\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    print(f\"Processed {len(address_results)} images\")\n",
    "    print(f\"Found addresses in {sum(1 for r in address_results if r['associated_address'])} images\")\n",
    "    \n",
    "    # Display table of results\n",
    "    if address_results:\n",
    "        results_df = pd.DataFrame(address_results)\n",
    "        print(\"\\nExtracted Associated Addresses:\")\n",
    "        print(results_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No test images available for associated address extraction testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d4e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6.7: Main Processing Pipeline\n",
    "\n",
    "\"\"\"\n",
    "Main processing pipeline that uses the specialized extraction functions\n",
    "to process each image and extract all fields.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from paddleocr import PaddleOCR\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize OCR engine if not already done\n",
    "try:\n",
    "    ocr\n",
    "except NameError:\n",
    "    ocr = PaddleOCR(use_angle_cls=True, lang=\"en\", show_log=False)\n",
    "\n",
    "# Initialize results list for CSV output\n",
    "csv_results = []\n",
    "\n",
    "# Process each image file in the test folder\n",
    "for filename in file_list:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {filename}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Construct full file path\n",
    "    file_path = os.path.join(test_folder, filename)\n",
    "    \n",
    "    try:\n",
    "        # Load image using OpenCV\n",
    "        image = cv2.imread(file_path)\n",
    "        if image is None:\n",
    "            print(f\"Error: Could not load image {filename}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Image dimensions: {image.shape}\")\n",
    "        \n",
    "        # Get file metadata for record keeping\n",
    "        metadata = get_file_metadata(file_path)\n",
    "        \n",
    "        # Execute OCR on the image\n",
    "        ocr_results = ocr.ocr(image, cls=True)\n",
    "        \n",
    "        if not ocr_results or not ocr_results[0]:\n",
    "            print(f\"No text detected in {filename}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract and process OCR results\n",
    "        results = ocr_results[0]\n",
    "        processed_results = []\n",
    "        all_text_parts = []\n",
    "        \n",
    "        print(f\"OCR detected {len(results)} text elements\")\n",
    "        \n",
    "        # Process each detected text element\n",
    "        for bbox, (text, confidence) in results:\n",
    "            if confidence > 0.5:  # Filter low-confidence detections\n",
    "                # Clean and process the text\n",
    "                cleaned_text = text.strip()\n",
    "                \n",
    "                # Apply spaced character combination for potential serial numbers\n",
    "                combined_text = combine_spaced_alphanumeric(cleaned_text)\n",
    "                \n",
    "                processed_results.append((bbox, combined_text, confidence))\n",
    "                all_text_parts.append(combined_text)\n",
    "                \n",
    "                print(f\"  Text: '{cleaned_text}' -> '{combined_text}' (Confidence: {confidence:.2f})\")\n",
    "        \n",
    "        # Combine all detected text for comprehensive analysis\n",
    "        all_text_combined = \" \".join(all_text_parts)\n",
    "        print(f\"\\nCombined text: {all_text_combined}\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # FIELD EXTRACTION\n",
    "        # ====================================================================\n",
    "        \n",
    "        print(\"\\nExtracting structured data...\")\n",
    "        \n",
    "        # Extract serial number using robust method from Cell 5\n",
    "        selected_candidate, all_candidates = find_serial_number(\n",
    "            processed_results, all_text_combined, (image.shape[1], image.shape[0]), debug=True\n",
    "        )\n",
    "        \n",
    "        # Extract serial number\n",
    "        serial_number = selected_candidate[\"text\"] if selected_candidate else \"\"\n",
    "        print(f\"Serial Number: '{serial_number}'\")\n",
    "        \n",
    "        # Extract event type\n",
    "        event_type = find_event_type(processed_results, all_text_combined, debug=True)\n",
    "        print(f\"Event Type: '{event_type}'\")\n",
    "        \n",
    "        # Extract associated name\n",
    "        associated_name = find_associated_name(processed_results, all_text_combined, debug=True)\n",
    "        print(f\"Associated Name: '{associated_name}'\")\n",
    "        \n",
    "        # Extract event date\n",
    "        event_date = find_event_date(processed_results, all_text_combined, debug=True)\n",
    "        print(f\"Event Date: '{event_date}'\")\n",
    "        \n",
    "        # Extract associated address\n",
    "        associated_address = find_associated_address(processed_results, all_text_combined, debug=True)\n",
    "        print(f\"Associated Address: '{associated_address}'\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # VISUALIZATION\n",
    "        # ====================================================================\n",
    "        \n",
    "        print(\"\\nGenerating visualization...\")\n",
    "        \n",
    "        # Create a copy of the image for visualization\n",
    "        vis_image = image.copy()\n",
    "        \n",
    "        # Draw bounding boxes with color coding\n",
    "        for bbox, text, confidence in processed_results:\n",
    "            # Determine color based on content classification\n",
    "            t_lower = text.lower()\n",
    "            \n",
    "            # Check for specific field matches first\n",
    "            if text == serial_number:\n",
    "                color_key = \"serial_number\"\n",
    "            elif text == associated_name:\n",
    "                color_key = \"associated_name\"\n",
    "            elif text == associated_address:\n",
    "                color_key = \"associated_address\"\n",
    "            elif text == event_date:\n",
    "                color_key = \"event_date\"\n",
    "            elif text == event_type:\n",
    "                color_key = \"event_type\"\n",
    "            else:\n",
    "                # Use classification function for general categorization\n",
    "                color_key = classify_token(t_lower)\n",
    "            \n",
    "            # Get color for the category\n",
    "            color = CATEGORY_COLORS.get(color_key, CATEGORY_COLORS[\"other\"])\n",
    "            \n",
    "            # Draw bounding box\n",
    "            pts = np.array(bbox, np.int32).reshape((-1, 1, 2))\n",
    "            cv2.polylines(vis_image, [pts], True, color, 2)\n",
    "            \n",
    "            # Add text label\n",
    "            cv2.putText(vis_image, f\"{text[:20]}...\", \n",
    "                       (int(bbox[0][0]), int(bbox[0][1]) - 10),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "        \n",
    "        # Display the visualization\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.imshow(cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"OCR Results for {filename}\\n\"\n",
    "                 f\"Serial: {serial_number} | Event: {event_type} | \"\n",
    "                 f\"Name: {associated_name}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Add legend\n",
    "        legend_elements = []\n",
    "        for category, color in CATEGORY_COLORS.items():\n",
    "            legend_elements.append(plt.Rectangle((0,0),1,1, facecolor=[c/255 for c in color], \n",
    "                                               edgecolor='black', label=category.title()))\n",
    "        plt.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1, 1))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # ====================================================================\n",
    "        # RECORD RESULTS\n",
    "        # ====================================================================\n",
    "        \n",
    "        # Prepare CSV record\n",
    "        csv_record = {\n",
    "            'filename': filename,\n",
    "            'file_creation_date': metadata['file_creation_date'],\n",
    "            'file_modification_date': metadata['file_modification_date'],\n",
    "            'file_location': file_path,\n",
    "            'serial_number': serial_number,\n",
    "            'event_type': event_type,\n",
    "            'associated_name': associated_name,\n",
    "            'event_date': event_date,\n",
    "            'associated_address': associated_address,\n",
    "            'processing_timestamp': datetime.now().strftime(\"%m/%d/%y %H:%M\")\n",
    "        }\n",
    "        \n",
    "        csv_results.append(csv_record)\n",
    "        \n",
    "        print(f\"\\nProcessing completed for {filename}\")\n",
    "        print(f\"Results recorded: {len(csv_results)} files processed so far\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL OUTPUT GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PROCESSING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Successfully processed {len(csv_results)} files\")\n",
    "\n",
    "if csv_results:\n",
    "    # Create DataFrame from results\n",
    "    results_df = pd.DataFrame(csv_results)\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nExtraction Summary:\")\n",
    "    print(f\"- Files with serial numbers: {sum(1 for r in csv_results if r['serial_number'])}\")\n",
    "    print(f\"- Files with event types: {sum(1 for r in csv_results if r['event_type'])}\")\n",
    "    print(f\"- Files with associated names: {sum(1 for r in csv_results if r['associated_name'])}\")\n",
    "    print(f\"- Files with dates: {sum(1 for r in csv_results if r['event_date'])}\")\n",
    "    print(f\"- Files with addresses: {sum(1 for r in csv_results if r['associated_address'])}\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = \"extracted_data_detailed.csv\"\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nResults saved to: {output_file}\")\n",
    "    \n",
    "    # Display first few results\n",
    "    print(f\"\\nSample Results:\")\n",
    "    print(results_df.head().to_string(index=False))\n",
    "else:\n",
    "    print(\"No files were successfully processed.\")\n",
    "\n",
    "print(f\"\\nProcessing pipeline completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allied",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
