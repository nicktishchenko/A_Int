{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe05af2",
   "metadata": {
    "cellUniqueIdByVincent": "cce53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete with target schema:\n",
      "['serial_number', 'event_type', 'date', 'associated_name', 'associated_address', 'filename', 'file_creation_date', 'file_modification_date', 'file_location']\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Import Libraries and Environment Setup with Unified Schema & Visualization Support\n",
    "import os\n",
    "import re\n",
    "import platform\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import easyocr\n",
    "import zipcodes\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "EXPECTED_COLUMNS = [\n",
    "    \"serial_number\",\n",
    "    \"event_type\",\n",
    "    \"date\",\n",
    "    \"associated_name\",\n",
    "    \"associated_address\",\n",
    "    \"filename\",\n",
    "    \"file_creation_date\",\n",
    "    \"file_modification_date\",\n",
    "    \"file_location\",\n",
    "]\n",
    "_reader_instance = None\n",
    "\n",
    "\n",
    "def get_easyocr_reader(langs=(\"en\",)):\n",
    "    global _reader_instance\n",
    "    if _reader_instance is None:\n",
    "        try:\n",
    "            _reader_instance = easyocr.Reader(list(langs))\n",
    "        except Exception as e:\n",
    "            print(f\"EasyOCR initialization failed: {e}\")\n",
    "            _reader_instance = None\n",
    "    return _reader_instance\n",
    "\n",
    "\n",
    "def ensure_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in EXPECTED_COLUMNS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    return df[EXPECTED_COLUMNS].copy()\n",
    "\n",
    "\n",
    "if \"df1\" not in globals():\n",
    "    df1 = pd.DataFrame(columns=EXPECTED_COLUMNS)\n",
    "else:\n",
    "    df1 = ensure_schema(df1)\n",
    "\n",
    "print(\"Environment setup complete with target schema:\")\n",
    "print(EXPECTED_COLUMNS)\n",
    "\n",
    "# Visualization helper with semantic highlighting\n",
    "CATEGORY_COLORS = {\n",
    "    \"serial\": (0, 255, 0),  # Green\n",
    "    \"event_type\": (255, 140, 0),  # Dark Orange\n",
    "    \"name\": (30, 144, 255),  # Dodger Blue\n",
    "    \"address\": (148, 0, 211),  # Dark Violet\n",
    "    \"other\": (0, 0, 255),  # Red\n",
    "}\n",
    "\n",
    "\n",
    "def classify_token(text_lower):\n",
    "    # Lightweight classification for event_type cues / names / address parts\n",
    "    event_keywords = [\n",
    "        \"inventory\",\n",
    "        \"inspection\",\n",
    "        \"theft\",\n",
    "        \"loss\",\n",
    "        \"stolen\",\n",
    "        \"missing\",\n",
    "        \"burglary\",\n",
    "        \"incident\",\n",
    "        \"transfer\",\n",
    "        \"disposal\",\n",
    "    ]\n",
    "    address_keywords = [\n",
    "        \"street\",\n",
    "        \"st\",\n",
    "        \"ave\",\n",
    "        \"road\",\n",
    "        \"rd\",\n",
    "        \"dr\",\n",
    "        \"drive\",\n",
    "        \"lane\",\n",
    "        \"ln\",\n",
    "        \"blvd\",\n",
    "        \"suite\",\n",
    "        \"ste\",\n",
    "        \"apt\",\n",
    "        \"unit\",\n",
    "    ]\n",
    "    if any(k in text_lower for k in event_keywords):\n",
    "        return \"event_type\"\n",
    "    if re.fullmatch(r\"[a-z]+,?\", text_lower) and text_lower.istitle() is False:\n",
    "        # fallback simple heuristic\n",
    "        return \"other\"\n",
    "    if any(k == text_lower for k in address_keywords):\n",
    "        return \"address\"\n",
    "    return \"other\"\n",
    "\n",
    "\n",
    "def visualize_ocr(\n",
    "    img_bgr,\n",
    "    ocr_results,\n",
    "    serial_candidate=None,\n",
    "    metadata_info=None,\n",
    "    show=True,\n",
    "    title=None,\n",
    "):\n",
    "    draw = img_bgr.copy()\n",
    "    serial_norm = str(serial_candidate) if serial_candidate else None\n",
    "    name_parts = set()\n",
    "    if metadata_info and metadata_info.get(\"name\"):\n",
    "        for part in str(metadata_info[\"name\"]).split():\n",
    "            name_parts.add(part.lower())\n",
    "    address_tokens = set()\n",
    "    if metadata_info and metadata_info.get(\"complete_address\"):\n",
    "        for part in re.split(r\"[\\s,]\", str(metadata_info[\"complete_address\"])):\n",
    "            if part:\n",
    "                address_tokens.add(part.lower())\n",
    "    for bbox, text, conf in ocr_results:\n",
    "        t_norm = text.strip()\n",
    "        t_lower = t_norm.lower()\n",
    "        if serial_norm and t_norm == serial_norm:\n",
    "            color_key = \"serial\"\n",
    "        elif t_lower in name_parts:\n",
    "            color_key = \"name\"\n",
    "        elif t_lower in address_tokens:\n",
    "            color_key = \"address\"\n",
    "        else:\n",
    "            color_key = classify_token(t_lower)\n",
    "        color = CATEGORY_COLORS.get(color_key, CATEGORY_COLORS[\"other\"])\n",
    "        pts = np.array(bbox, np.int32).reshape((-1, 1, 2))\n",
    "        cv2.polylines(\n",
    "            draw,\n",
    "            [pts],\n",
    "            isClosed=True,\n",
    "            color=color,\n",
    "            thickness=2 if color_key != \"serial\" else 3,\n",
    "        )\n",
    "        x, y = pts[0][0]\n",
    "        label = f\"{t_norm} ({conf:.2f})\"\n",
    "        cv2.putText(\n",
    "            draw,\n",
    "            label[:38],\n",
    "            (x, y - 6),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.55,\n",
    "            color,\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "    if show:\n",
    "        img_rgb = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "        plt.figure(figsize=(13, 9))\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\n",
    "            title\n",
    "            or \"OCR Visualization (serial=green, event=orange, name=blue, addr=violet)\"\n",
    "        )\n",
    "        plt.show()\n",
    "    return draw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellUniqueIdByVincent": "a681b"
   },
   "outputs": [],
   "source": [
    "# CELL 2: Metadata Extraction (Platform Agnostic - No Timestamp Manipulation)\n",
    "import os\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_file_metadata(file_path):\n",
    "    \"\"\"Get comprehensive file metadata - platform agnostic, honest timestamps\"\"\"\n",
    "    try:\n",
    "        file_stats = os.stat(file_path)\n",
    "        system = platform.system().lower()\n",
    "\n",
    "        # Platform-specific creation time handling - report what the OS actually provides\n",
    "        if system == \"windows\":\n",
    "            # Windows: st_ctime is actual creation time\n",
    "            creation_time = file_stats.st_ctime\n",
    "            creation_source = \"st_ctime (Windows creation time)\"\n",
    "            creation_reliable = True\n",
    "        elif hasattr(file_stats, \"st_birthtime\"):\n",
    "            # macOS/BSD: st_birthtime is true creation time\n",
    "            creation_time = file_stats.st_birthtime\n",
    "            creation_source = \"st_birthtime (macOS/BSD creation time)\"\n",
    "            creation_reliable = True\n",
    "        else:\n",
    "            # Linux/Other: No reliable creation time available\n",
    "            # Report st_ctime but mark as unreliable\n",
    "            creation_time = file_stats.st_ctime\n",
    "            creation_source = \"st_ctime (Linux - metadata change time, NOT creation)\"\n",
    "            creation_reliable = False\n",
    "\n",
    "        # Modification time is consistent across platforms\n",
    "        modification_time = file_stats.st_mtime\n",
    "\n",
    "        metadata = {\n",
    "            \"filename\": os.path.basename(file_path),\n",
    "            \"file_creation_date\": datetime.fromtimestamp(creation_time).strftime(\n",
    "                \"%Y-%m-%d %H:%M:%S\"\n",
    "            ),\n",
    "            \"file_modification_date\": datetime.fromtimestamp(\n",
    "                modification_time\n",
    "            ).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"file_location\": os.path.abspath(file_path),\n",
    "            \"platform\": system,\n",
    "            \"creation_source\": creation_source,\n",
    "            \"creation_reliable\": creation_reliable,  # Indicates if creation time is trustworthy\n",
    "        }\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting metadata for {file_path}: {e}\")\n",
    "        return {\n",
    "            \"filename\": os.path.basename(file_path) if file_path else \"Unknown\",\n",
    "            \"file_creation_date\": \"Unknown\",\n",
    "            \"file_modification_date\": \"Unknown\",\n",
    "            \"file_location\": (\n",
    "                os.path.abspath(file_path)\n",
    "                if file_path and os.path.exists(file_path)\n",
    "                else str(file_path)\n",
    "            ),\n",
    "            \"platform\": platform.system().lower(),\n",
    "            \"creation_source\": \"Error occurred\",\n",
    "            \"creation_reliable\": False,\n",
    "        }\n",
    "\n",
    "\n",
    "# Optional: Diagnostic function to show what timestamps are actually available\n",
    "def diagnose_file_timestamps(file_path):\n",
    "    \"\"\"Show all available timestamps for debugging\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        file_stats = os.stat(file_path)\n",
    "        system = platform.system().lower()\n",
    "\n",
    "        print(f\"\\nFile: {os.path.basename(file_path)}\")\n",
    "        print(f\"Platform: {system}\")\n",
    "        print(\"\\nAvailable timestamps:\")\n",
    "\n",
    "        if hasattr(file_stats, \"st_birthtime\"):\n",
    "            print(\n",
    "                f\"  st_birthtime: {datetime.fromtimestamp(file_stats.st_birthtime).strftime('%Y-%m-%d %H:%M:%S')} (creation time)\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"  st_birthtime: Not available\")\n",
    "\n",
    "        print(\n",
    "            f\"  st_ctime:     {datetime.fromtimestamp(file_stats.st_ctime).strftime('%Y-%m-%d %H:%M:%S')} ({'creation' if system == 'windows' else 'metadata change'} time)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  st_mtime:     {datetime.fromtimestamp(file_stats.st_mtime).strftime('%Y-%m-%d %H:%M:%S')} (modification time)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  st_atime:     {datetime.fromtimestamp(file_stats.st_atime).strftime('%Y-%m-%d %H:%M:%S')} (access time)\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading timestamps: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "30683"
   },
   "outputs": [],
   "source": [
    "# CELL 3: BMP files processing\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import zipcodes\n",
    "\n",
    "# Setup\n",
    "df1[\"filename\"] = df1[\"filename\"].astype(str)\n",
    "df1[\"file_creation_date\"] = df1[\"file_creation_date\"].astype(str)\n",
    "df1[\"file_modification_date\"] = df1[\"file_modification_date\"].astype(str)\n",
    "df1[\"file_location\"] = df1[\"file_location\"].astype(str)\n",
    "\n",
    "test_folder = os.path.join(os.getcwd(), \"test\")\n",
    "reader = easyocr.Reader([\"en\"], gpu=True)\n",
    "file_list = [f for f in os.listdir(test_folder) if f.lower().endswith(\".bmp\")]\n",
    "\n",
    "print(f\"Found {len(file_list)} BMP files in test folder\")\n",
    "\n",
    "# Initialize results list for CSV output\n",
    "csv_results = []\n",
    "\n",
    "\n",
    "def combine_spaced_alphanumeric(text):\n",
    "    \"\"\"Combine spaced alphanumeric characters that form a sequence\"\"\"\n",
    "    parts = [part for part in text.split() if part]\n",
    "    if len(parts) >= 3:\n",
    "        single_chars = [\n",
    "            part\n",
    "            for part in parts\n",
    "            if len(part) == 1 and (part.isalpha() or part.isdigit())\n",
    "        ]\n",
    "        if len(single_chars) >= 2:\n",
    "            combined = \"\".join(parts)\n",
    "            if 5 <= len(combined) <= 20 and combined.isalnum():\n",
    "                return combined\n",
    "    return text\n",
    "\n",
    "\n",
    "def find_serial(processed_results, all_text_combined, original_results):\n",
    "    \"\"\"Find serial number using multiple strategies\"\"\"\n",
    "    patterns = [\n",
    "        r\"\\b[A-Z0-9]{7,12}\\b\",\n",
    "        r\"\\b[A-Z0-9]{4,20}\\b\",\n",
    "        r\"\\b[A-Z]{1,2}[0-9]{4,10}\\b\",\n",
    "        r\"\\b[0-9]{4,15}\\b\",\n",
    "        r\"\\b[A-Z]{2,10}[0-9]{2,10}\\b\",\n",
    "        r\"\\b[A-Z]{3,15}\\b\",\n",
    "        r\"\\b[0-9]{5,15}\\b\",\n",
    "    ]\n",
    "\n",
    "    # Try processed results first\n",
    "    for pattern in patterns:\n",
    "        for bbox, text, confidence in processed_results:\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                candidate = match.group()\n",
    "                if (\n",
    "                    5 <= len(candidate) <= 20\n",
    "                    and candidate.lower()\n",
    "                    not in [\"image\", \"photo\", \"document\", \"serial\", \"number\"]\n",
    "                    and candidate.isalnum()\n",
    "                ):\n",
    "                    return candidate\n",
    "\n",
    "    # Try combined text\n",
    "    if all_text_combined:\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, all_text_combined)\n",
    "            if match:\n",
    "                candidate = match.group()\n",
    "                if 5 <= len(candidate) <= 20 and candidate.isalnum():\n",
    "                    return candidate\n",
    "\n",
    "    # Try spaced reconstruction\n",
    "    for bbox, text, confidence in original_results:\n",
    "        if \" \" in text and len(text.split()) >= 3:\n",
    "            parts = text.split()\n",
    "            single_chars = [p for p in parts if len(p) == 1]\n",
    "            if len(single_chars) >= 3:\n",
    "                reconstructed = \"\".join(parts)\n",
    "                if 5 <= len(reconstructed) <= 20 and reconstructed.isalnum():\n",
    "                    return reconstructed\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_metadata(processed_results):\n",
    "    \"\"\"Extract name, address, date, zipcode, city, state, and event name\"\"\"\n",
    "\n",
    "    # Business/organization names to exclude when looking for person names\n",
    "    business_exclusions = [\n",
    "        \"federal\",\n",
    "        \"firearms\",\n",
    "        \"gun\",\n",
    "        \"shop\",\n",
    "        \"store\",\n",
    "        \"company\",\n",
    "        \"corp\",\n",
    "        \"corporation\",\n",
    "        \"inc\",\n",
    "        \"incorporated\",\n",
    "        \"llc\",\n",
    "        \"ltd\",\n",
    "        \"limited\",\n",
    "        \"business\",\n",
    "        \"enterprise\",\n",
    "        \"arms\",\n",
    "        \"armory\",\n",
    "        \"tactical\",\n",
    "        \"shooting\",\n",
    "        \"sports\",\n",
    "        \"outdoor\",\n",
    "        \"hunting\",\n",
    "        \"dealer\",\n",
    "        \"sales\",\n",
    "        \"supply\",\n",
    "        \"equipment\",\n",
    "        \"services\",\n",
    "        \"solutions\",\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "    matches = 0\n",
    "\n",
    "    # Extract street address\n",
    "    street_address = None\n",
    "    address_pattern = r\"\\b\\d{1,5}\\s[A-Z][a-z]+\\s[A-Z][a-z]+\\b\"\n",
    "    for bbox, text, confidence in processed_results:\n",
    "        match = re.search(address_pattern, text)\n",
    "        if match:\n",
    "            street_address = match.group()\n",
    "            matches += 1\n",
    "            break\n",
    "\n",
    "    results[\"street_address\"] = street_address\n",
    "\n",
    "    # Special handling for person name extraction\n",
    "    name = None\n",
    "    name_candidates = []\n",
    "\n",
    "    # Look for person name patterns (2-3 words, proper case)\n",
    "    name_patterns = [\n",
    "        r\"\\b[A-Z][a-z]+\\s[A-Z][a-z]+\\s[A-Z][a-z]+\\b\",  # First Middle Last\n",
    "        r\"\\b[A-Z][a-z]+\\s[A-Z]\\.\\s[A-Z][a-z]+\\b\",  # First M. Last\n",
    "        r\"\\b[A-Z][a-z]+\\s[A-Z][a-z]+\\b\",  # First Last\n",
    "    ]\n",
    "\n",
    "    for pattern in name_patterns:\n",
    "        for bbox, text, confidence in processed_results:\n",
    "            matches_found = re.findall(pattern, text)\n",
    "            for match in matches_found:\n",
    "                # Check if this looks like a person name (not business)\n",
    "                match_lower = match.lower()\n",
    "                is_business = any(\n",
    "                    exclusion in match_lower for exclusion in business_exclusions\n",
    "                )\n",
    "\n",
    "                if not is_business and confidence > 0.7:\n",
    "                    name_candidates.append((match, confidence))\n",
    "\n",
    "    # Select the best name candidate\n",
    "    if name_candidates:\n",
    "        # Sort by confidence and prefer longer names (likely more complete)\n",
    "        name_candidates.sort(key=lambda x: (x[1], len(x[0])), reverse=True)\n",
    "        name = name_candidates[0][0]\n",
    "        matches += 1\n",
    "\n",
    "    results[\"name\"] = name\n",
    "\n",
    "    # Semantic event name extraction - focus on actual events, not form metadata\n",
    "    event_name = None\n",
    "    event_candidates = []\n",
    "\n",
    "    # Words to exclude (organizational/administrative terms, not events)\n",
    "    excluded_words = {\n",
    "        \"atf\",\n",
    "        \"bureau\",\n",
    "        \"federal\",\n",
    "        \"department\",\n",
    "        \"justice\",\n",
    "        \"alcohol\",\n",
    "        \"tobacco\",\n",
    "        \"firearms\",\n",
    "        \"explosives\",\n",
    "        \"form\",\n",
    "        \"section\",\n",
    "        \"page\",\n",
    "        \"number\",\n",
    "        \"code\",\n",
    "        \"licensee\",\n",
    "        \"information\",\n",
    "        \"details\",\n",
    "        \"description\",\n",
    "        \"brief\",\n",
    "        \"name\",\n",
    "        \"address\",\n",
    "        \"telephone\",\n",
    "        \"date\",\n",
    "        \"time\",\n",
    "        \"signature\",\n",
    "        \"certification\",\n",
    "    }\n",
    "\n",
    "    # PRIORITY 1: Look for document purpose indicators (what is being reported)\n",
    "    purpose_patterns = [\n",
    "        (\n",
    "            r\"(Theft|Loss|Stolen|Missing|Burglary|Robbery|Larceny).*?Report\",\n",
    "            \"Theft/Loss\",\n",
    "        ),\n",
    "        (r\"Inventory\\s+(Theft|Loss)\", \"Theft/Loss\"),\n",
    "        (\n",
    "            r\"(Purchase|Sale|Transfer|Registration|Acquisition|Disposition).*?Report\",\n",
    "            None,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for pattern, fixed_event in purpose_patterns:\n",
    "        for i, (bbox, text, confidence) in enumerate(processed_results):\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match and confidence > 0.4:\n",
    "                if fixed_event:\n",
    "                    event_type = fixed_event\n",
    "                else:\n",
    "                    event_type = match.group(1).title()\n",
    "                event_candidates.append(\n",
    "                    (event_type, confidence + 1.0, \"document_purpose\")\n",
    "                )\n",
    "\n",
    "    # PRIORITY 2: Look for specific incident/crime types\n",
    "    incident_types = {\n",
    "        \"burglary\": \"Burglary\",\n",
    "        \"robbery\": \"Robbery\",\n",
    "        \"larceny\": \"Larceny\",\n",
    "        \"theft\": \"Theft\",\n",
    "        \"stolen\": \"Theft\",\n",
    "        \"missing\": \"Missing\",\n",
    "        \"lost\": \"Loss\",\n",
    "    }\n",
    "\n",
    "    for i, (bbox, text, confidence) in enumerate(processed_results):\n",
    "        text_clean = text.lower().strip()\n",
    "        if text_clean in incident_types and confidence > 0.5:\n",
    "            event_candidates.append(\n",
    "                (incident_types[text_clean], confidence + 0.8, \"incident_type\")\n",
    "            )\n",
    "\n",
    "    # PRIORITY 3: Look for action descriptions (what happened to the firearm)\n",
    "    action_patterns = [\n",
    "        (r\"was\\s+(stolen|taken|missing|lost)\", None),\n",
    "        (r\"were\\s+(stolen|taken|missing|lost)\", None),\n",
    "        (r\"firearm\\s+was\\s+(\\w+)\", None),\n",
    "        (r\"gun\\s+was\\s+(\\w+)\", None),\n",
    "    ]\n",
    "\n",
    "    for pattern, _ in action_patterns:\n",
    "        for i, (bbox, text, confidence) in enumerate(processed_results):\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match and confidence > 0.6:\n",
    "                action = match.group(1).lower()\n",
    "                if action in incident_types:\n",
    "                    event_candidates.append(\n",
    "                        (incident_types[action], confidence + 0.6, \"action_description\")\n",
    "                    )\n",
    "\n",
    "    # PRIORITY 4: Look for section headers that indicate event type\n",
    "    section_patterns = [\n",
    "        (r\"(Theft|Loss|Stolen|Missing)\\s+Information\", None),\n",
    "        (r\"(Purchase|Sale|Transfer)\\s+Information\", None),\n",
    "    ]\n",
    "\n",
    "    for pattern, _ in section_patterns:\n",
    "        for i, (bbox, text, confidence) in enumerate(processed_results):\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match and confidence > 0.6:\n",
    "                event_type = match.group(1).title()\n",
    "                if event_type.lower() not in excluded_words:\n",
    "                    event_candidates.append(\n",
    "                        (event_type, confidence + 0.4, \"section_header\")\n",
    "                    )\n",
    "\n",
    "    # PRIORITY 5: Look for meaningful single words (but filter out administrative terms)\n",
    "    for i, (bbox, text, confidence) in enumerate(processed_results):\n",
    "        text_clean = text.strip()\n",
    "        if (\n",
    "            len(text_clean.split()) == 1\n",
    "            and len(text_clean) > 3\n",
    "            and confidence > 0.7\n",
    "            and text_clean.isalpha()\n",
    "            and text_clean.lower() not in excluded_words\n",
    "        ):\n",
    "            # Check if it's a potential event type\n",
    "            if text_clean.lower() in incident_types:\n",
    "                event_candidates.append(\n",
    "                    (\n",
    "                        incident_types[text_clean.lower()],\n",
    "                        confidence + 0.2,\n",
    "                        \"meaningful_word\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    # Select the best event candidate with semantic priority\n",
    "    if event_candidates:\n",
    "        # Sort by: 1) Source priority (semantic importance), 2) Confidence\n",
    "        priority_order = {\n",
    "            \"document_purpose\": 5,  # Highest - what the document is for\n",
    "            \"incident_type\": 4,  # High - specific crime/incident type\n",
    "            \"action_description\": 3,  # Medium-high - what happened\n",
    "            \"section_header\": 2,  # Medium - section context\n",
    "            \"meaningful_word\": 1,  # Low - single words\n",
    "        }\n",
    "\n",
    "        event_candidates.sort(\n",
    "            key=lambda x: (priority_order.get(x[2], 0), x[1]), reverse=True\n",
    "        )\n",
    "\n",
    "        event_name = event_candidates[0][0]\n",
    "        matches += 1\n",
    "\n",
    "    results[\"event_name\"] = event_name\n",
    "\n",
    "    # Extract event date - look for explicit event date labels, then find nearby dates\n",
    "    event_date = None\n",
    "    date_candidates = []\n",
    "\n",
    "    # PRIORITY 1: Look for explicit event date labels, then find nearby dates\n",
    "    event_label_patterns = [\n",
    "        r\"Date\\s+of\\s+(Theft|Loss|Stolen|Missing|Incident|Crime)\",\n",
    "        r\"(Theft|Loss|Stolen|Missing|Incident|Crime)\\s+.*?Date\",\n",
    "        r\"When\\s+.*?(stolen|lost|missing|taken)\",\n",
    "        r\"Date.*?(Discovered|Reported|Occurred)\",\n",
    "    ]\n",
    "\n",
    "    # First, find text blocks with event date labels\n",
    "    event_label_positions = []\n",
    "    for bbox, text, confidence in processed_results:\n",
    "        for pattern in event_label_patterns:\n",
    "            if re.search(pattern, text, re.IGNORECASE) and confidence > 0.4:\n",
    "                text_y = min([point[1] for point in bbox])\n",
    "                event_label_positions.append((text_y, text, confidence))\n",
    "                break\n",
    "\n",
    "    # Then, find dates near these labels\n",
    "    if event_label_positions:\n",
    "        for bbox, text, confidence in processed_results:\n",
    "            date_match = re.search(r\"\\b\\d{1,2}[/\\-]\\d{1,2}[/\\-]\\d{2,4}\\b\", text)\n",
    "            if date_match and confidence > 0.5:\n",
    "                date_y = min([point[1] for point in bbox])\n",
    "\n",
    "                # Find the closest event label to this date\n",
    "                for label_y, label_text, label_conf in event_label_positions:\n",
    "                    y_distance = abs(date_y - label_y)\n",
    "                    if y_distance < 150:  # Within 150 pixels of event label\n",
    "                        date_candidates.append(\n",
    "                            (\n",
    "                                date_match.group(),\n",
    "                                confidence + 1.0,\n",
    "                                \"explicit_event_context\",\n",
    "                                f\"Near label: {label_text}\",\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "    # PRIORITY 2: Look for dates near event-related text (if no explicit labels found)\n",
    "    if not date_candidates:\n",
    "        event_keywords = [\n",
    "            \"theft\",\n",
    "            \"loss\",\n",
    "            \"stolen\",\n",
    "            \"missing\",\n",
    "            \"larceny\",\n",
    "            \"burglary\",\n",
    "            \"incident\",\n",
    "        ]\n",
    "\n",
    "        # Find text blocks containing event keywords\n",
    "        event_text_positions = []\n",
    "        for i, (bbox, text, confidence) in enumerate(processed_results):\n",
    "            text_lower = text.lower()\n",
    "            if (\n",
    "                any(keyword in text_lower for keyword in event_keywords)\n",
    "                and confidence > 0.5\n",
    "            ):\n",
    "                text_y = min([point[1] for point in bbox])\n",
    "                event_text_positions.append((i, text_y, text))\n",
    "\n",
    "        # Look for dates near event-related text\n",
    "        for bbox, text, confidence in processed_results:\n",
    "            date_match = re.search(r\"\\b\\d{1,2}[/\\-]\\d{1,2}[/\\-]\\d{2,4}\\b\", text)\n",
    "            if date_match and confidence > 0.5:\n",
    "                text_y = min([point[1] for point in bbox])\n",
    "\n",
    "                # Check if this date is near any event-related text\n",
    "                for event_i, event_y, event_text in event_text_positions:\n",
    "                    y_distance = abs(text_y - event_y)\n",
    "                    if y_distance < 100:  # Within 100 pixels of event text\n",
    "                        date_candidates.append(\n",
    "                            (\n",
    "                                date_match.group(),\n",
    "                                confidence + 0.8,\n",
    "                                \"near_event_text\",\n",
    "                                f\"Near: {event_text}\",\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "    # PRIORITY 3: Exclude administrative dates\n",
    "    administrative_keywords = [\n",
    "        \"atf\",\n",
    "        \"notification\",\n",
    "        \"form\",\n",
    "        \"created\",\n",
    "        \"printed\",\n",
    "        \"letterhead\",\n",
    "        \"bureau\",\n",
    "    ]\n",
    "\n",
    "    # Filter out dates that are clearly administrative\n",
    "    filtered_candidates = []\n",
    "    for date_val, conf, source, context in date_candidates:\n",
    "        context_lower = context.lower()\n",
    "        is_administrative = any(\n",
    "            admin_word in context_lower for admin_word in administrative_keywords\n",
    "        )\n",
    "\n",
    "        if not is_administrative:\n",
    "            filtered_candidates.append((date_val, conf, source, context))\n",
    "\n",
    "    # Select the best event date\n",
    "    if filtered_candidates:\n",
    "        # Sort by source priority, then confidence\n",
    "        priority_order = {\n",
    "            \"explicit_event_context\": 3,  # Highest - explicit event date labels\n",
    "            \"near_event_text\": 2,  # Medium - dates near event descriptions\n",
    "            \"positional\": 1,  # Lowest - just positional filtering\n",
    "        }\n",
    "\n",
    "        filtered_candidates.sort(\n",
    "            key=lambda x: (priority_order.get(x[2], 0), x[1]), reverse=True\n",
    "        )\n",
    "        event_date = filtered_candidates[0][0]\n",
    "        matches += 1\n",
    "\n",
    "        # DEBUG: Uncomment to see date selection process\n",
    "        # print(f\"Event date selected: '{event_date}' from {filtered_candidates[0][2]} - Context: {filtered_candidates[0][3]}\")\n",
    "\n",
    "    elif date_candidates:\n",
    "        # Fallback to any date found, but still avoid administrative ones\n",
    "        event_date = date_candidates[0][0]\n",
    "        matches += 1\n",
    "    else:\n",
    "        # Final fallback: any date in the document (existing logic)\n",
    "        for bbox, text, confidence in processed_results:\n",
    "            date_match = re.search(r\"\\b\\d{1,2}[/\\-]\\d{1,2}[/\\-]\\d{2,4}\\b\", text)\n",
    "            if date_match and confidence > 0.5:\n",
    "                event_date = date_match.group()\n",
    "                matches += 1\n",
    "                break\n",
    "\n",
    "    results[\"event_date\"] = event_date\n",
    "\n",
    "    # Zipcode with positional filtering\n",
    "    zipcode = None\n",
    "    if processed_results:\n",
    "        all_y_coords = [point[1] for bbox, _, _ in processed_results for point in bbox]\n",
    "        image_height = max(all_y_coords) if all_y_coords else 0\n",
    "        top_boundary = image_height * 0.25\n",
    "        bottom_boundary = image_height * 0.50\n",
    "\n",
    "        candidates = []\n",
    "        for bbox, text, confidence in processed_results:\n",
    "            if re.match(r\"\\b\\d{5}(-\\d{4})?\\b\", text.strip()):\n",
    "                top_y = min([point[1] for point in bbox])\n",
    "                bottom_y = max([point[1] for point in bbox])\n",
    "                text_center_y = (top_y + bottom_y) / 2\n",
    "                if top_boundary <= text_center_y <= bottom_boundary:\n",
    "                    candidates.append((text.strip(), confidence))\n",
    "\n",
    "        if candidates:\n",
    "            zipcode = max(candidates, key=lambda x: x[1])[0]\n",
    "            matches += 1\n",
    "\n",
    "    # Get city/state from zipcode\n",
    "    city = state = None\n",
    "    if zipcode:\n",
    "        try:\n",
    "            location = zipcodes.matching(str(zipcode))\n",
    "            if location:\n",
    "                city = location[0][\"city\"]\n",
    "                state = location[0][\"state\"]\n",
    "                matches += 2\n",
    "        except Exception as e:\n",
    "            print(f\"Error looking up zipcode {zipcode}: {e}\")\n",
    "\n",
    "    # Construct complete address\n",
    "    complete_address = \"\"\n",
    "    if street_address:\n",
    "        complete_address = street_address\n",
    "        if city and state:\n",
    "            complete_address += f\", {city}, {state}\"\n",
    "            if zipcode:\n",
    "                complete_address += f\" {zipcode}\"\n",
    "        elif zipcode:\n",
    "            complete_address += f\" {zipcode}\"\n",
    "\n",
    "    results.update(\n",
    "        {\n",
    "            \"complete_address\": complete_address,\n",
    "            \"zipcode\": zipcode,\n",
    "            \"city\": city,\n",
    "            \"state\": state,\n",
    "            \"matches\": matches,\n",
    "        }\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "# Main processing loop\n",
    "for filename in file_list:\n",
    "    full_path = os.path.join(test_folder, filename)\n",
    "    relative_path = os.path.join(\"test\", filename)  # Create relative path\n",
    "    # print(f\"\\nProcessing BMP: {filename}\")\n",
    "\n",
    "    try:\n",
    "        # Load BMP with OpenCV and convert for EasyOCR\n",
    "        img_cv = cv2.imread(full_path)\n",
    "        if img_cv is not None and isinstance(img_cv, np.ndarray):\n",
    "            img_for_ocr = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n",
    "            results = reader.readtext(img_for_ocr)\n",
    "        else:\n",
    "            print(f\"Failed to load BMP file {filename}\")\n",
    "            results = []\n",
    "\n",
    "        # Visualize results (comment out if not needed)\n",
    "        img = cv2.imread(full_path)\n",
    "        if img is not None and isinstance(img, np.ndarray):\n",
    "            for bbox, text, confidence in results:\n",
    "                pts = np.array(bbox, np.int32).reshape((-1, 1, 2))\n",
    "                cv2.polylines(img, [pts], isClosed=True, color=(0, 0, 255), thickness=2)\n",
    "                x, y = pts[0][0]\n",
    "                cv2.putText(\n",
    "                    img, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.imshow(img_rgb)\n",
    "                plt.axis(\"off\")\n",
    "                plt.title(f\"OCR Results for {filename}\")\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not display image {filename}\")\n",
    "\n",
    "        # Process OCR results\n",
    "        processed_results = []\n",
    "        for bbox, text, confidence in results:\n",
    "            processed_text = combine_spaced_alphanumeric(text)\n",
    "            processed_results.append((bbox, processed_text, confidence))\n",
    "\n",
    "        all_text_combined = \" \".join([text for _, text, _ in processed_results])\n",
    "\n",
    "        # Find serial number\n",
    "        serial_num = find_serial(processed_results, all_text_combined, results)\n",
    "\n",
    "        # Extract metadata\n",
    "        metadata_info = extract_metadata(processed_results)\n",
    "\n",
    "        # Get file metadata using absolute path but store relative path\n",
    "        file_metadata = get_file_metadata(full_path)\n",
    "\n",
    "        # Create CSV record with fixed address and event date\n",
    "        csv_record = {\n",
    "            \"serial_number\": str(serial_num) if serial_num else \"\",\n",
    "            \"event_type\": (\n",
    "                str(metadata_info[\"event_name\"]) if metadata_info[\"event_name\"] else \"\"\n",
    "            ),\n",
    "            \"event_date\": (\n",
    "                str(metadata_info[\"event_date\"]) if metadata_info[\"event_date\"] else \"\"\n",
    "            ),\n",
    "            \"associated_name\": (\n",
    "                str(metadata_info[\"name\"]) if metadata_info[\"name\"] else \"\"\n",
    "            ),\n",
    "            \"associated_address\": (\n",
    "                str(metadata_info[\"complete_address\"])\n",
    "                if metadata_info[\"complete_address\"]\n",
    "                else \"\"\n",
    "            ),\n",
    "            \"source_file\": relative_path,\n",
    "            \"file_created\": file_metadata[\"file_creation_date\"],\n",
    "            \"file_modified\": file_metadata[\"file_modification_date\"],\n",
    "        }\n",
    "\n",
    "        csv_results.append(csv_record)\n",
    "\n",
    "        # Add to dataframe (including event_name)\n",
    "        new_row = {\n",
    "            \"serial_number\": str(serial_num) if serial_num else None,\n",
    "            \"name\": str(metadata_info[\"name\"]) if metadata_info[\"name\"] else None,\n",
    "            \"address\": (\n",
    "                str(metadata_info[\"street_address\"])\n",
    "                if metadata_info[\"street_address\"]\n",
    "                else None\n",
    "            ),\n",
    "            \"date\": (\n",
    "                str(metadata_info[\"event_date\"])\n",
    "                if metadata_info[\"event_date\"]\n",
    "                else None\n",
    "            ),\n",
    "            \"zipcode\": (\n",
    "                str(metadata_info[\"zipcode\"]) if metadata_info[\"zipcode\"] else None\n",
    "            ),\n",
    "            \"city\": str(metadata_info[\"city\"]) if metadata_info[\"city\"] else None,\n",
    "            \"state\": str(metadata_info[\"state\"]) if metadata_info[\"state\"] else None,\n",
    "            \"event_name\": (\n",
    "                str(metadata_info[\"event_name\"])\n",
    "                if metadata_info[\"event_name\"]\n",
    "                else None\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        df1 = pd.concat([df1, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "        # Add file metadata to df1 with relative path\n",
    "        if serial_num:\n",
    "            df1.loc[df1[\"serial_number\"] == str(serial_num), \"filename\"] = filename\n",
    "            df1.loc[df1[\"serial_number\"] == str(serial_num), \"file_creation_date\"] = (\n",
    "                file_metadata[\"file_creation_date\"]\n",
    "            )\n",
    "            df1.loc[\n",
    "                df1[\"serial_number\"] == str(serial_num), \"file_modification_date\"\n",
    "            ] = file_metadata[\"file_modification_date\"]\n",
    "            df1.loc[df1[\"serial_number\"] == str(serial_num), \"file_location\"] = (\n",
    "                relative_path\n",
    "            )\n",
    "\n",
    "        # Enhanced detailed per-image output for control\n",
    "        detail_line = (\n",
    "            f'✓ Processed: {filename} | Serial: {serial_num or \"None\"} | '\n",
    "            f'Event: {metadata_info.get(\"event_name\") or \"None\"} | Date: {metadata_info.get(\"event_date\") or \"None\"} | '\n",
    "            f'Name: {metadata_info.get(\"name\") or \"None\"} | Street: {metadata_info.get(\"street_address\") or \"None\"} | '\n",
    "            f'Zip: {metadata_info.get(\"zipcode\") or \"None\"} | City: {metadata_info.get(\"city\") or \"None\"} | State: {metadata_info.get(\"state\") or \"None\"} | '\n",
    "            f'CompleteAddr: {metadata_info.get(\"complete_address\") or \"None\"}'\n",
    "        )\n",
    "        print(detail_line)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing BMP {filename}: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\nBMP processing complete!\")\n",
    "\n",
    "# Create and display CSV-formatted results\n",
    "if csv_results:\n",
    "    results_df = pd.DataFrame(csv_results)\n",
    "    # print(\"\\n\" + \"=\" * 80)\n",
    "    # print(\"EXTRACTION RESULTS (CSV FORMAT)\")\n",
    "    # print(\"=\" * 80)\n",
    "    # print(results_df.to_csv(index=False))\n",
    "\n",
    "    # Create reports folder if it doesn't exist\n",
    "    reports_folder = os.path.join(os.getcwd(), \"reports\")\n",
    "    os.makedirs(reports_folder, exist_ok=True)\n",
    "\n",
    "    # Save to file in reports folder\n",
    "    output_file = os.path.join(reports_folder, \"bmp_extraction_results.csv\")\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    # print(f\"\\nResults saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"\\nNo results to display.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "798b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: test/3310 BAHR353.bmp\n",
      "Raw timestamps (Unix epoch):\n",
      "  st_birthtime: 1754294258.1703198\n",
      "  st_birthtime formatted: 2025-08-04 03:57:38.170320\n",
      "  st_ctime: 1754294469.162189\n",
      "  st_ctime formatted: 2025-08-04 04:01:09.162189\n",
      "  st_mtime: 1754294258.385938\n",
      "  st_mtime formatted: 2025-08-04 03:57:38.385938\n",
      "\n",
      "Our function uses: st_birthtime\n",
      "Creation: 2025-08-04 03:57:38.170320\n",
      "Modification: 2025-08-04 03:57:38.385938\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Diagnostic code - add this to a cell and run it on one of your BMP files\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def debug_timestamps(file_path):\n",
    "    \"\"\"Debug what timestamps are actually being extracted\"\"\"\n",
    "    try:\n",
    "        file_stats = os.stat(file_path)\n",
    "\n",
    "        print(f\"File: {file_path}\")\n",
    "        print(\"Raw timestamps (Unix epoch):\")\n",
    "\n",
    "        if hasattr(file_stats, \"st_birthtime\"):\n",
    "            print(f\"  st_birthtime: {file_stats.st_birthtime}\")\n",
    "            print(\n",
    "                f\"  st_birthtime formatted: {datetime.fromtimestamp(file_stats.st_birthtime)}\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"  st_birthtime: Not available\")\n",
    "\n",
    "        print(f\"  st_ctime: {file_stats.st_ctime}\")\n",
    "        print(f\"  st_ctime formatted: {datetime.fromtimestamp(file_stats.st_ctime)}\")\n",
    "\n",
    "        print(f\"  st_mtime: {file_stats.st_mtime}\")\n",
    "        print(f\"  st_mtime formatted: {datetime.fromtimestamp(file_stats.st_mtime)}\")\n",
    "\n",
    "        # Show what our function would return\n",
    "        if hasattr(file_stats, \"st_birthtime\"):\n",
    "            creation_time = file_stats.st_birthtime\n",
    "            source = \"st_birthtime\"\n",
    "        else:\n",
    "            creation_time = file_stats.st_ctime\n",
    "            source = \"st_ctime\"\n",
    "\n",
    "        print(f\"\\nOur function uses: {source}\")\n",
    "        print(f\"Creation: {datetime.fromtimestamp(creation_time)}\")\n",
    "        print(f\"Modification: {datetime.fromtimestamp(file_stats.st_mtime)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "# Test on one of your BMP files\n",
    "debug_timestamps(\"test/3310 BAHR353.bmp\")  # Replace with actual file path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742443e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5.0: Process non BMP image files with OCR-focused serial extraction and CSV output\n",
    "# Add metadata columns to df1\n",
    "df1[\"filename\"] = df1[\"filename\"].astype(str)\n",
    "df1[\"file_creation_date\"] = df1[\"file_creation_date\"].astype(str)\n",
    "df1[\"file_modification_date\"] = df1[\"file_modification_date\"].astype(str)\n",
    "df1[\"file_location\"] = df1[\"file_location\"].astype(str)\n",
    "\n",
    "# Path to 'test' folder\n",
    "test_folder = os.path.join(os.getcwd(), \"test\")\n",
    "\n",
    "# Initialize EasyOCR\n",
    "reader = easyocr.Reader([\"en\"], gpu=True)\n",
    "\n",
    "# Get all files in 'test' folder\n",
    "file_list = os.listdir(test_folder)\n",
    "\n",
    "# Initialize CSV results list\n",
    "csv_results = []\n",
    "\n",
    "print(f\"Found {len(file_list)} files in test folder\")\n",
    "\n",
    "\n",
    "def extract_serial_with_improved_ocr(results, full_path):\n",
    "    \"\"\"Extract serial number focusing on OCR accuracy improvements\"\"\"\n",
    "\n",
    "    def preprocess_image_for_ocr(image_path):\n",
    "        \"\"\"Apply image preprocessing to improve OCR accuracy\"\"\"\n",
    "        import cv2\n",
    "        import numpy as np\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return None\n",
    "\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply different preprocessing techniques\n",
    "        preprocessed_images = []\n",
    "\n",
    "        # Original grayscale\n",
    "        preprocessed_images.append((\"original_gray\", gray))\n",
    "\n",
    "        # Gaussian blur to reduce noise\n",
    "        blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        preprocessed_images.append((\"blurred\", blurred))\n",
    "\n",
    "        # Sharpen\n",
    "        kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "        sharpened = cv2.filter2D(gray, -1, kernel)\n",
    "        preprocessed_images.append((\"sharpened\", sharpened))\n",
    "\n",
    "        # Adaptive threshold\n",
    "        adaptive = cv2.adaptiveThreshold(\n",
    "            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
    "        )\n",
    "        preprocessed_images.append((\"adaptive_thresh\", adaptive))\n",
    "\n",
    "        # Morphological operations to clean up\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "        morph = cv2.morphologyEx(adaptive, cv2.MORPH_CLOSE, kernel)\n",
    "        preprocessed_images.append((\"morphological\", morph))\n",
    "\n",
    "        return preprocessed_images\n",
    "\n",
    "    def run_multiple_ocr_passes(image_path):\n",
    "        \"\"\"Run OCR with different parameters and preprocessing\"\"\"\n",
    "        all_candidates = []\n",
    "\n",
    "        # Standard OCR (already done)\n",
    "        for bbox, text, confidence in results:\n",
    "            if text.strip() and confidence > 0.3:\n",
    "                all_candidates.append(\n",
    "                    {\n",
    "                        \"text\": text.strip(),\n",
    "                        \"confidence\": confidence,\n",
    "                        \"source\": \"standard_ocr\",\n",
    "                        \"bbox\": bbox,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Try different preprocessing\n",
    "        preprocessed_images = preprocess_image_for_ocr(image_path)\n",
    "        if preprocessed_images:\n",
    "            for prep_name, prep_img in preprocessed_images:\n",
    "                try:\n",
    "                    # Run OCR on preprocessed image\n",
    "                    prep_results = reader.readtext(\n",
    "                        prep_img,\n",
    "                        detail=1,\n",
    "                        paragraph=False,\n",
    "                        text_threshold=0.1,  # Lower threshold\n",
    "                        low_text=0.05,\n",
    "                        link_threshold=0.3,\n",
    "                        mag_ratio=2.0,  # Higher magnification\n",
    "                    )\n",
    "\n",
    "                    for bbox, text, confidence in prep_results:\n",
    "                        if (\n",
    "                            text.strip() and confidence > 0.2\n",
    "                        ):  # Lower threshold for preprocessed\n",
    "                            all_candidates.append(\n",
    "                                {\n",
    "                                    \"text\": text.strip(),\n",
    "                                    \"confidence\": confidence\n",
    "                                    * 0.9,  # Slight penalty for preprocessed\n",
    "                                    \"source\": f\"preprocessed_{prep_name}\",\n",
    "                                    \"bbox\": bbox,\n",
    "                                }\n",
    "                            )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {prep_name} preprocessing: {e}\")\n",
    "\n",
    "        return all_candidates\n",
    "\n",
    "    def find_potential_serials(candidates):\n",
    "        \"\"\"Find potential serial numbers from all OCR candidates\"\"\"\n",
    "        potential_serials = []\n",
    "\n",
    "        # Common words that are clearly NOT serial numbers\n",
    "        non_serial_words = {\n",
    "            \"date\",\n",
    "            \"city\",\n",
    "            \"state\",\n",
    "            \"name\",\n",
    "            \"address\",\n",
    "            \"phone\",\n",
    "            \"email\",\n",
    "            \"zip\",\n",
    "            \"code\",\n",
    "            \"serial\",\n",
    "            \"number\",\n",
    "            \"model\",\n",
    "            \"make\",\n",
    "            \"caliber\",\n",
    "            \"type\",\n",
    "            \"manufacturer\",\n",
    "            \"license\",\n",
    "            \"permit\",\n",
    "            \"registration\",\n",
    "            \"form\",\n",
    "            \"page\",\n",
    "            \"section\",\n",
    "            \"dallas\",\n",
    "            \"texas\",\n",
    "            \"california\",\n",
    "            \"florida\",\n",
    "            \"new\",\n",
    "            \"york\",\n",
    "            \"smith\",\n",
    "            \"wesson\",\n",
    "            \"colt\",\n",
    "            \"ruger\",\n",
    "            \"glock\",\n",
    "            \"sig\",\n",
    "            \"sauer\",\n",
    "            \"the\",\n",
    "            \"and\",\n",
    "            \"for\",\n",
    "            \"with\",\n",
    "            \"this\",\n",
    "            \"that\",\n",
    "            \"from\",\n",
    "            \"have\",\n",
    "            \"been\",\n",
    "            \"firearm\",\n",
    "            \"pistol\",\n",
    "            \"rifle\",\n",
    "            \"gun\",\n",
    "            \"weapon\",\n",
    "            \"barrel\",\n",
    "            \"frame\",\n",
    "            \"slide\",\n",
    "        }\n",
    "\n",
    "        for candidate in candidates:\n",
    "            text = candidate[\"text\"]\n",
    "            confidence = candidate[\"confidence\"]\n",
    "            source = candidate[\"source\"]\n",
    "\n",
    "            # Remove spaces and special characters\n",
    "            cleaned = \"\".join(c for c in text if c.isalnum())\n",
    "\n",
    "            # Skip if it's clearly not a serial number\n",
    "            if cleaned.lower() in non_serial_words:\n",
    "                continue\n",
    "\n",
    "            # Skip if it's all letters and looks like a common word (likely not a serial)\n",
    "            if cleaned.isalpha() and len(cleaned) <= 8:\n",
    "                continue\n",
    "\n",
    "            # Check if it could be a serial (basic length check)\n",
    "            if 4 <= len(cleaned) <= 15 and cleaned.isalnum():\n",
    "                # Additional check: prefer mixed alphanumeric or longer sequences\n",
    "                has_letters = any(c.isalpha() for c in cleaned)\n",
    "                has_numbers = any(c.isdigit() for c in cleaned)\n",
    "\n",
    "                # Boost confidence for mixed alphanumeric (more likely to be serials)\n",
    "                if has_letters and has_numbers:\n",
    "                    confidence *= 1.2\n",
    "                elif len(cleaned) >= 6:  # Or longer sequences\n",
    "                    confidence *= 1.1\n",
    "\n",
    "                potential_serials.append(\n",
    "                    {\n",
    "                        \"serial\": cleaned,\n",
    "                        \"original_text\": text,\n",
    "                        \"confidence\": confidence,\n",
    "                        \"source\": source,\n",
    "                        \"bbox\": candidate[\"bbox\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Also try spaced character reconstruction for this candidate\n",
    "            if \" \" in text and len(text.split()) >= 3:\n",
    "                parts = [p.strip() for p in text.split() if p.strip().isalnum()]\n",
    "                if len(parts) >= 3:\n",
    "                    reconstructed = \"\".join(parts)\n",
    "                    if (\n",
    "                        4 <= len(reconstructed) <= 15\n",
    "                        and reconstructed.isalnum()\n",
    "                        and reconstructed.lower() not in non_serial_words\n",
    "                    ):\n",
    "                        potential_serials.append(\n",
    "                            {\n",
    "                                \"serial\": reconstructed,\n",
    "                                \"original_text\": text,\n",
    "                                \"confidence\": confidence * 0.8,\n",
    "                                \"source\": f\"{source}_reconstructed\",\n",
    "                                \"bbox\": candidate[\"bbox\"],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "        return potential_serials\n",
    "\n",
    "    # Run multiple OCR passes\n",
    "    print(\"Running multiple OCR passes for improved accuracy...\")\n",
    "    all_candidates = run_multiple_ocr_passes(full_path)\n",
    "\n",
    "    # Find potential serials\n",
    "    potential_serials = find_potential_serials(all_candidates)\n",
    "\n",
    "    # Remove duplicates and sort by confidence\n",
    "    unique_serials = {}\n",
    "    for serial_info in potential_serials:\n",
    "        serial = serial_info[\"serial\"]\n",
    "        if (\n",
    "            serial not in unique_serials\n",
    "            or serial_info[\"confidence\"] > unique_serials[serial][\"confidence\"]\n",
    "        ):\n",
    "            unique_serials[serial] = serial_info\n",
    "\n",
    "    # Sort by confidence\n",
    "    sorted_serials = sorted(\n",
    "        unique_serials.values(), key=lambda x: x[\"confidence\"], reverse=True\n",
    "    )\n",
    "\n",
    "    # Display top candidates for transparency\n",
    "    print(f\"\\nSerial number candidates found:\")\n",
    "    if sorted_serials:\n",
    "        for i, serial_info in enumerate(sorted_serials[:5]):  # Show top 5\n",
    "            marker = \"→ SELECTED\" if i == 0 else \"  \"\n",
    "            print(\n",
    "                f\"  {marker} '{serial_info['serial']}' (confidence: {serial_info['confidence']:.3f}) from {serial_info['source']}\"\n",
    "            )\n",
    "            print(f\"      Original text: '{serial_info['original_text']}'\")\n",
    "        return sorted_serials[0][\"serial\"]\n",
    "    else:\n",
    "        print(\"  No potential serial numbers found\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_event_type_semantic(results):\n",
    "    \"\"\"\n",
    "    Mirror BMP-style event detection with semantic priorities.\n",
    "    Returns best event_type or None.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    event_candidates = []\n",
    "\n",
    "    # Excluded administrative words\n",
    "    excluded_words = {\n",
    "        \"atf\",\n",
    "        \"bureau\",\n",
    "        \"federal\",\n",
    "        \"department\",\n",
    "        \"justice\",\n",
    "        \"alcohol\",\n",
    "        \"tobacco\",\n",
    "        \"firearms\",\n",
    "        \"explosives\",\n",
    "        \"form\",\n",
    "        \"section\",\n",
    "        \"page\",\n",
    "        \"number\",\n",
    "        \"code\",\n",
    "        \"licensee\",\n",
    "        \"information\",\n",
    "        \"details\",\n",
    "        \"description\",\n",
    "        \"brief\",\n",
    "        \"name\",\n",
    "        \"address\",\n",
    "        \"telephone\",\n",
    "        \"date\",\n",
    "        \"time\",\n",
    "        \"signature\",\n",
    "        \"certification\",\n",
    "    }\n",
    "\n",
    "    # Priority 1: Document purpose indicators\n",
    "    purpose_patterns = [\n",
    "        (\n",
    "            r\"(Theft|Loss|Stolen|Missing|Burglary|Robbery|Larceny).*?Report\",\n",
    "            \"Theft/Loss\",\n",
    "        ),\n",
    "        (r\"Inventory\\s+(Theft|Loss)\", \"Theft/Loss\"),\n",
    "        (\n",
    "            r\"(Purchase|Sale|Transfer|Registration|Acquisition|Disposition).*?Report\",\n",
    "            None,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for pattern, fixed_event in purpose_patterns:\n",
    "        for _, text, conf in results:\n",
    "            m = re.search(pattern, text, re.IGNORECASE)\n",
    "            if m and conf > 0.4:\n",
    "                event = fixed_event if fixed_event else m.group(1).title()\n",
    "                event_candidates.append((event, conf + 1.0, \"document_purpose\", text))\n",
    "\n",
    "    # Priority 2: Specific incident/crime types\n",
    "    incident_types = {\n",
    "        \"burglary\": \"Burglary\",\n",
    "        \"robbery\": \"Robbery\",\n",
    "        \"larceny\": \"Larceny\",\n",
    "        \"theft\": \"Theft\",\n",
    "        \"stolen\": \"Theft\",\n",
    "        \"missing\": \"Missing\",\n",
    "        \"lost\": \"Loss\",\n",
    "    }\n",
    "    for _, text, conf in results:\n",
    "        text_clean = text.lower().strip()\n",
    "        if text_clean in incident_types and conf > 0.5:\n",
    "            event_candidates.append(\n",
    "                (incident_types[text_clean], conf + 0.8, \"incident_type\", text)\n",
    "            )\n",
    "\n",
    "    # Priority 3: Action descriptions\n",
    "    action_patterns = [\n",
    "        (r\"was\\s+(stolen|taken|missing|lost)\", None),\n",
    "        (r\"were\\s+(stolen|taken|missing|lost)\", None),\n",
    "        (r\"firearm\\s+was\\s+(\\w+)\", None),\n",
    "        (r\"gun\\s+was\\s+(\\w+)\", None),\n",
    "    ]\n",
    "    for pattern, _ in action_patterns:\n",
    "        for _, text, conf in results:\n",
    "            m = re.search(pattern, text, re.IGNORECASE)\n",
    "            if m and conf > 0.6:\n",
    "                action = m.group(1).lower()\n",
    "                if action in incident_types:\n",
    "                    event_candidates.append(\n",
    "                        (incident_types[action], conf + 0.6, \"action_description\", text)\n",
    "                    )\n",
    "\n",
    "    # Priority 4: Section headers\n",
    "    section_patterns = [\n",
    "        (r\"(Theft|Loss|Stolen|Missing)\\s+Information\", None),\n",
    "        (r\"(Purchase|Sale|Transfer)\\s+Information\", None),\n",
    "    ]\n",
    "    for pattern, _ in section_patterns:\n",
    "        for _, text, conf in results:\n",
    "            m = re.search(pattern, text, re.IGNORECASE)\n",
    "            if m and conf > 0.6:\n",
    "                event = m.group(1).title()\n",
    "                if event.lower() not in excluded_words:\n",
    "                    event_candidates.append((event, conf + 0.4, \"section_header\", text))\n",
    "\n",
    "    # Priority 5: Meaningful single words\n",
    "    for _, text, conf in results:\n",
    "        t = text.strip()\n",
    "        if len(t.split()) == 1 and len(t) > 3 and conf > 0.7 and t.isalpha():\n",
    "            low = t.lower()\n",
    "            if low not in excluded_words and low in incident_types:\n",
    "                event_candidates.append(\n",
    "                    (incident_types[low], conf + 0.2, \"meaningful_word\", text)\n",
    "                )\n",
    "\n",
    "    if not event_candidates:\n",
    "        return None\n",
    "\n",
    "    priority_order = {\n",
    "        \"document_purpose\": 5,\n",
    "        \"incident_type\": 4,\n",
    "        \"action_description\": 3,\n",
    "        \"section_header\": 2,\n",
    "        \"meaningful_word\": 1,\n",
    "    }\n",
    "    event_candidates.sort(\n",
    "        key=lambda x: (priority_order.get(x[2], 0), x[1]), reverse=True\n",
    "    )\n",
    "    return event_candidates[0][0]\n",
    "\n",
    "\n",
    "for filename in file_list:\n",
    "    full_path = os.path.join(test_folder, filename)\n",
    "\n",
    "    # Skip if it's a folder\n",
    "    if os.path.isdir(full_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing: {filename}\")\n",
    "\n",
    "    # Check if it's an image file (excluding BMP which is handled in another cell)\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".tiff\", \".gif\")):\n",
    "        try:\n",
    "            # Process non-BMP image formats with enhanced parameters\n",
    "            results = reader.readtext(\n",
    "                full_path,\n",
    "                detail=1,\n",
    "                paragraph=False,\n",
    "                text_threshold=0.2,\n",
    "                low_text=0.1,\n",
    "                link_threshold=0.4,\n",
    "                mag_ratio=1.5,\n",
    "            )\n",
    "\n",
    "            # Place file in a DataFrame\n",
    "            img_id = filename.split(\"/\")[-1].split(\".\")[0]\n",
    "            box_dataframe = pd.DataFrame(\n",
    "                results, columns=[\"bbox\", \"text\", \"confidence\"]\n",
    "            )\n",
    "            box_dataframe[\"img_id\"] = img_id\n",
    "\n",
    "            # Read image for visualization\n",
    "            img = cv2.imread(full_path)\n",
    "\n",
    "            # Display image with detected text if loaded successfully\n",
    "            if img is not None and isinstance(img, np.ndarray):\n",
    "                for bbox, text, confidence in results:\n",
    "                    pts = np.array(bbox, np.int32)\n",
    "                    pts = pts.reshape((-1, 1, 2))\n",
    "                    cv2.polylines(\n",
    "                        img, [pts], isClosed=True, color=(0, 0, 255), thickness=2\n",
    "                    )\n",
    "                    x, y = pts[0][0]\n",
    "                    cv2.putText(\n",
    "                        img,\n",
    "                        text,\n",
    "                        (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        1,\n",
    "                        (0, 0, 255),\n",
    "                        2,\n",
    "                    )\n",
    "\n",
    "                # Display the image\n",
    "                try:\n",
    "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    plt.figure(figsize=(12, 8))\n",
    "                    plt.imshow(img_rgb)\n",
    "                    plt.axis(\"off\")\n",
    "                    plt.title(f\"OCR Results for {filename}\")\n",
    "                    plt.show()\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not display image {filename}\")\n",
    "\n",
    "            # OCR-focused serial extraction\n",
    "            print(\"\\n--- OCR-Focused Serial Number Detection ---\")\n",
    "\n",
    "            # Use OCR-focused extraction\n",
    "            serial_num = extract_serial_with_improved_ocr(results, full_path)\n",
    "\n",
    "            if not serial_num:\n",
    "                print(\"No serial number found with OCR-focused extraction.\")\n",
    "                # Show all detected text for manual inspection\n",
    "                if results:\n",
    "                    print(\"All OCR detections:\")\n",
    "                    for i, (bbox, text, confidence) in enumerate(results):\n",
    "                        print(f\"  {i+1}. '{text}' (confidence: {confidence:.4f})\")\n",
    "\n",
    "            print(\"--- End OCR-Focused Serial Number Detection ---\")\n",
    "\n",
    "            # Extract other metadata\n",
    "            matches = 0\n",
    "\n",
    "            # Initialize variables to avoid NameError\n",
    "            name = None\n",
    "            addy = None\n",
    "            date = None\n",
    "            zipcode = None\n",
    "            city = None\n",
    "            state = None\n",
    "\n",
    "            # Name extraction\n",
    "            name_pat = r\"\\b[A-Z][a-z]+\\s[A-Z][a-z]+\\b\"\n",
    "            for bbox, text, confidence in results:\n",
    "                match = re.search(name_pat, text)\n",
    "                if match and confidence > 0.8:\n",
    "                    name = match.group()\n",
    "                    matches += 1\n",
    "                    break\n",
    "\n",
    "            # Address extraction\n",
    "            address_pat = r\"\\b\\d{1,5}\\s[A-Z][a-z]+\\s[A-Z][a-z]+\\b\"\n",
    "            for bbox, text, confidence in results:\n",
    "                match = re.search(address_pat, text)\n",
    "                if match:\n",
    "                    addy = match.group()\n",
    "                    matches += 1\n",
    "                    break\n",
    "\n",
    "            # Date extraction (support / and -)\n",
    "            date_pat = r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b\"\n",
    "            for bbox, text, confidence in results:\n",
    "                match = re.search(date_pat, text)\n",
    "                if match:\n",
    "                    date = match.group()\n",
    "                    matches += 1\n",
    "                    break\n",
    "\n",
    "            # Event type extraction (semantic - mirrored from BMP logic)\n",
    "            event_type = extract_event_type_semantic(results)\n",
    "            if event_type:\n",
    "                matches += 1\n",
    "            else:\n",
    "                event_type = None\n",
    "\n",
    "            # Zipcode extraction\n",
    "            zipcode_pat = r\"\\b\\d{5}(-\\d{4})?\\b\"\n",
    "            if results:\n",
    "                # Get image dimensions for targeted zipcode search\n",
    "                all_y_coords = [point[1] for bbox, _, _ in results for point in bbox]\n",
    "                image_height = max(all_y_coords) if all_y_coords else 0\n",
    "\n",
    "                # Target middle region for zipcodes\n",
    "                top_boundary = image_height * 0.25\n",
    "                bottom_boundary = image_height * 0.50\n",
    "\n",
    "                # Find zipcode candidates\n",
    "                candidates = []\n",
    "                for bbox, text, confidence in results:\n",
    "                    text = text.strip()\n",
    "                    if re.match(zipcode_pat, text):\n",
    "                        # Get text position\n",
    "                        top_y = min([point[1] for point in bbox])\n",
    "                        bottom_y = max([point[1] for point in bbox])\n",
    "                        text_center_y = (top_y + bottom_y) / 2\n",
    "\n",
    "                        # Check if in target region\n",
    "                        if top_boundary <= text_center_y <= bottom_boundary:\n",
    "                            candidates.append((text, confidence, text_center_y))\n",
    "\n",
    "                if candidates:\n",
    "                    # Return the candidate with highest confidence\n",
    "                    best_candidate = max(candidates, key=lambda x: x[1])\n",
    "                    zipcode = best_candidate[0]\n",
    "                    matches += 1\n",
    "\n",
    "            # Get city and state information from zipcode\n",
    "            if zipcode:\n",
    "                try:\n",
    "                    end = zipcodes.matching(str(zipcode))\n",
    "                    if end:\n",
    "                        city = end[0][\"city\"]\n",
    "                        state = end[0][\"state\"]\n",
    "                        matches += 2\n",
    "                except Exception as e:\n",
    "                    print(f\"Error looking up zipcode {zipcode}: {e}\")\n",
    "\n",
    "            # Get file metadata\n",
    "            metadata = get_file_metadata(full_path)\n",
    "\n",
    "            # Create relative path for source_file\n",
    "            relative_path = os.path.join(\"test\", filename)\n",
    "\n",
    "            # Construct complete address\n",
    "            complete_address = \"\"\n",
    "            if addy:\n",
    "                complete_address = addy\n",
    "                if city and state:\n",
    "                    complete_address += f\", {city}, {state}\"\n",
    "                    if zipcode:\n",
    "                        complete_address += f\" {zipcode}\"\n",
    "                elif zipcode:\n",
    "                    complete_address += f\" {zipcode}\"\n",
    "\n",
    "            # Create CSV record with standardized columns, using 'Missing' for empty values\n",
    "            csv_record = {\n",
    "                \"serial_number\": str(serial_num) if serial_num else \"Missing\",\n",
    "                \"event_type\": str(event_type) if event_type else \"Missing\",\n",
    "                \"event_date\": str(date) if date else \"Missing\",\n",
    "                \"associated_name\": str(name) if name else \"Missing\",\n",
    "                \"associated_address\": (\n",
    "                    str(complete_address) if complete_address else \"Missing\"\n",
    "                ),\n",
    "                \"source_file\": relative_path,\n",
    "                \"file_created\": metadata[\"file_creation_date\"],\n",
    "                \"file_modified\": metadata[\"file_modification_date\"],\n",
    "            }\n",
    "\n",
    "            csv_results.append(csv_record)\n",
    "\n",
    "            # Add extracted information to df1 (legacy format)\n",
    "            new_row = {\n",
    "                \"serial_number\": str(serial_num) if serial_num else None,\n",
    "                \"name\": str(name) if name else None,\n",
    "                \"address\": str(addy) if addy else None,\n",
    "                \"date\": str(date) if date else None,\n",
    "                \"zipcode\": str(zipcode) if zipcode else None,\n",
    "                \"city\": str(city) if city else None,\n",
    "                \"state\": str(state) if state else None,\n",
    "            }\n",
    "\n",
    "            # Add the row\n",
    "            df1 = pd.concat([df1, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "            # Summary\n",
    "            print(\"Summary...\")\n",
    "            print(\"Serial Number:\", serial_num if serial_num else \"Missing\")\n",
    "            print(\"Event Type:\", event_type if event_type else \"Missing\")\n",
    "            print(\"Name:\", name if name else \"Missing\")\n",
    "            print(\"Address:\", complete_address if complete_address else \"Missing\")\n",
    "            print(\"Date:\", date if date else \"Missing\")\n",
    "            print(\"Zipcode:\", zipcode if zipcode else \"Missing\")\n",
    "            print(\"City:\", city if city else \"Missing\")\n",
    "            print(\"State:\", state if state else \"Missing\")\n",
    "            print(\"Matches found:\", matches, \"/ 7\")\n",
    "\n",
    "            print(\"File Metadata:\")\n",
    "            for key, value in metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "            # Add metadata to df1 (legacy format)\n",
    "            if serial_num:\n",
    "                df1.loc[df1[\"serial_number\"] == str(serial_num), \"filename\"] = metadata[\n",
    "                    \"filename\"\n",
    "                ]\n",
    "                df1.loc[\n",
    "                    df1[\"serial_number\"] == str(serial_num), \"file_creation_date\"\n",
    "                ] = metadata[\"file_creation_date\"]\n",
    "                df1.loc[\n",
    "                    df1[\"serial_number\"] == str(serial_num), \"file_modification_date\"\n",
    "                ] = metadata[\"file_modification_date\"]\n",
    "                df1.loc[df1[\"serial_number\"] == str(serial_num), \"file_location\"] = (\n",
    "                    metadata[\"file_location\"]\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "\n",
    "    elif filename.lower().endswith(\".bmp\"):\n",
    "        print(f\"BMP file (handled in separate cell): {filename}\")\n",
    "\n",
    "    elif filename.lower().endswith(\".pdf\"):\n",
    "        print(f\"PDF file (would need special handling): {filename}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping non-image file: {filename}\")\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "\n",
    "# Create and save CSV results to reports folder\n",
    "if csv_results:\n",
    "    results_df = pd.DataFrame(csv_results)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"NON-BMP IMAGE EXTRACTION RESULTS (CSV FORMAT)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results_df.to_csv(index=False))\n",
    "\n",
    "    # Create reports folder if it doesn't exist\n",
    "    reports_folder = os.path.join(os.getcwd(), \"reports\")\n",
    "    os.makedirs(reports_folder, exist_ok=True)\n",
    "\n",
    "    # Save to file in reports folder\n",
    "    output_file = os.path.join(reports_folder, \"non_bmp_image_extraction_results.csv\")\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nResults saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"\\nNo results to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "a7164"
   },
   "outputs": [],
   "source": [
    "# CELL 6: Process non BMP image files with OCR-focused serial extraction and CSV output\n",
    "# Add metadata columns to df1\n",
    "df1[\"filename\"] = df1[\"filename\"].astype(str)\n",
    "df1[\"file_creation_date\"] = df1[\"file_creation_date\"].astype(str)\n",
    "df1[\"file_modification_date\"] = df1[\"file_modification_date\"].astype(str)\n",
    "df1[\"file_location\"] = df1[\"file_location\"].astype(str)\n",
    "\n",
    "# Path to 'test' folder\n",
    "test_folder = os.path.join(os.getcwd(), \"test\")\n",
    "\n",
    "# Initialize EasyOCR\n",
    "reader = easyocr.Reader([\"en\"], gpu=True)\n",
    "\n",
    "# Get all files in 'test' folder\n",
    "file_list = os.listdir(test_folder)\n",
    "\n",
    "# Initialize CSV results list\n",
    "csv_results = []\n",
    "\n",
    "print(f\"Found {len(file_list)} files in test folder\")\n",
    "\n",
    "\n",
    "def extract_serial_with_improved_ocr(results, full_path):\n",
    "    \"\"\"Extract serial number focusing on OCR accuracy improvements\"\"\"\n",
    "\n",
    "    def preprocess_image_for_ocr(image_path):\n",
    "        \"\"\"Apply image preprocessing to improve OCR accuracy\"\"\"\n",
    "        import cv2\n",
    "        import numpy as np\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return None\n",
    "\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply different preprocessing techniques\n",
    "        preprocessed_images = []\n",
    "\n",
    "        # Original grayscale\n",
    "        preprocessed_images.append((\"original_gray\", gray))\n",
    "\n",
    "        # Gaussian blur to reduce noise\n",
    "        blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        preprocessed_images.append((\"blurred\", blurred))\n",
    "\n",
    "        # Sharpen\n",
    "        kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "        sharpened = cv2.filter2D(gray, -1, kernel)\n",
    "        preprocessed_images.append((\"sharpened\", sharpened))\n",
    "\n",
    "        # Adaptive threshold\n",
    "        adaptive = cv2.adaptiveThreshold(\n",
    "            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
    "        )\n",
    "        preprocessed_images.append((\"adaptive_thresh\", adaptive))\n",
    "\n",
    "        # Morphological operations to clean up\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "        morph = cv2.morphologyEx(adaptive, cv2.MORPH_CLOSE, kernel)\n",
    "        preprocessed_images.append((\"morphological\", morph))\n",
    "\n",
    "        return preprocessed_images\n",
    "\n",
    "    def run_multiple_ocr_passes(image_path):\n",
    "        \"\"\"Run OCR with different parameters and preprocessing\"\"\"\n",
    "        all_candidates = []\n",
    "\n",
    "        # Standard OCR (already done)\n",
    "        for bbox, text, confidence in results:\n",
    "            if text.strip() and confidence > 0.3:\n",
    "                all_candidates.append(\n",
    "                    {\n",
    "                        \"text\": text.strip(),\n",
    "                        \"confidence\": confidence,\n",
    "                        \"source\": \"standard_ocr\",\n",
    "                        \"bbox\": bbox,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Try different preprocessing\n",
    "        preprocessed_images = preprocess_image_for_ocr(image_path)\n",
    "        if preprocessed_images:\n",
    "            for prep_name, prep_img in preprocessed_images:\n",
    "                try:\n",
    "                    # Run OCR on preprocessed image\n",
    "                    prep_results = reader.readtext(\n",
    "                        prep_img,\n",
    "                        detail=1,\n",
    "                        paragraph=False,\n",
    "                        text_threshold=0.1,  # Lower threshold\n",
    "                        low_text=0.05,\n",
    "                        link_threshold=0.3,\n",
    "                        mag_ratio=2.0,  # Higher magnification\n",
    "                    )\n",
    "\n",
    "                    for bbox, text, confidence in prep_results:\n",
    "                        if (\n",
    "                            text.strip() and confidence > 0.2\n",
    "                        ):  # Lower threshold for preprocessed\n",
    "                            all_candidates.append(\n",
    "                                {\n",
    "                                    \"text\": text.strip(),\n",
    "                                    \"confidence\": confidence\n",
    "                                    * 0.9,  # Slight penalty for preprocessed\n",
    "                                    \"source\": f\"preprocessed_{prep_name}\",\n",
    "                                    \"bbox\": bbox,\n",
    "                                }\n",
    "                            )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {prep_name} preprocessing: {e}\")\n",
    "\n",
    "        return all_candidates\n",
    "\n",
    "    def find_potential_serials(candidates):\n",
    "        \"\"\"Find potential serial numbers from all OCR candidates\"\"\"\n",
    "        potential_serials = []\n",
    "\n",
    "        # Common words that are clearly NOT serial numbers\n",
    "        non_serial_words = {\n",
    "            \"date\",\n",
    "            \"city\",\n",
    "            \"state\",\n",
    "            \"name\",\n",
    "            \"address\",\n",
    "            \"phone\",\n",
    "            \"email\",\n",
    "            \"zip\",\n",
    "            \"code\",\n",
    "            \"serial\",\n",
    "            \"number\",\n",
    "            \"model\",\n",
    "            \"make\",\n",
    "            \"caliber\",\n",
    "            \"type\",\n",
    "            \"manufacturer\",\n",
    "            \"license\",\n",
    "            \"permit\",\n",
    "            \"registration\",\n",
    "            \"form\",\n",
    "            \"page\",\n",
    "            \"section\",\n",
    "            \"dallas\",\n",
    "            \"texas\",\n",
    "            \"california\",\n",
    "            \"florida\",\n",
    "            \"new\",\n",
    "            \"york\",\n",
    "            \"smith\",\n",
    "            \"wesson\",\n",
    "            \"colt\",\n",
    "            \"ruger\",\n",
    "            \"glock\",\n",
    "            \"sig\",\n",
    "            \"sauer\",\n",
    "            \"the\",\n",
    "            \"and\",\n",
    "            \"for\",\n",
    "            \"with\",\n",
    "            \"this\",\n",
    "            \"that\",\n",
    "            \"from\",\n",
    "            \"have\",\n",
    "            \"been\",\n",
    "            \"firearm\",\n",
    "            \"pistol\",\n",
    "            \"rifle\",\n",
    "            \"gun\",\n",
    "            \"weapon\",\n",
    "            \"barrel\",\n",
    "            \"frame\",\n",
    "            \"slide\",\n",
    "        }\n",
    "\n",
    "        for candidate in candidates:\n",
    "            text = candidate[\"text\"]\n",
    "            confidence = candidate[\"confidence\"]\n",
    "            source = candidate[\"source\"]\n",
    "\n",
    "            # Remove spaces and special characters\n",
    "            cleaned = \"\".join(c for c in text if c.isalnum())\n",
    "\n",
    "            # Skip if it's clearly not a serial number\n",
    "            if cleaned.lower() in non_serial_words:\n",
    "                continue\n",
    "\n",
    "            # Skip if it's all letters and looks like a common word (likely not a serial)\n",
    "            if cleaned.isalpha() and len(cleaned) <= 8:\n",
    "                continue\n",
    "\n",
    "            # Check if it could be a serial (basic length check)\n",
    "            if 4 <= len(cleaned) <= 15 and cleaned.isalnum():\n",
    "                # Additional check: prefer mixed alphanumeric or longer sequences\n",
    "                has_letters = any(c.isalpha() for c in cleaned)\n",
    "                has_numbers = any(c.isdigit() for c in cleaned)\n",
    "\n",
    "                # Boost confidence for mixed alphanumeric (more likely to be serials)\n",
    "                if has_letters and has_numbers:\n",
    "                    confidence *= 1.2\n",
    "                elif len(cleaned) >= 6:  # Or longer sequences\n",
    "                    confidence *= 1.1\n",
    "\n",
    "                potential_serials.append(\n",
    "                    {\n",
    "                        \"serial\": cleaned,\n",
    "                        \"original_text\": text,\n",
    "                        \"confidence\": confidence,\n",
    "                        \"source\": source,\n",
    "                        \"bbox\": candidate[\"bbox\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Also try spaced character reconstruction for this candidate\n",
    "            if \" \" in text and len(text.split()) >= 3:\n",
    "                parts = [p.strip() for p in text.split() if p.strip().isalnum()]\n",
    "                if len(parts) >= 3:\n",
    "                    reconstructed = \"\".join(parts)\n",
    "                    if (\n",
    "                        4 <= len(reconstructed) <= 15\n",
    "                        and reconstructed.isalnum()\n",
    "                        and reconstructed.lower() not in non_serial_words\n",
    "                    ):\n",
    "                        potential_serials.append(\n",
    "                            {\n",
    "                                \"serial\": reconstructed,\n",
    "                                \"original_text\": text,\n",
    "                                \"confidence\": confidence * 0.8,\n",
    "                                \"source\": f\"{source}_reconstructed\",\n",
    "                                \"bbox\": candidate[\"bbox\"],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "        return potential_serials\n",
    "\n",
    "    # Run multiple OCR passes\n",
    "    print(\"Running multiple OCR passes for improved accuracy...\")\n",
    "    all_candidates = run_multiple_ocr_passes(full_path)\n",
    "\n",
    "    # Find potential serials\n",
    "    potential_serials = find_potential_serials(all_candidates)\n",
    "\n",
    "    # Remove duplicates and sort by confidence\n",
    "    unique_serials = {}\n",
    "    for serial_info in potential_serials:\n",
    "        serial = serial_info[\"serial\"]\n",
    "        if (\n",
    "            serial not in unique_serials\n",
    "            or serial_info[\"confidence\"] > unique_serials[serial][\"confidence\"]\n",
    "        ):\n",
    "            unique_serials[serial] = serial_info\n",
    "\n",
    "    # Sort by confidence\n",
    "    sorted_serials = sorted(\n",
    "        unique_serials.values(), key=lambda x: x[\"confidence\"], reverse=True\n",
    "    )\n",
    "\n",
    "    # Display top candidates for transparency\n",
    "    print(f\"\\nSerial number candidates found:\")\n",
    "    if sorted_serials:\n",
    "        for i, serial_info in enumerate(sorted_serials[:5]):  # Show top 5\n",
    "            marker = \"→ SELECTED\" if i == 0 else \"  \"\n",
    "            print(\n",
    "                f\"  {marker} '{serial_info['serial']}' (confidence: {serial_info['confidence']:.3f}) from {serial_info['source']}\"\n",
    "            )\n",
    "            print(f\"      Original text: '{serial_info['original_text']}'\")\n",
    "        return sorted_serials[0][\"serial\"]\n",
    "    else:\n",
    "        print(\"  No potential serial numbers found\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_event_type_semantic(results):\n",
    "    \"\"\"\n",
    "    Mirror BMP-style event detection with semantic priorities.\n",
    "    Returns best event_type or None.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    event_candidates = []\n",
    "\n",
    "    # Excluded administrative words\n",
    "    excluded_words = {\n",
    "        \"atf\",\n",
    "        \"bureau\",\n",
    "        \"federal\",\n",
    "        \"department\",\n",
    "        \"justice\",\n",
    "        \"alcohol\",\n",
    "        \"tobacco\",\n",
    "        \"firearms\",\n",
    "        \"explosives\",\n",
    "        \"form\",\n",
    "        \"section\",\n",
    "        \"page\",\n",
    "        \"number\",\n",
    "        \"code\",\n",
    "        \"licensee\",\n",
    "        \"information\",\n",
    "        \"details\",\n",
    "        \"description\",\n",
    "        \"brief\",\n",
    "        \"name\",\n",
    "        \"address\",\n",
    "        \"telephone\",\n",
    "        \"date\",\n",
    "        \"time\",\n",
    "        \"signature\",\n",
    "        \"certification\",\n",
    "    }\n",
    "\n",
    "    # Priority 1: Document purpose indicators\n",
    "    purpose_patterns = [\n",
    "        (\n",
    "            r\"(Theft|Loss|Stolen|Missing|Burglary|Robbery|Larceny).*?Report\",\n",
    "            \"Theft/Loss\",\n",
    "        ),\n",
    "        (r\"Inventory\\s+(Theft|Loss)\", \"Theft/Loss\"),\n",
    "        (\n",
    "            r\"(Purchase|Sale|Transfer|Registration|Acquisition|Disposition).*?Report\",\n",
    "            None,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for pattern, fixed_event in purpose_patterns:\n",
    "        for _, text, conf in results:\n",
    "            m = re.search(pattern, text, re.IGNORECASE)\n",
    "            if m and conf > 0.4:\n",
    "                event = fixed_event if fixed_event else m.group(1).title()\n",
    "                event_candidates.append((event, conf + 1.0, \"document_purpose\", text))\n",
    "\n",
    "    # Priority 2: Specific incident/crime types\n",
    "    incident_types = {\n",
    "        \"burglary\": \"Burglary\",\n",
    "        \"robbery\": \"Robbery\",\n",
    "        \"larceny\": \"Larceny\",\n",
    "        \"theft\": \"Theft\",\n",
    "        \"stolen\": \"Theft\",\n",
    "        \"missing\": \"Missing\",\n",
    "        \"lost\": \"Loss\",\n",
    "    }\n",
    "    for _, text, conf in results:\n",
    "        text_clean = text.lower().strip()\n",
    "        if text_clean in incident_types and conf > 0.5:\n",
    "            event_candidates.append(\n",
    "                (incident_types[text_clean], conf + 0.8, \"incident_type\", text)\n",
    "            )\n",
    "\n",
    "    # Priority 3: Action descriptions\n",
    "    action_patterns = [\n",
    "        (r\"was\\s+(stolen|taken|missing|lost)\", None),\n",
    "        (r\"were\\s+(stolen|taken|missing|lost)\", None),\n",
    "        (r\"firearm\\s+was\\s+(\\w+)\", None),\n",
    "        (r\"gun\\s+was\\s+(\\w+)\", None),\n",
    "    ]\n",
    "    for pattern, _ in action_patterns:\n",
    "        for _, text, conf in results:\n",
    "            m = re.search(pattern, text, re.IGNORECASE)\n",
    "            if m and conf > 0.6:\n",
    "                action = m.group(1).lower()\n",
    "                if action in incident_types:\n",
    "                    event_candidates.append(\n",
    "                        (incident_types[action], conf + 0.6, \"action_description\", text)\n",
    "                    )\n",
    "\n",
    "    # Priority 4: Section headers\n",
    "    section_patterns = [\n",
    "        (r\"(Theft|Loss|Stolen|Missing)\\s+Information\", None),\n",
    "        (r\"(Purchase|Sale|Transfer)\\s+Information\", None),\n",
    "    ]\n",
    "    for pattern, _ in section_patterns:\n",
    "        for _, text, conf in results:\n",
    "            m = re.search(pattern, text, re.IGNORECASE)\n",
    "            if m and conf > 0.6:\n",
    "                event = m.group(1).title()\n",
    "                if event.lower() not in excluded_words:\n",
    "                    event_candidates.append((event, conf + 0.4, \"section_header\", text))\n",
    "\n",
    "    # Priority 5: Meaningful single words\n",
    "    for _, text, conf in results:\n",
    "        t = text.strip()\n",
    "        if len(t.split()) == 1 and len(t) > 3 and conf > 0.7 and t.isalpha():\n",
    "            low = t.lower()\n",
    "            if low not in excluded_words and low in incident_types:\n",
    "                event_candidates.append(\n",
    "                    (incident_types[low], conf + 0.2, \"meaningful_word\", text)\n",
    "                )\n",
    "\n",
    "    if not event_candidates:\n",
    "        return None\n",
    "\n",
    "    priority_order = {\n",
    "        \"document_purpose\": 5,\n",
    "        \"incident_type\": 4,\n",
    "        \"action_description\": 3,\n",
    "        \"section_header\": 2,\n",
    "        \"meaningful_word\": 1,\n",
    "    }\n",
    "    event_candidates.sort(\n",
    "        key=lambda x: (priority_order.get(x[2], 0), x[1]), reverse=True\n",
    "    )\n",
    "    return event_candidates[0][0]\n",
    "\n",
    "\n",
    "for filename in file_list:\n",
    "    full_path = os.path.join(test_folder, filename)\n",
    "\n",
    "    # Skip if it's a folder\n",
    "    if os.path.isdir(full_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing: {filename}\")\n",
    "\n",
    "    # Check if it's an image file (excluding BMP which is handled in another cell)\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".tiff\", \".gif\")):\n",
    "        try:\n",
    "            # Process non-BMP image formats with enhanced parameters\n",
    "            results = reader.readtext(\n",
    "                full_path,\n",
    "                detail=1,\n",
    "                paragraph=False,\n",
    "                text_threshold=0.2,\n",
    "                low_text=0.1,\n",
    "                link_threshold=0.4,\n",
    "                mag_ratio=1.5,\n",
    "            )\n",
    "\n",
    "            # Place file in a DataFrame\n",
    "            img_id = filename.split(\"/\")[-1].split(\".\")[0]\n",
    "            box_dataframe = pd.DataFrame(\n",
    "                results, columns=[\"bbox\", \"text\", \"confidence\"]\n",
    "            )\n",
    "            box_dataframe[\"img_id\"] = img_id\n",
    "\n",
    "            # Read image for visualization\n",
    "            img = cv2.imread(full_path)\n",
    "\n",
    "            # Display image with detected text if loaded successfully\n",
    "            if img is not None and isinstance(img, np.ndarray):\n",
    "                for bbox, text, confidence in results:\n",
    "                    pts = np.array(bbox, np.int32)\n",
    "                    pts = pts.reshape((-1, 1, 2))\n",
    "                    cv2.polylines(\n",
    "                        img, [pts], isClosed=True, color=(0, 0, 255), thickness=2\n",
    "                    )\n",
    "                    x, y = pts[0][0]\n",
    "                    cv2.putText(\n",
    "                        img,\n",
    "                        text,\n",
    "                        (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        1,\n",
    "                        (0, 0, 255),\n",
    "                        2,\n",
    "                    )\n",
    "\n",
    "                # Display the image\n",
    "                try:\n",
    "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    plt.figure(figsize=(12, 8))\n",
    "                    plt.imshow(img_rgb)\n",
    "                    plt.axis(\"off\")\n",
    "                    plt.title(f\"OCR Results for {filename}\")\n",
    "                    plt.show()\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not display image {filename}\")\n",
    "\n",
    "            # OCR-focused serial extraction\n",
    "            print(\"\\n--- OCR-Focused Serial Number Detection ---\")\n",
    "\n",
    "            # Use OCR-focused extraction\n",
    "            serial_num = extract_serial_with_improved_ocr(results, full_path)\n",
    "\n",
    "            if not serial_num:\n",
    "                print(\"No serial number found with OCR-focused extraction.\")\n",
    "                # Show all detected text for manual inspection\n",
    "                if results:\n",
    "                    print(\"All OCR detections:\")\n",
    "                    for i, (bbox, text, confidence) in enumerate(results):\n",
    "                        print(f\"  {i+1}. '{text}' (confidence: {confidence:.4f})\")\n",
    "\n",
    "            print(\"--- End OCR-Focused Serial Number Detection ---\")\n",
    "\n",
    "            # Extract other metadata\n",
    "            matches = 0\n",
    "\n",
    "            # Initialize variables to avoid NameError\n",
    "            name = None\n",
    "            addy = None\n",
    "            date = None\n",
    "            zipcode = None\n",
    "            city = None\n",
    "            state = None\n",
    "\n",
    "            # Name extraction\n",
    "            name_pat = r\"\\b[A-Z][a-z]+\\s[A-Z][a-z]+\\b\"\n",
    "            for bbox, text, confidence in results:\n",
    "                match = re.search(name_pat, text)\n",
    "                if match and confidence > 0.8:\n",
    "                    name = match.group()\n",
    "                    matches += 1\n",
    "                    break\n",
    "\n",
    "            # Address extraction\n",
    "            address_pat = r\"\\b\\d{1,5}\\s[A-Z][a-z]+\\s[A-Z][a-z]+\\b\"\n",
    "            for bbox, text, confidence in results:\n",
    "                match = re.search(address_pat, text)\n",
    "                if match:\n",
    "                    addy = match.group()\n",
    "                    matches += 1\n",
    "                    break\n",
    "\n",
    "            # Date extraction (support / and -)\n",
    "            date_pat = r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b\"\n",
    "            for bbox, text, confidence in results:\n",
    "                match = re.search(date_pat, text)\n",
    "                if match:\n",
    "                    date = match.group()\n",
    "                    matches += 1\n",
    "                    break\n",
    "\n",
    "            # Event type extraction (semantic - mirrored from BMP logic)\n",
    "            event_type = extract_event_type_semantic(results)\n",
    "            if event_type:\n",
    "                matches += 1\n",
    "            else:\n",
    "                event_type = None\n",
    "\n",
    "            # Zipcode extraction\n",
    "            zipcode_pat = r\"\\b\\d{5}(-\\d{4})?\\b\"\n",
    "            if results:\n",
    "                # Get image dimensions for targeted zipcode search\n",
    "                all_y_coords = [point[1] for bbox, _, _ in results for point in bbox]\n",
    "                image_height = max(all_y_coords) if all_y_coords else 0\n",
    "\n",
    "                # Target middle region for zipcodes\n",
    "                top_boundary = image_height * 0.25\n",
    "                bottom_boundary = image_height * 0.50\n",
    "\n",
    "                # Find zipcode candidates\n",
    "                candidates = []\n",
    "                for bbox, text, confidence in results:\n",
    "                    text = text.strip()\n",
    "                    if re.match(zipcode_pat, text):\n",
    "                        # Get text position\n",
    "                        top_y = min([point[1] for point in bbox])\n",
    "                        bottom_y = max([point[1] for point in bbox])\n",
    "                        text_center_y = (top_y + bottom_y) / 2\n",
    "\n",
    "                        # Check if in target region\n",
    "                        if top_boundary <= text_center_y <= bottom_boundary:\n",
    "                            candidates.append((text, confidence, text_center_y))\n",
    "\n",
    "                if candidates:\n",
    "                    # Return the candidate with highest confidence\n",
    "                    best_candidate = max(candidates, key=lambda x: x[1])\n",
    "                    zipcode = best_candidate[0]\n",
    "                    matches += 1\n",
    "\n",
    "            # Get city and state information from zipcode\n",
    "            if zipcode:\n",
    "                try:\n",
    "                    end = zipcodes.matching(str(zipcode))\n",
    "                    if end:\n",
    "                        city = end[0][\"city\"]\n",
    "                        state = end[0][\"state\"]\n",
    "                        matches += 2\n",
    "                except Exception as e:\n",
    "                    print(f\"Error looking up zipcode {zipcode}: {e}\")\n",
    "\n",
    "            # Get file metadata\n",
    "            metadata = get_file_metadata(full_path)\n",
    "\n",
    "            # Create relative path for source_file\n",
    "            relative_path = os.path.join(\"test\", filename)\n",
    "\n",
    "            # Construct complete address\n",
    "            complete_address = \"\"\n",
    "            if addy:\n",
    "                complete_address = addy\n",
    "                if city and state:\n",
    "                    complete_address += f\", {city}, {state}\"\n",
    "                    if zipcode:\n",
    "                        complete_address += f\" {zipcode}\"\n",
    "                elif zipcode:\n",
    "                    complete_address += f\" {zipcode}\"\n",
    "\n",
    "            # Create CSV record with standardized columns, using 'Missing' for empty values\n",
    "            csv_record = {\n",
    "                \"serial_number\": str(serial_num) if serial_num else \"Missing\",\n",
    "                \"event_type\": str(event_type) if event_type else \"Missing\",\n",
    "                \"event_date\": str(date) if date else \"Missing\",\n",
    "                \"associated_name\": str(name) if name else \"Missing\",\n",
    "                \"associated_address\": (\n",
    "                    str(complete_address) if complete_address else \"Missing\"\n",
    "                ),\n",
    "                \"source_file\": relative_path,\n",
    "                \"file_created\": metadata[\"file_creation_date\"],\n",
    "                \"file_modified\": metadata[\"file_modification_date\"],\n",
    "            }\n",
    "\n",
    "            csv_results.append(csv_record)\n",
    "\n",
    "            # Add extracted information to df1 (legacy format)\n",
    "            new_row = {\n",
    "                \"serial_number\": str(serial_num) if serial_num else None,\n",
    "                \"name\": str(name) if name else None,\n",
    "                \"address\": str(addy) if addy else None,\n",
    "                \"date\": str(date) if date else None,\n",
    "                \"zipcode\": str(zipcode) if zipcode else None,\n",
    "                \"city\": str(city) if city else None,\n",
    "                \"state\": str(state) if state else None,\n",
    "            }\n",
    "\n",
    "            # Add the row\n",
    "            df1 = pd.concat([df1, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "            # Summary\n",
    "            print(\"Summary...\")\n",
    "            print(\"Serial Number:\", serial_num if serial_num else \"Missing\")\n",
    "            print(\"Event Type:\", event_type if event_type else \"Missing\")\n",
    "            print(\"Name:\", name if name else \"Missing\")\n",
    "            print(\"Address:\", complete_address if complete_address else \"Missing\")\n",
    "            print(\"Date:\", date if date else \"Missing\")\n",
    "            print(\"Zipcode:\", zipcode if zipcode else \"Missing\")\n",
    "            print(\"City:\", city if city else \"Missing\")\n",
    "            print(\"State:\", state if state else \"Missing\")\n",
    "            print(\"Matches found:\", matches, \"/ 7\")\n",
    "\n",
    "            print(\"File Metadata:\")\n",
    "            for key, value in metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "            # Add metadata to df1 (legacy format)\n",
    "            if serial_num:\n",
    "                df1.loc[df1[\"serial_number\"] == str(serial_num), \"filename\"] = metadata[\n",
    "                    \"filename\"\n",
    "                ]\n",
    "                df1.loc[\n",
    "                    df1[\"serial_number\"] == str(serial_num), \"file_creation_date\"\n",
    "                ] = metadata[\"file_creation_date\"]\n",
    "                df1.loc[\n",
    "                    df1[\"serial_number\"] == str(serial_num), \"file_modification_date\"\n",
    "                ] = metadata[\"file_modification_date\"]\n",
    "                df1.loc[df1[\"serial_number\"] == str(serial_num), \"file_location\"] = (\n",
    "                    metadata[\"file_location\"]\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "\n",
    "    elif filename.lower().endswith(\".bmp\"):\n",
    "        print(f\"BMP file (handled in separate cell): {filename}\")\n",
    "\n",
    "    elif filename.lower().endswith(\".pdf\"):\n",
    "        print(f\"PDF file (would need special handling): {filename}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping non-image file: {filename}\")\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "\n",
    "# Create and save CSV results to reports folder\n",
    "if csv_results:\n",
    "    results_df = pd.DataFrame(csv_results)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"NON-BMP IMAGE EXTRACTION RESULTS (CSV FORMAT)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results_df.to_csv(index=False))\n",
    "\n",
    "    # Create reports folder if it doesn't exist\n",
    "    reports_folder = os.path.join(os.getcwd(), \"reports\")\n",
    "    os.makedirs(reports_folder, exist_ok=True)\n",
    "\n",
    "    # Save to file in reports folder\n",
    "    output_file = os.path.join(reports_folder, \"non_bmp_image_extraction_results.csv\")\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nResults saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"\\nNo results to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5559edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: Non-BMP processing with EasyOCR, Apple Silicon support (MPS-ENABLED)\n",
    "\n",
    "import os, re, cv2, numpy as np, pandas as pd, matplotlib.pyplot as plt, zipcodes, torch, easyocr\n",
    "\n",
    "# Toggle MPS here\n",
    "USE_MPS = True\n",
    "MPS_AVAILABLE = torch.backends.mps.is_available()\n",
    "os.environ.setdefault(\n",
    "    \"PYTORCH_ENABLE_MPS_FALLBACK\", \"1\"\n",
    ")  # allow silent CPU fallback for unsupported ops\n",
    "\n",
    "_mps_active = False\n",
    "_mps_disabled = False\n",
    "_mps_fail_notified = False\n",
    "\n",
    "\n",
    "def init_easyocr():\n",
    "    global _mps_active, _mps_disabled\n",
    "    if USE_MPS and MPS_AVAILABLE and not _mps_disabled:\n",
    "        print(\"Initializing EasyOCR on Apple MPS (experimental)...\")\n",
    "        rdr = easyocr.Reader([\"en\"], gpu=False)  # gpu flag is CUDA-only\n",
    "        try:\n",
    "            mps_device = torch.device(\"mps\")\n",
    "            # Move internal models where possible\n",
    "            for attr in (\"detector\", \"recognizer\", \"model\"):\n",
    "                mdl = getattr(rdr, attr, None)\n",
    "                if mdl and hasattr(mdl, \"to\"):\n",
    "                    mdl.to(mps_device)\n",
    "            _mps_active = True\n",
    "            print(\"MPS models loaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"MPS model move failed: {e}. Using CPU.\")\n",
    "            _mps_active = False\n",
    "        return rdr\n",
    "    print(\"Using CPU (MPS disabled or unavailable).\")\n",
    "    return easyocr.Reader([\"en\"], gpu=False)\n",
    "\n",
    "\n",
    "reader = init_easyocr()\n",
    "\n",
    "\n",
    "def switch_to_cpu():\n",
    "    global reader, _mps_active, _mps_disabled\n",
    "    if _mps_active:\n",
    "        print(\"Switching models to CPU...\")\n",
    "        try:\n",
    "            cpu_device = torch.device(\"cpu\")\n",
    "            for attr in (\"detector\", \"recognizer\", \"model\"):\n",
    "                mdl = getattr(reader, attr, None)\n",
    "                if mdl and hasattr(mdl, \"to\"):\n",
    "                    mdl.to(cpu_device)\n",
    "        except Exception:\n",
    "            pass\n",
    "    _mps_active = False\n",
    "    _mps_disabled = True\n",
    "\n",
    "\n",
    "def readtext_stable(image_or_path, **kwargs):\n",
    "    global _mps_fail_notified\n",
    "    try:\n",
    "        return reader.readtext(image_or_path, **kwargs)\n",
    "    except RuntimeError as e:\n",
    "        msg = str(e).lower()\n",
    "        if _mps_active and (\n",
    "            \"mps\" in msg or \"same device\" in msg or \"slow_conv2d\" in msg\n",
    "        ):\n",
    "            if not _mps_fail_notified:\n",
    "                print(\"MPS runtime issue encountered -> permanent CPU fallback.\")\n",
    "                _mps_fail_notified = True\n",
    "            switch_to_cpu()\n",
    "            # Rebuild reader on CPU for safety\n",
    "            reader_cpu = easyocr.Reader([\"en\"], gpu=False)\n",
    "            return reader_cpu.readtext(image_or_path, **kwargs)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Reuse existing helper from earlier cells if present; else define minimalist one:\n",
    "def extract_event_type_semantic(results):\n",
    "    incident_types = {\n",
    "        \"burglary\": \"Burglary\",\n",
    "        \"robbery\": \"Robbery\",\n",
    "        \"larceny\": \"Larceny\",\n",
    "        \"theft\": \"Theft\",\n",
    "        \"stolen\": \"Theft\",\n",
    "        \"missing\": \"Missing\",\n",
    "        \"lost\": \"Loss\",\n",
    "    }\n",
    "    excluded = {\n",
    "        \"atf\",\n",
    "        \"bureau\",\n",
    "        \"federal\",\n",
    "        \"department\",\n",
    "        \"justice\",\n",
    "        \"alcohol\",\n",
    "        \"tobacco\",\n",
    "        \"firearms\",\n",
    "        \"explosives\",\n",
    "        \"form\",\n",
    "        \"section\",\n",
    "        \"page\",\n",
    "        \"number\",\n",
    "        \"code\",\n",
    "        \"licensee\",\n",
    "        \"information\",\n",
    "        \"details\",\n",
    "        \"description\",\n",
    "        \"brief\",\n",
    "        \"name\",\n",
    "        \"address\",\n",
    "        \"telephone\",\n",
    "        \"date\",\n",
    "        \"time\",\n",
    "        \"signature\",\n",
    "        \"certification\",\n",
    "    }\n",
    "    purpose_patterns = [\n",
    "        (\n",
    "            r\"(Theft|Loss|Stolen|Missing|Burglary|Robbery|Larceny).*?Report\",\n",
    "            \"Theft/Loss\",\n",
    "        ),\n",
    "        (r\"Inventory\\s+(Theft|Loss)\", \"Theft/Loss\"),\n",
    "        (\n",
    "            r\"(Purchase|Sale|Transfer|Registration|Acquisition|Disposition).*?Report\",\n",
    "            None,\n",
    "        ),\n",
    "    ]\n",
    "    cands = []\n",
    "    import re\n",
    "\n",
    "    for pat, fixed in purpose_patterns:\n",
    "        for _, text, conf in results:\n",
    "            m = re.search(pat, text, re.IGNORECASE)\n",
    "            if m and conf > 0.4:\n",
    "                cands.append(\n",
    "                    (fixed if fixed else m.group(1).title(), conf + 1.0, \"purpose\")\n",
    "                )\n",
    "    for _, text, conf in results:\n",
    "        t = text.lower().strip()\n",
    "        if t in incident_types and conf > 0.5:\n",
    "            cands.append((incident_types[t], conf + 0.8, \"incident\"))\n",
    "    if not cands:\n",
    "        return None\n",
    "    order = {\"purpose\": 2, \"incident\": 1}\n",
    "    cands.sort(key=lambda x: (order.get(x[2], 0), x[1]), reverse=True)\n",
    "    return cands[0][0]\n",
    "\n",
    "\n",
    "def extract_serial(results, full_path):\n",
    "    # Lightweight multi-pass similar to earlier logic\n",
    "    import cv2, numpy as np\n",
    "\n",
    "    def preprocess(path):\n",
    "        im = cv2.imread(path)\n",
    "        if im is None:\n",
    "            return []\n",
    "        g = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "        out = [(\"gray\", g)]\n",
    "        out.append((\"blur\", cv2.GaussianBlur(g, (3, 3), 0)))\n",
    "        sharp = cv2.filter2D(g, -1, np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]]))\n",
    "        out.append((\"sharp\", sharp))\n",
    "        thr = cv2.adaptiveThreshold(\n",
    "            g, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
    "        )\n",
    "        out.append((\"thr\", thr))\n",
    "        return out\n",
    "\n",
    "    cands = []\n",
    "\n",
    "    def add(res, tag, penalty=1.0):\n",
    "        for _, text, conf in res:\n",
    "            t = text.strip()\n",
    "            if not t:\n",
    "                continue\n",
    "            cleaned = \"\".join(c for c in t if c.isalnum())\n",
    "            if 4 <= len(cleaned) <= 15 and cleaned.isalnum():\n",
    "                has_a = any(c.isalpha() for c in cleaned)\n",
    "                has_d = any(c.isdigit() for c in cleaned)\n",
    "                score = conf * penalty\n",
    "                if has_a and has_d:\n",
    "                    score *= 1.2\n",
    "                elif len(cleaned) >= 6:\n",
    "                    score *= 1.1\n",
    "                cands.append((cleaned, t, score, tag))\n",
    "\n",
    "    add(results, \"base\", 1.0)\n",
    "    for tag, imgp in preprocess(full_path):\n",
    "        try:\n",
    "            r = readtext_stable(\n",
    "                imgp,\n",
    "                detail=1,\n",
    "                paragraph=False,\n",
    "                text_threshold=0.15,\n",
    "                low_text=0.05,\n",
    "                link_threshold=0.3,\n",
    "                mag_ratio=2.0,\n",
    "            )\n",
    "            add(r, tag, 0.9)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not cands:\n",
    "        print(\"  No serial candidates.\")\n",
    "        return None\n",
    "    best = {}\n",
    "    for serial, orig, sc, src in cands:\n",
    "        if serial not in best or sc > best[serial][2]:\n",
    "            best[serial] = (serial, orig, sc, src)\n",
    "    ordered = sorted(best.values(), key=lambda x: x[2], reverse=True)\n",
    "    print(\"Serial number candidates:\")\n",
    "    for i, (s, orig, sc, src) in enumerate(ordered[:5]):\n",
    "        mark = \"→ SELECTED\" if i == 0 else \"  \"\n",
    "        print(f\"  {mark} '{s}' (score {sc:.3f}) from {src} | raw: '{orig}'\")\n",
    "    return ordered[0][0]\n",
    "\n",
    "\n",
    "# Prepare df1 metadata columns\n",
    "df1[\"filename\"] = df1[\"filename\"].astype(str)\n",
    "df1[\"file_creation_date\"] = df1[\"file_creation_date\"].astype(str)\n",
    "df1[\"file_modification_date\"] = df1[\"file_modification_date\"].astype(str)\n",
    "df1[\"file_location\"] = df1[\"file_location\"].astype(str)\n",
    "\n",
    "test_folder = os.path.join(os.getcwd(), \"test\")\n",
    "file_list = os.listdir(test_folder)\n",
    "csv_results = []\n",
    "\n",
    "print(f\"Found {len(file_list)} files in test folder\")\n",
    "\n",
    "for filename in file_list:\n",
    "    full_path = os.path.join(test_folder, filename)\n",
    "    if os.path.isdir(full_path):\n",
    "        continue\n",
    "    if filename.lower().endswith(\".bmp\"):\n",
    "        continue  # handled elsewhere\n",
    "    if not filename.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".tiff\", \".gif\")):\n",
    "        continue\n",
    "    print(f\"\\nProcessing: {filename}\")\n",
    "    try:\n",
    "        results = readtext_stable(\n",
    "            full_path,\n",
    "            detail=1,\n",
    "            paragraph=False,\n",
    "            text_threshold=0.2,\n",
    "            low_text=0.1,\n",
    "            link_threshold=0.4,\n",
    "            mag_ratio=1.5,\n",
    "        )\n",
    "        print(\"\\n--- OCR-Focused Serial Number Detection ---\")\n",
    "        serial_num = extract_serial(results, full_path)\n",
    "        print(\"--- End OCR-Focused Serial Number Detection ---\")\n",
    "\n",
    "        # Simple metadata extraction (unchanged logic)\n",
    "        name = addy = date = zipcode = city = state = None\n",
    "        name_pat = r\"\\b[A-Z][a-z]+\\s[A-Z][a-z]+\\b\"\n",
    "        address_pat = r\"\\b\\d{1,5}\\s[A-Z][a-z]+\\s[A-Z][a-z]+\\b\"\n",
    "        date_pat = r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b\"\n",
    "        zipcode_pat = r\"\\b\\d{5}(-\\d{4})?\\b\"\n",
    "\n",
    "        for _, text, conf in results:\n",
    "            if not name and conf > 0.8 and re.search(name_pat, text):\n",
    "                name = re.search(name_pat, text).group()\n",
    "            if not addy and re.search(address_pat, text):\n",
    "                addy = re.search(address_pat, text).group()\n",
    "            if not date and re.search(date_pat, text):\n",
    "                date = re.search(date_pat, text).group()\n",
    "            if name and addy and date:\n",
    "                break\n",
    "\n",
    "        event_type = extract_event_type_semantic(results)\n",
    "\n",
    "        if results:\n",
    "            try:\n",
    "                ys = [p[1] for b, _, _ in results for p in b]\n",
    "                H = max(ys) if ys else 0\n",
    "            except Exception:\n",
    "                H = 0\n",
    "            top, bottom = H * 0.25, H * 0.5\n",
    "            zcands = []\n",
    "            for b, text, conf in results:\n",
    "                t = text.strip()\n",
    "                if re.match(zipcode_pat, t):\n",
    "                    try:\n",
    "                        top_y = min(p[1] for p in b)\n",
    "                        bottom_y = max(p[1] for p in b)\n",
    "                        cy = (top_y + bottom_y) / 2\n",
    "                    except Exception:\n",
    "                        cy = 0\n",
    "                    if top <= cy <= bottom:\n",
    "                        zcands.append((t, conf))\n",
    "            if zcands:\n",
    "                zipcode = max(zcands, key=lambda x: x[1])[0]\n",
    "                try:\n",
    "                    loc = zipcodes.matching(zipcode)\n",
    "                    if loc:\n",
    "                        city = loc[0][\"city\"]\n",
    "                        state = loc[0][\"state\"]\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        complete_address = \"\"\n",
    "        if addy:\n",
    "            complete_address = addy\n",
    "            if city and state:\n",
    "                complete_address += f\", {city}, {state}\"\n",
    "                if zipcode:\n",
    "                    complete_address += f\" {zipcode}\"\n",
    "            elif zipcode:\n",
    "                complete_address += f\" {zipcode}\"\n",
    "\n",
    "        metadata = get_file_metadata(full_path)\n",
    "        relative_path = os.path.join(\"test\", filename)\n",
    "\n",
    "        csv_results.append(\n",
    "            {\n",
    "                \"serial_number\": serial_num or \"Missing\",\n",
    "                \"event_type\": event_type or \"Missing\",\n",
    "                \"event_date\": date or \"Missing\",\n",
    "                \"associated_name\": name or \"Missing\",\n",
    "                \"associated_address\": complete_address or \"Missing\",\n",
    "                \"source_file\": relative_path,\n",
    "                \"file_created\": metadata[\"file_creation_date\"],\n",
    "                \"file_modified\": metadata[\"file_modification_date\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Legacy df1 append\n",
    "        df1 = pd.concat(\n",
    "            [\n",
    "                df1,\n",
    "                pd.DataFrame(\n",
    "                    [\n",
    "                        {\n",
    "                            \"serial_number\": serial_num,\n",
    "                            \"name\": name,\n",
    "                            \"address\": addy,\n",
    "                            \"date\": date,\n",
    "                            \"zipcode\": zipcode,\n",
    "                            \"city\": city,\n",
    "                            \"state\": state,\n",
    "                        }\n",
    "                    ]\n",
    "                ),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "        if serial_num:\n",
    "            sel = df1[\"serial_number\"] == serial_num\n",
    "            df1.loc[sel, \"filename\"] = metadata[\"filename\"]\n",
    "            df1.loc[sel, \"file_creation_date\"] = metadata[\"file_creation_date\"]\n",
    "            df1.loc[sel, \"file_modification_date\"] = metadata[\"file_modification_date\"]\n",
    "            df1.loc[sel, \"file_location\"] = metadata[\"file_location\"]\n",
    "\n",
    "        print(\"Summary:\")\n",
    "        print(\"  Serial :\", serial_num or \"Missing\")\n",
    "        print(\"  Event  :\", event_type or \"Missing\")\n",
    "        print(\"  Name   :\", name or \"Missing\")\n",
    "        print(\"  Address:\", complete_address or \"Missing\")\n",
    "        print(\"  Date   :\", date or \"Missing\")\n",
    "        print(\"  Zip    :\", zipcode or \"Missing\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\nProcessing complete!\")\n",
    "\n",
    "if csv_results:\n",
    "    reports_folder = os.path.join(os.getcwd(), \"reports\")\n",
    "    os.makedirs(reports_folder, exist_ok=True)\n",
    "    out = os.path.join(reports_folder, \"non_bmp_image_extraction_results.csv\")\n",
    "    pd.DataFrame(csv_results).to_csv(out, index=False)\n",
    "    print(f\"Results saved to: {out}\")\n",
    "else:\n",
    "    print(\"No results to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe1cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8 (CLEAN & CORRECTED): Non-BMP processing with EasyOCR + Apple Silicon (MPS) fallback\n",
    "\n",
    "import os, re, cv2, numpy as np, pandas as pd, zipcodes, torch, easyocr\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Assumes:\n",
    "# - df1 DataFrame already exists (earlier cells)\n",
    "# - get_file_metadata(full_path) already defined (Cell 2)\n",
    "\n",
    "# ---------------- Configuration ----------------\n",
    "USE_MPS = True  # Try to use Apple Silicon MPS\n",
    "VERBOSE_DEVICES = True  # Extra diagnostics\n",
    "FORCE_READER_REINIT = True  # Always rebuild reader in this cell (isolated)\n",
    "ALLOWED_EXT = (\".jpg\", \".jpeg\", \".png\", \".tiff\", \".gif\")\n",
    "\n",
    "# ---------------- MPS Reader Initialization ----------------\n",
    "_mps_available = torch.backends.mps.is_available()\n",
    "_target_device = (\n",
    "    torch.device(\"mps\") if (USE_MPS and _mps_available) else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "if USE_MPS and not _mps_available:\n",
    "    print(\"MPS not available -> using CPU.\")\n",
    "\n",
    "_reader = None\n",
    "_mps_active = False\n",
    "_fallback_used = False\n",
    "\n",
    "\n",
    "def _summarize_param_devices(rdr: easyocr.Reader) -> set:\n",
    "    devs = set()\n",
    "    for attr in (\"detector\", \"recognizer\"):\n",
    "        mdl = getattr(rdr, attr, None)\n",
    "        if mdl:\n",
    "            for p in mdl.parameters():\n",
    "                devs.add(p.device.type)\n",
    "    return devs\n",
    "\n",
    "\n",
    "def _move_model_to(device: torch.device, rdr: easyocr.Reader):\n",
    "    # EasyOCR: detector, recognizer (recognizer.model nested)\n",
    "    for attr in (\"detector\", \"recognizer\"):\n",
    "        mdl = getattr(rdr, attr, None)\n",
    "        if mdl:\n",
    "            try:\n",
    "                mdl.to(device)\n",
    "            except Exception as e:\n",
    "                if VERBOSE_DEVICES:\n",
    "                    print(f\"  Warn: could not move {attr}: {e}\")\n",
    "            sub = getattr(mdl, \"model\", None)\n",
    "            if sub:\n",
    "                try:\n",
    "                    sub.to(device)\n",
    "                except Exception as e:\n",
    "                    if VERBOSE_DEVICES:\n",
    "                        print(f\"  Warn: could not move nested model in {attr}: {e}\")\n",
    "\n",
    "\n",
    "def _init_reader() -> easyocr.Reader:\n",
    "    global _mps_active\n",
    "    # EasyOCR's gpu=True only targets CUDA, so keep gpu=False and move manually.\n",
    "    rdr = easyocr.Reader([\"en\"], gpu=False)\n",
    "    if _target_device.type == \"mps\":\n",
    "        try:\n",
    "            rdr.device = _target_device\n",
    "            _move_model_to(_target_device, rdr)\n",
    "            devs = _summarize_param_devices(rdr)\n",
    "            print(f\"EasyOCR initialized (requested MPS). Parameter devices: {devs}\")\n",
    "            if \"mps\" in devs:\n",
    "                _mps_active = True\n",
    "                print(\"MPS active.\")\n",
    "            else:\n",
    "                print(\"Parameters not on MPS -> CPU execution.\")\n",
    "        except Exception as e:\n",
    "            print(f\"MPS move failed: {e} -> CPU only.\")\n",
    "            _mps_active = False\n",
    "    else:\n",
    "        print(\"EasyOCR initialized on CPU.\")\n",
    "    return rdr\n",
    "\n",
    "\n",
    "def get_reader():\n",
    "    global _reader\n",
    "    if _reader is None or FORCE_READER_REINIT:\n",
    "        if _reader is not None and FORCE_READER_REINIT:\n",
    "            print(\"Reinitializing EasyOCR reader...\")\n",
    "        _reader = _init_reader()\n",
    "    return _reader\n",
    "\n",
    "\n",
    "reader = get_reader()\n",
    "\n",
    "\n",
    "# ---------------- Robust readtext wrapper ----------------\n",
    "def readtext_stable(image_or_ndarray, **kwargs):\n",
    "    \"\"\"\n",
    "    Run OCR, fallback to CPU once if MPS raises runtime issues (device mismatch / slow_conv2d).\n",
    "    \"\"\"\n",
    "    global reader, _fallback_used, _mps_active\n",
    "    try:\n",
    "        return reader.readtext(image_or_ndarray, **kwargs)\n",
    "    except RuntimeError as e:\n",
    "        msg = str(e).lower()\n",
    "        trigger = any(\n",
    "            k in msg\n",
    "            for k in (\"mps\", \"slow_conv2d\", \"same device\", \"metalperformanceshaders\")\n",
    "        )\n",
    "        if _mps_active and trigger and not _fallback_used:\n",
    "            print(\"MPS runtime issue -> rebuilding on CPU for remainder of session.\")\n",
    "            _fallback_used = True\n",
    "            _mps_active = False\n",
    "            reader = easyocr.Reader([\"en\"], gpu=False)\n",
    "            return reader.readtext(image_or_ndarray, **kwargs)\n",
    "        raise\n",
    "\n",
    "\n",
    "# ---------------- Event Type Extraction ----------------\n",
    "def extract_event_type_semantic(results: List[Tuple]) -> str | None:\n",
    "    incident_types = {\n",
    "        \"burglary\": \"Burglary\",\n",
    "        \"robbery\": \"Robbery\",\n",
    "        \"larceny\": \"Larceny\",\n",
    "        \"theft\": \"Theft\",\n",
    "        \"stolen\": \"Theft\",\n",
    "        \"missing\": \"Missing\",\n",
    "        \"lost\": \"Loss\",\n",
    "    }\n",
    "    excluded = {\n",
    "        \"atf\",\n",
    "        \"bureau\",\n",
    "        \"federal\",\n",
    "        \"department\",\n",
    "        \"justice\",\n",
    "        \"alcohol\",\n",
    "        \"tobacco\",\n",
    "        \"firearms\",\n",
    "        \"explosives\",\n",
    "        \"form\",\n",
    "        \"section\",\n",
    "        \"page\",\n",
    "        \"number\",\n",
    "        \"code\",\n",
    "        \"licensee\",\n",
    "        \"information\",\n",
    "        \"details\",\n",
    "        \"description\",\n",
    "        \"brief\",\n",
    "        \"name\",\n",
    "        \"address\",\n",
    "        \"telephone\",\n",
    "        \"date\",\n",
    "        \"time\",\n",
    "        \"signature\",\n",
    "        \"certification\",\n",
    "    }\n",
    "    purpose_patterns = [\n",
    "        (\n",
    "            r\"(Theft|Loss|Stolen|Missing|Burglary|Robbery|Larceny).*?Report\",\n",
    "            \"Theft/Loss\",\n",
    "        ),\n",
    "        (r\"Inventory\\s+(Theft|Loss)\", \"Theft/Loss\"),\n",
    "        (\n",
    "            r\"(Purchase|Sale|Transfer|Registration|Acquisition|Disposition).*?Report\",\n",
    "            None,\n",
    "        ),\n",
    "    ]\n",
    "    import re\n",
    "\n",
    "    cands = []\n",
    "    for pat, fixed in purpose_patterns:\n",
    "        for _, text, conf in results:\n",
    "            m = re.search(pat, text, re.IGNORECASE)\n",
    "            if m and conf > 0.4:\n",
    "                cands.append(\n",
    "                    (fixed if fixed else m.group(1).title(), conf + 1.0, \"purpose\")\n",
    "                )\n",
    "    for _, text, conf in results:\n",
    "        t = text.lower().strip()\n",
    "        if t in incident_types and conf > 0.5:\n",
    "            cands.append((incident_types[t], conf + 0.8, \"incident\"))\n",
    "    if not cands:\n",
    "        return None\n",
    "    order = {\"purpose\": 2, \"incident\": 1}\n",
    "    cands.sort(key=lambda x: (order.get(x[2], 0), x[1]), reverse=True)\n",
    "    return cands[0][0]\n",
    "\n",
    "\n",
    "# ---------------- Serial Extraction (multi-pass) ----------------\n",
    "def extract_serial(results, full_path: str):\n",
    "    import cv2, numpy as np\n",
    "\n",
    "    def preprocess(path):\n",
    "        im = cv2.imread(path)\n",
    "        if im is None:\n",
    "            return []\n",
    "        g = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "        out = [(\"gray\", g)]\n",
    "        out.append((\"blur\", cv2.GaussianBlur(g, (3, 3), 0)))\n",
    "        sharp = cv2.filter2D(g, -1, np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]]))\n",
    "        out.append((\"sharp\", sharp))\n",
    "        thr = cv2.adaptiveThreshold(\n",
    "            g, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
    "        )\n",
    "        out.append((\"thr\", thr))\n",
    "        return out\n",
    "\n",
    "    cands = []\n",
    "\n",
    "    def add(res, tag, penalty=1.0):\n",
    "        for _, text, conf in res:\n",
    "            t = text.strip()\n",
    "            if not t:\n",
    "                continue\n",
    "            cleaned = \"\".join(c for c in t if c.isalnum())\n",
    "            if 4 <= len(cleaned) <= 15:\n",
    "                has_a = any(c.isalpha() for c in cleaned)\n",
    "                has_d = any(c.isdigit() for c in cleaned)\n",
    "                score = conf * penalty\n",
    "                if has_a and has_d:\n",
    "                    score *= 1.2\n",
    "                elif len(cleaned) >= 6:\n",
    "                    score *= 1.1\n",
    "                cands.append((cleaned, t, score, tag))\n",
    "\n",
    "    add(results, \"base\", 1.0)\n",
    "    for tag, imgp in preprocess(full_path):\n",
    "        try:\n",
    "            r = readtext_stable(\n",
    "                imgp,\n",
    "                detail=1,\n",
    "                paragraph=False,\n",
    "                text_threshold=0.15,\n",
    "                low_text=0.05,\n",
    "                link_threshold=0.3,\n",
    "                mag_ratio=2.0,\n",
    "            )\n",
    "            add(r, tag, 0.9)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not cands:\n",
    "        print(\"  No serial candidates.\")\n",
    "        return None\n",
    "    # Deduplicate keep best score\n",
    "    best = {}\n",
    "    for serial, orig, sc, src in cands:\n",
    "        if serial not in best or sc > best[serial][2]:\n",
    "            best[serial] = (serial, orig, sc, src)\n",
    "    ordered = sorted(best.values(), key=lambda x: x[2], reverse=True)\n",
    "    print(\"Serial number candidates:\")\n",
    "    for i, (s, orig, sc, src) in enumerate(ordered[:5]):\n",
    "        mark = \"→ SELECTED\" if i == 0 else \"  \"\n",
    "        print(f\"  {mark} '{s}' (score {sc:.3f}) from {src} | raw: '{orig}'\")\n",
    "    return ordered[0][0]\n",
    "\n",
    "\n",
    "# ---------------- Ensure df1 has required metadata columns ----------------\n",
    "for col in (\n",
    "    \"filename\",\n",
    "    \"file_creation_date\",\n",
    "    \"file_modification_date\",\n",
    "    \"file_location\",\n",
    "):\n",
    "    if col not in df1.columns:\n",
    "        df1[col] = \"\"\n",
    "    df1[col] = df1[col].astype(str)\n",
    "\n",
    "# ---------------- Collect files ----------------\n",
    "test_folder = os.path.join(os.getcwd(), \"test\")\n",
    "if not os.path.isdir(test_folder):\n",
    "    raise RuntimeError(f\"Test folder not found: {test_folder}\")\n",
    "\n",
    "file_list = os.listdir(test_folder)\n",
    "csv_results = []\n",
    "print(f\"Found {len(file_list)} files in test folder\")\n",
    "\n",
    "# ---------------- Processing Loop ----------------\n",
    "for filename in file_list:\n",
    "    full_path = os.path.join(test_folder, filename)\n",
    "    if os.path.isdir(full_path):\n",
    "        continue\n",
    "    if filename.lower().endswith(\".bmp\"):\n",
    "        continue  # handled by dedicated BMP cell\n",
    "    if not filename.lower().endswith(ALLOWED_EXT):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing: {filename}\")\n",
    "    try:\n",
    "        results = readtext_stable(\n",
    "            full_path,\n",
    "            detail=1,\n",
    "            paragraph=False,\n",
    "            text_threshold=0.2,\n",
    "            low_text=0.1,\n",
    "            link_threshold=0.4,\n",
    "            mag_ratio=1.5,\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- OCR-Focused Serial Number Detection ---\")\n",
    "        serial_num = extract_serial(results, full_path)\n",
    "        print(\"--- End OCR-Focused Serial Number Detection ---\")\n",
    "\n",
    "        name = addy = date = zipcode = city = state = None\n",
    "        name_pat = r\"\\b[A-Z][a-z]+\\s[A-Z][a-z]+\\b\"\n",
    "        address_pat = r\"\\b\\d{1,5}\\s[A-Z][a-z]+\\s[A-Z][a-z]+\\b\"\n",
    "        date_pat = r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b\"\n",
    "        zipcode_pat = r\"\\b\\d{5}(-\\d{4})?\\b\"\n",
    "\n",
    "        for _, text, conf in results:\n",
    "            if not name and conf > 0.8:\n",
    "                m = re.search(name_pat, text)\n",
    "                if m:\n",
    "                    name = m.group()\n",
    "            if not addy:\n",
    "                m = re.search(address_pat, text)\n",
    "                if m:\n",
    "                    addy = m.group()\n",
    "            if not date:\n",
    "                m = re.search(date_pat, text)\n",
    "                if m:\n",
    "                    date = m.group()\n",
    "            if name and addy and date:\n",
    "                break\n",
    "\n",
    "        event_type = extract_event_type_semantic(results)\n",
    "\n",
    "        # Zipcode with positional filter (middle region)\n",
    "        if results:\n",
    "            try:\n",
    "                all_y = [pt[1] for b, _, _ in results for pt in b]\n",
    "                H = max(all_y) if all_y else 0\n",
    "            except Exception:\n",
    "                H = 0\n",
    "            top, bottom = H * 0.25, H * 0.50\n",
    "            zcands = []\n",
    "            for b, text, conf in results:\n",
    "                t = text.strip()\n",
    "                if re.match(zipcode_pat, t):\n",
    "                    try:\n",
    "                        top_y = min(p[1] for p in b)\n",
    "                        bot_y = max(p[1] for p in b)\n",
    "                        cy = (top_y + bot_y) / 2\n",
    "                    except Exception:\n",
    "                        cy = 0\n",
    "                    if top <= cy <= bottom:\n",
    "                        zcands.append((t, conf))\n",
    "            if zcands:\n",
    "                zipcode = max(zcands, key=lambda x: x[1])[0]\n",
    "                try:\n",
    "                    loc = zipcodes.matching(zipcode)\n",
    "                    if loc:\n",
    "                        city = loc[0][\"city\"]\n",
    "                        state = loc[0][\"state\"]\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        complete_address = \"\"\n",
    "        if addy:\n",
    "            complete_address = addy\n",
    "            if city and state:\n",
    "                complete_address += f\", {city}, {state}\"\n",
    "                if zipcode:\n",
    "                    complete_address += f\" {zipcode}\"\n",
    "            elif zipcode:\n",
    "                complete_address += f\" {zipcode}\"\n",
    "\n",
    "        metadata = get_file_metadata(full_path)\n",
    "        relative_path = os.path.join(\"test\", filename)\n",
    "\n",
    "        csv_results.append(\n",
    "            {\n",
    "                \"serial_number\": serial_num or \"Missing\",\n",
    "                \"event_type\": event_type or \"Missing\",\n",
    "                \"event_date\": date or \"Missing\",\n",
    "                \"associated_name\": name or \"Missing\",\n",
    "                \"associated_address\": complete_address or \"Missing\",\n",
    "                \"source_file\": relative_path,\n",
    "                \"file_created\": metadata[\"file_creation_date\"],\n",
    "                \"file_modified\": metadata[\"file_modification_date\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Legacy-style append to df1\n",
    "        legacy_row = {\n",
    "            \"serial_number\": serial_num,\n",
    "            \"name\": name,\n",
    "            \"address\": addy,\n",
    "            \"date\": date,\n",
    "            \"zipcode\": zipcode,\n",
    "            \"city\": city,\n",
    "            \"state\": state,\n",
    "        }\n",
    "        df1 = pd.concat([df1, pd.DataFrame([legacy_row])], ignore_index=True)\n",
    "\n",
    "        if serial_num:\n",
    "            sel = df1[\"serial_number\"] == serial_num\n",
    "            df1.loc[sel, \"filename\"] = metadata[\"filename\"]\n",
    "            df1.loc[sel, \"file_creation_date\"] = metadata[\"file_creation_date\"]\n",
    "            df1.loc[sel, \"file_modification_date\"] = metadata[\"file_modification_date\"]\n",
    "            df1.loc[sel, \"file_location\"] = metadata[\"file_location\"]\n",
    "\n",
    "        print(\"Summary:\")\n",
    "        print(\"  Serial :\", serial_num or \"Missing\")\n",
    "        print(\"  Event  :\", event_type or \"Missing\")\n",
    "        print(\"  Name   :\", name or \"Missing\")\n",
    "        print(\"  Address:\", complete_address or \"Missing\")\n",
    "        print(\"  Date   :\", date or \"Missing\")\n",
    "        print(\"  Zip    :\", zipcode or \"Missing\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\nProcessing complete!\")\n",
    "\n",
    "# ---------------- Write CSV Output ----------------\n",
    "if csv_results:\n",
    "    reports_folder = os.path.join(os.getcwd(), \"reports\")\n",
    "    os.makedirs(reports_folder, exist_ok=True)\n",
    "    out_csv = os.path.join(reports_folder, \"non_bmp_image_extraction_results.csv\")\n",
    "    pd.DataFrame(csv_results).to_csv(out_csv, index=False)\n",
    "    print(f\"Results saved to: {out_csv}\")\n",
    "else:\n",
    "    print(\"No results to save.\")\n",
    "\n",
    "# ---------------- Diagnostics ----------------\n",
    "if VERBOSE_DEVICES:\n",
    "    try:\n",
    "        devs = _summarize_param_devices(reader)\n",
    "        print(\n",
    "            \"Final reader param devices:\",\n",
    "            devs,\n",
    "            \"| MPS active:\",\n",
    "            _mps_active,\n",
    "            \"| Fallback used:\",\n",
    "            _fallback_used,\n",
    "            \"| reader.device:\",\n",
    "            getattr(reader, \"device\", \"N/A\"),\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab3c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: Process non BMP image files with OCR-focused serial extraction and CSV output\n",
    "# Apple Silicon GPU (MPS) first, with automatic CPU fallback\n",
    "\n",
    "# Safe imports (ok to re-import in a notebook cell)\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import zipcodes\n",
    "import torch  # for MPS (Apple Silicon GPU)\n",
    "import easyocr\n",
    "\n",
    "# Add metadata columns to df1\n",
    "df1[\"filename\"] = df1[\"filename\"].astype(str)\n",
    "df1[\"file_creation_date\"] = df1[\"file_creation_date\"].astype(str)\n",
    "df1[\"file_modification_date\"] = df1[\"file_modification_date\"].astype(str)\n",
    "df1[\"file_location\"] = df1[\"file_location\"].astype(str)\n",
    "\n",
    "# Path to 'test' folder\n",
    "test_folder = os.path.join(os.getcwd(), \"test\")\n",
    "\n",
    "# Decide device and set default BEFORE constructing EasyOCR reader\n",
    "mps_available = torch.backends.mps.is_available()\n",
    "device = \"mps\" if mps_available else \"cpu\"\n",
    "try:\n",
    "    if mps_available:\n",
    "        torch.set_default_device(\"mps\")\n",
    "        print(\"Using Apple Silicon GPU (MPS) by default.\")\n",
    "    else:\n",
    "        print(\"MPS not available; using CPU.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not set default device to MPS: {e}\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "\n",
    "def make_reader(on_device: str):\n",
    "    \"\"\"\n",
    "    Construct an EasyOCR reader targeting the given device.\n",
    "    Note: EasyOCR's 'gpu=True' assumes CUDA, so we keep gpu=False, and rely on\n",
    "    PyTorch default device (set above) to place tensors/models on MPS.\n",
    "    \"\"\"\n",
    "    # Create reader with gpu=False (CUDA-only flag). Tensors will follow default device.\n",
    "    rdr = easyocr.Reader([\"en\"], gpu=False)\n",
    "    return rdr\n",
    "\n",
    "\n",
    "# Create initial reader on desired device\n",
    "reader = make_reader(device)\n",
    "\n",
    "\n",
    "# Helper: robust OCR with device fallback\n",
    "def readtext_with_fallback(reader_obj, image_or_path, **kwargs):\n",
    "    \"\"\"\n",
    "    Try OCR with current device. On MPS-specific device mismatch errors,\n",
    "    fallback to CPU by rebuilding the reader and retrying once.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return reader_obj.readtext(image_or_path, **kwargs)\n",
    "    except RuntimeError as e:\n",
    "        msg = str(e)\n",
    "        # Known MPS error: input on CPU, model on MPS or similar mismatch\n",
    "        if (\n",
    "            \"slow_conv2d_forward_mps\" in msg\n",
    "            or \"must be on the same device\" in msg\n",
    "            or \"mps\" in msg.lower()\n",
    "        ):\n",
    "            print(\"MPS error detected. Falling back to CPU and retrying once...\")\n",
    "            try:\n",
    "                # Switch default device back to CPU\n",
    "                torch.set_default_device(\"cpu\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            rdr_cpu = make_reader(\"cpu\")\n",
    "            return rdr_cpu.readtext(image_or_path, **kwargs)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "# Get all files in 'test' folder (we'll still skip BMPs in the loop)\n",
    "file_list = os.listdir(test_folder)\n",
    "\n",
    "# Initialize CSV results list\n",
    "csv_results = []\n",
    "\n",
    "print(f\"Found {len(file_list)} files in test folder\")\n",
    "\n",
    "\n",
    "def extract_serial_with_improved_ocr(results, full_path):\n",
    "    \"\"\"Extract serial number focusing on OCR accuracy improvements\"\"\"\n",
    "\n",
    "    def preprocess_image_for_ocr(image_path):\n",
    "        \"\"\"Apply image preprocessing to improve OCR accuracy\"\"\"\n",
    "        import cv2\n",
    "        import numpy as np\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return None\n",
    "\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply different preprocessing techniques\n",
    "        preprocessed_images = []\n",
    "\n",
    "        # Original grayscale\n",
    "        preprocessed_images.append((\"original_gray\", gray))\n",
    "\n",
    "        # Gaussian blur to reduce noise\n",
    "        blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        preprocessed_images.append((\"blurred\", blurred))\n",
    "\n",
    "        # Sharpen\n",
    "        kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "        sharpened = cv2.filter2D(gray, -1, kernel)\n",
    "        preprocessed_images.append((\"sharpened\", sharpened))\n",
    "\n",
    "        # Adaptive threshold\n",
    "        adaptive = cv2.adaptiveThreshold(\n",
    "            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
    "        )\n",
    "        preprocessed_images.append((\"adaptive_thresh\", adaptive))\n",
    "\n",
    "        # Morphological operations to clean up\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "        morph = cv2.morphologyEx(adaptive, cv2.MORPH_CLOSE, kernel)\n",
    "        preprocessed_images.append((\"morphological\", morph))\n",
    "\n",
    "        return preprocessed_images\n",
    "\n",
    "    def run_multiple_ocr_passes(image_path):\n",
    "        \"\"\"Run OCR with different parameters and preprocessing\"\"\"\n",
    "        all_candidates = []\n",
    "\n",
    "        # Standard OCR (already done)\n",
    "        for bbox, text, confidence in results:\n",
    "            if text.strip() and confidence > 0.3:\n",
    "                all_candidates.append(\n",
    "                    {\n",
    "                        \"text\": text.strip(),\n",
    "                        \"confidence\": confidence,\n",
    "                        \"source\": \"standard_ocr\",\n",
    "                        \"bbox\": bbox,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Try different preprocessing\n",
    "        preprocessed_images = preprocess_image_for_ocr(image_path)\n",
    "        if preprocessed_images:\n",
    "            for prep_name, prep_img in preprocessed_images:\n",
    "                try:\n",
    "                    # Run OCR on preprocessed image (with device fallback)\n",
    "                    prep_results = readtext_with_fallback(\n",
    "                        reader,\n",
    "                        prep_img,\n",
    "                        detail=1,\n",
    "                        paragraph=False,\n",
    "                        text_threshold=0.1,  # Lower threshold\n",
    "                        low_text=0.05,\n",
    "                        link_threshold=0.3,\n",
    "                        mag_ratio=2.0,  # Higher magnification\n",
    "                    )\n",
    "\n",
    "                    for bbox, text, confidence in prep_results:\n",
    "                        if (\n",
    "                            text.strip() and confidence > 0.2\n",
    "                        ):  # Lower threshold for preprocessed\n",
    "                            all_candidates.append(\n",
    "                                {\n",
    "                                    \"text\": text.strip(),\n",
    "                                    \"confidence\": confidence\n",
    "                                    * 0.9,  # Slight penalty for preprocessed\n",
    "                                    \"source\": f\"preprocessed_{prep_name}\",\n",
    "                                    \"bbox\": bbox,\n",
    "                                }\n",
    "                            )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {prep_name} preprocessing: {e}\")\n",
    "\n",
    "        return all_candidates\n",
    "\n",
    "    def find_potential_serials(candidates):\n",
    "        \"\"\"Find potential serial numbers from all OCR candidates\"\"\"\n",
    "        potential_serials = []\n",
    "\n",
    "        # Common words that are clearly NOT serial numbers\n",
    "        non_serial_words = {\n",
    "            \"date\",\n",
    "            \"city\",\n",
    "            \"state\",\n",
    "            \"name\",\n",
    "            \"address\",\n",
    "            \"phone\",\n",
    "            \"email\",\n",
    "            \"zip\",\n",
    "            \"code\",\n",
    "            \"serial\",\n",
    "            \"number\",\n",
    "            \"model\",\n",
    "            \"make\",\n",
    "            \"caliber\",\n",
    "            \"type\",\n",
    "            \"manufacturer\",\n",
    "            \"license\",\n",
    "            \"permit\",\n",
    "            \"registration\",\n",
    "            \"form\",\n",
    "            \"page\",\n",
    "            \"section\",\n",
    "            \"dallas\",\n",
    "            \"texas\",\n",
    "            \"california\",\n",
    "            \"florida\",\n",
    "            \"new\",\n",
    "            \"york\",\n",
    "            \"smith\",\n",
    "            \"wesson\",\n",
    "            \"colt\",\n",
    "            \"ruger\",\n",
    "            \"glock\",\n",
    "            \"sig\",\n",
    "            \"sauer\",\n",
    "            \"the\",\n",
    "            \"and\",\n",
    "            \"for\",\n",
    "            \"with\",\n",
    "            \"this\",\n",
    "            \"that\",\n",
    "            \"from\",\n",
    "            \"have\",\n",
    "            \"been\",\n",
    "            \"firearm\",\n",
    "            \"pistol\",\n",
    "            \"rifle\",\n",
    "            \"gun\",\n",
    "            \"weapon\",\n",
    "            \"barrel\",\n",
    "            \"frame\",\n",
    "            \"slide\",\n",
    "        }\n",
    "\n",
    "        for candidate in candidates:\n",
    "            text = candidate[\"text\"]\n",
    "            confidence = candidate[\"confidence\"]\n",
    "            source = candidate[\"source\"]\n",
    "\n",
    "            # Remove spaces and special characters\n",
    "            cleaned = \"\".join(c for c in text if c.isalnum())\n",
    "\n",
    "            # Skip if it's clearly not a serial number\n",
    "            if cleaned.lower() in non_serial_words:\n",
    "                continue\n",
    "\n",
    "            # Skip if it's all letters and looks like a common word (likely not a serial)\n",
    "            if cleaned.isalpha() and len(cleaned) <= 8:\n",
    "                continue\n",
    "\n",
    "            # Check if it could be a serial (basic length check)\n",
    "            if 4 <= len(cleaned) <= 15 and cleaned.isalnum():\n",
    "                # Additional check: prefer mixed alphanumeric or longer sequences\n",
    "                has_letters = any(c.isalpha() for c in cleaned)\n",
    "                has_numbers = any(c.isdigit() for c in cleaned)\n",
    "\n",
    "                # Boost confidence for mixed alphanumeric (more likely to be serials)\n",
    "                if has_letters and has_numbers:\n",
    "                    confidence *= 1.2\n",
    "                elif len(cleaned) >= 6:  # Or longer sequences\n",
    "                    confidence *= 1.1\n",
    "\n",
    "                potential_serials.append(\n",
    "                    {\n",
    "                        \"serial\": cleaned,\n",
    "                        \"original_text\": text,\n",
    "                        \"confidence\": confidence,\n",
    "                        \"source\": source,\n",
    "                        \"bbox\": candidate[\"bbox\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Also try spaced character reconstruction for this candidate\n",
    "            if \" \" in text and len(text.split()) >= 3:\n",
    "                parts = [p.strip() for p in text.split() if p.strip().isalnum()]\n",
    "                if len(parts) >= 3:\n",
    "                    reconstructed = \"\".join(parts)\n",
    "                    if (\n",
    "                        4 <= len(reconstructed) <= 15\n",
    "                        and reconstructed.isalnum()\n",
    "                        and reconstructed.lower() not in non_serial_words\n",
    "                    ):\n",
    "                        potential_serials.append(\n",
    "                            {\n",
    "                                \"serial\": reconstructed,\n",
    "                                \"original_text\": text,\n",
    "                                \"confidence\": confidence * 0.8,\n",
    "                                \"source\": f\"{source}_reconstructed\",\n",
    "                                \"bbox\": candidate[\"bbox\"],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "        return potential_serials\n",
    "\n",
    "    # Run multiple OCR passes\n",
    "    print(\"Running multiple OCR passes for improved accuracy...\")\n",
    "    all_candidates = run_multiple_ocr_passes(full_path)\n",
    "\n",
    "    # Find potential serials\n",
    "    potential_serials = find_potential_serials(all_candidates)\n",
    "\n",
    "    # Remove duplicates and sort by confidence\n",
    "    unique_serials = {}\n",
    "    for serial_info in potential_serials:\n",
    "        serial = serial_info[\"serial\"]\n",
    "        if (\n",
    "            serial not in unique_serials\n",
    "            or serial_info[\"confidence\"] > unique_serials[serial][\"confidence\"]\n",
    "        ):\n",
    "            unique_serials[serial] = serial_info\n",
    "\n",
    "    # Sort by confidence\n",
    "    sorted_serials = sorted(\n",
    "        unique_serials.values(), key=lambda x: x[\"confidence\"], reverse=True\n",
    "    )\n",
    "\n",
    "    # Display top candidates for transparency\n",
    "    print(f\"\\nSerial number candidates found:\")\n",
    "    if sorted_serials:\n",
    "        for i, serial_info in enumerate(sorted_serials[:5]):  # Show top 5\n",
    "            marker = \"→ SELECTED\" if i == 0 else \"  \"\n",
    "            print(\n",
    "                f\"  {marker} '{serial_info['serial']}' (confidence: {serial_info['confidence']:.3f}) from {serial_info['source']}\"\n",
    "            )\n",
    "            print(f\"      Original text: '{serial_info['original_text']}'\")\n",
    "        return sorted_serials[0][\"serial\"]\n",
    "    else:\n",
    "        print(\"  No potential serial numbers found\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_event_type_semantic(results):\n",
    "    \"\"\"\n",
    "    Mirror BMP-style event detection with semantic priorities.\n",
    "    Returns best event_type or None.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    event_candidates = []\n",
    "\n",
    "    # Excluded administrative words\n",
    "    excluded_words = {\n",
    "        \"atf\",\n",
    "        \"bureau\",\n",
    "        \"federal\",\n",
    "        \"department\",\n",
    "        \"justice\",\n",
    "        \"alcohol\",\n",
    "        \"tobacco\",\n",
    "        \"firearms\",\n",
    "        \"explosives\",\n",
    "        \"form\",\n",
    "        \"section\",\n",
    "        \"page\",\n",
    "        \"number\",\n",
    "        \"code\",\n",
    "        \"licensee\",\n",
    "        \"information\",\n",
    "        \"details\",\n",
    "        \"description\",\n",
    "        \"brief\",\n",
    "        \"name\",\n",
    "        \"address\",\n",
    "        \"telephone\",\n",
    "        \"date\",\n",
    "        \"time\",\n",
    "        \"signature\",\n",
    "        \"certification\",\n",
    "    }\n",
    "\n",
    "    # Priority 1: Document purpose indicators\n",
    "    purpose_patterns = [\n",
    "        (\n",
    "            r\"(Theft|Loss|Stolen|Missing|Burglary|Robbery|Larceny).*?Report\",\n",
    "            \"Theft/Loss\",\n",
    "        ),\n",
    "        (r\"Inventory\\s+(Theft|Loss)\", \"Theft/Loss\"),\n",
    "        (\n",
    "            r\"(Purchase|Sale|Transfer|Registration|Acquisition|Disposition).*?Report\",\n",
    "            None,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for pattern, fixed_event in purpose_patterns:\n",
    "        for _, text, conf in results:\n",
    "            m = re.search(pattern, text, re.IGNORECASE)\n",
    "            if m and conf > 0.4:\n",
    "                event = fixed_event if fixed_event else m.group(1).title()\n",
    "                event_candidates.append((event, conf + 1.0, \"document_purpose\", text))\n",
    "\n",
    "    # Priority 2: Specific incident/crime types\n",
    "    incident_types = {\n",
    "        \"burglary\": \"Burglary\",\n",
    "        \"robbery\": \"Robbery\",\n",
    "        \"larceny\": \"Larceny\",\n",
    "        \"theft\": \"Theft\",\n",
    "        \"stolen\": \"Theft\",\n",
    "        \"missing\": \"Missing\",\n",
    "        \"lost\": \"Loss\",\n",
    "    }\n",
    "    for _, text, conf in results:\n",
    "        text_clean = text.lower().strip()\n",
    "        if text_clean in incident_types and conf > 0.5:\n",
    "            event_candidates.append(\n",
    "                (incident_types[text_clean], conf + 0.8, \"incident_type\", text)\n",
    "            )\n",
    "\n",
    "    # Priority 3: Action descriptions\n",
    "    action_patterns = [\n",
    "        (r\"was\\s+(stolen|taken|missing|lost)\", None),\n",
    "        (r\"were\\s+(stolen|taken|missing|lost)\", None),\n",
    "        (r\"firearm\\s+was\\s+(\\w+)\", None),\n",
    "        (r\"gun\\s+was\\s+(\\w+)\", None),\n",
    "    ]\n",
    "    for pattern, _ in action_patterns:\n",
    "        for _, text, conf in results:\n",
    "            m = re.search(pattern, text, re.IGNORECASE)\n",
    "            if m and conf > 0.6:\n",
    "                action = m.group(1).lower()\n",
    "                if action in incident_types:\n",
    "                    event_candidates.append(\n",
    "                        (incident_types[action], conf + 0.6, \"action_description\", text)\n",
    "                    )\n",
    "\n",
    "    # Priority 4: Section headers\n",
    "    section_patterns = [\n",
    "        (r\"(Theft|Loss|Stolen|Missing)\\s+Information\", None),\n",
    "        (r\"(Purchase|Sale|Transfer)\\s+Information\", None),\n",
    "    ]\n",
    "    for pattern, _ in section_patterns:\n",
    "        for _, text, conf in results:\n",
    "            m = re.search(pattern, text, re.IGNORECASE)\n",
    "            if m and conf > 0.6:\n",
    "                event = m.group(1).title()\n",
    "                if event.lower() not in excluded_words:\n",
    "                    event_candidates.append((event, conf + 0.4, \"section_header\", text))\n",
    "\n",
    "    # Priority 5: Meaningful single words\n",
    "    for _, text, conf in results:\n",
    "        t = text.strip()\n",
    "        if len(t.split()) == 1 and len(t) > 3 and conf > 0.7 and t.isalpha():\n",
    "            low = t.lower()\n",
    "            if low not in excluded_words and low in incident_types:\n",
    "                event_candidates.append(\n",
    "                    (incident_types[low], conf + 0.2, \"meaningful_word\", text)\n",
    "                )\n",
    "\n",
    "    if not event_candidates:\n",
    "        return None\n",
    "\n",
    "    priority_order = {\n",
    "        \"document_purpose\": 5,\n",
    "        \"incident_type\": 4,\n",
    "        \"action_description\": 3,\n",
    "        \"section_header\": 2,\n",
    "        \"meaningful_word\": 1,\n",
    "    }\n",
    "    event_candidates.sort(\n",
    "        key=lambda x: (priority_order.get(x[2], 0), x[1]), reverse=True\n",
    "    )\n",
    "    return event_candidates[0][0]\n",
    "\n",
    "\n",
    "for filename in file_list:\n",
    "    full_path = os.path.join(test_folder, filename)\n",
    "\n",
    "    # Skip if it's a folder\n",
    "    if os.path.isdir(full_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing: {filename}\")\n",
    "\n",
    "    # Check if it's an image file (excluding BMP which is handled in another cell)\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".tiff\", \".gif\")):\n",
    "        try:\n",
    "            # OCR with device fallback (handles MPS -> CPU automatically on error)\n",
    "            results = readtext_with_fallback(\n",
    "                reader,\n",
    "                full_path,\n",
    "                detail=1,\n",
    "                paragraph=False,\n",
    "                text_threshold=0.2,\n",
    "                low_text=0.1,\n",
    "                link_threshold=0.4,\n",
    "                mag_ratio=1.5,\n",
    "            )\n",
    "\n",
    "            # Place file in a DataFrame (kept for consistency/debugging)\n",
    "            img_id = filename.split(\"/\")[-1].split(\".\")[0]\n",
    "            box_dataframe = pd.DataFrame(\n",
    "                results, columns=[\"bbox\", \"text\", \"confidence\"]\n",
    "            )\n",
    "            box_dataframe[\"img_id\"] = img_id\n",
    "\n",
    "            # Read image for visualization\n",
    "            img = cv2.imread(full_path)\n",
    "\n",
    "            # Display image with detected text if loaded successfully\n",
    "            if img is not None and isinstance(img, np.ndarray):\n",
    "                for bbox, text, confidence in results:\n",
    "                    pts = np.array(bbox, np.int32)\n",
    "                    pts = pts.reshape((-1, 1, 2))\n",
    "                    cv2.polylines(\n",
    "                        img, [pts], isClosed=True, color=(0, 0, 255), thickness=2\n",
    "                    )\n",
    "                    x, y = pts[0][0]\n",
    "                    cv2.putText(\n",
    "                        img,\n",
    "                        text,\n",
    "                        (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        1,\n",
    "                        (0, 0, 255),\n",
    "                        2,\n",
    "                    )\n",
    "\n",
    "                # Display the image\n",
    "                try:\n",
    "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    plt.figure(figsize=(12, 8))\n",
    "                    plt.imshow(img_rgb)\n",
    "                    plt.axis(\"off\")\n",
    "                    plt.title(f\"OCR Results for {filename}\")\n",
    "                    plt.show()\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not display image {filename}\")\n",
    "\n",
    "            # OCR-focused serial extraction\n",
    "            print(\"\\n--- OCR-Focused Serial Number Detection ---\")\n",
    "\n",
    "            # Use OCR-focused extraction\n",
    "            serial_num = extract_serial_with_improved_ocr(results, full_path)\n",
    "\n",
    "            if not serial_num:\n",
    "                print(\"No serial number found with OCR-focused extraction.\")\n",
    "                # Show all detected text for manual inspection\n",
    "                if results:\n",
    "                    print(\"All OCR detections:\")\n",
    "                    for i, (bbox, text, confidence) in enumerate(results):\n",
    "                        print(f\"  {i+1}. '{text}' (confidence: {confidence:.4f})\")\n",
    "\n",
    "            print(\"--- End OCR-Focused Serial Number Detection ---\")\n",
    "\n",
    "            # Extract other metadata\n",
    "            matches = 0\n",
    "\n",
    "            # Initialize variables to avoid NameError\n",
    "            name = None\n",
    "            addy = None\n",
    "            date = None\n",
    "            zipcode = None\n",
    "            city = None\n",
    "            state = None\n",
    "\n",
    "            # Name extraction\n",
    "            name_pat = r\"\\b[A-Z][a-z]+\\s[A-Z][a-z]+\\b\"\n",
    "            for bbox, text, confidence in results:\n",
    "                match = re.search(name_pat, text)\n",
    "                if match and confidence > 0.8:\n",
    "                    name = match.group()\n",
    "                    matches += 1\n",
    "                    break\n",
    "\n",
    "            # Address extraction\n",
    "            address_pat = r\"\\b\\d{1,5}\\s[A-Z][a-z]+\\s[A-Z][a-z]+\\b\"\n",
    "            for bbox, text, confidence in results:\n",
    "                match = re.search(address_pat, text)\n",
    "                if match:\n",
    "                    addy = match.group()\n",
    "                    matches += 1\n",
    "                    break\n",
    "\n",
    "            # Date extraction (support / and -)\n",
    "            date_pat = r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b\"\n",
    "            for bbox, text, confidence in results:\n",
    "                match = re.search(date_pat, text)\n",
    "                if match:\n",
    "                    date = match.group()\n",
    "                    matches += 1\n",
    "                    break\n",
    "\n",
    "            # Event type extraction (semantic - mirrored from BMP logic)\n",
    "            event_type = extract_event_type_semantic(results)\n",
    "            if event_type:\n",
    "                matches += 1\n",
    "            else:\n",
    "                event_type = None\n",
    "\n",
    "            # Zipcode extraction\n",
    "            zipcode_pat = r\"\\b\\d{5}(-\\d{4})?\\b\"\n",
    "            if results:\n",
    "                # Get image dimensions for targeted zipcode search\n",
    "                all_y_coords = [point[1] for bbox, _, _ in results for point in bbox]\n",
    "                image_height = max(all_y_coords) if all_y_coords else 0\n",
    "\n",
    "                # Target middle region for zipcodes\n",
    "                top_boundary = image_height * 0.25\n",
    "                bottom_boundary = image_height * 0.50\n",
    "\n",
    "                # Find zipcode candidates\n",
    "                candidates = []\n",
    "                for bbox, text, confidence in results:\n",
    "                    text = text.strip()\n",
    "                    if re.match(zipcode_pat, text):\n",
    "                        # Get text position\n",
    "                        top_y = min([point[1] for point in bbox])\n",
    "                        bottom_y = max([point[1] for point in bbox])\n",
    "                        text_center_y = (top_y + bottom_y) / 2\n",
    "\n",
    "                        # Check if in target region\n",
    "                        if top_boundary <= text_center_y <= bottom_boundary:\n",
    "                            candidates.append((text, confidence, text_center_y))\n",
    "\n",
    "                if candidates:\n",
    "                    # Return the candidate with highest confidence\n",
    "                    best_candidate = max(candidates, key=lambda x: x[1])\n",
    "                    zipcode = best_candidate[0]\n",
    "                    matches += 1\n",
    "\n",
    "            # Get city and state information from zipcode\n",
    "            if zipcode:\n",
    "                try:\n",
    "                    end = zipcodes.matching(str(zipcode))\n",
    "                    if end:\n",
    "                        city = end[0][\"city\"]\n",
    "                        state = end[0][\"state\"]\n",
    "                        matches += 2\n",
    "                except Exception as e:\n",
    "                    print(f\"Error looking up zipcode {zipcode}: {e}\")\n",
    "\n",
    "            # Get file metadata\n",
    "            metadata = get_file_metadata(full_path)\n",
    "\n",
    "            # Create relative path for source_file\n",
    "            relative_path = os.path.join(\"test\", filename)\n",
    "\n",
    "            # Construct complete address\n",
    "            complete_address = \"\"\n",
    "            if addy:\n",
    "                complete_address = addy\n",
    "                if city and state:\n",
    "                    complete_address += f\", {city}, {state}\"\n",
    "                    if zipcode:\n",
    "                        complete_address += f\" {zipcode}\"\n",
    "                elif zipcode:\n",
    "                    complete_address += f\" {zipcode}\"\n",
    "\n",
    "            # Create CSV record with standardized columns, using 'Missing' for empty values\n",
    "            csv_record = {\n",
    "                \"serial_number\": str(serial_num) if serial_num else \"Missing\",\n",
    "                \"event_type\": str(event_type) if event_type else \"Missing\",\n",
    "                \"event_date\": str(date) if date else \"Missing\",\n",
    "                \"associated_name\": str(name) if name else \"Missing\",\n",
    "                \"associated_address\": (\n",
    "                    str(complete_address) if complete_address else \"Missing\"\n",
    "                ),\n",
    "                \"source_file\": relative_path,\n",
    "                \"file_created\": metadata[\"file_creation_date\"],\n",
    "                \"file_modified\": metadata[\"file_modification_date\"],\n",
    "            }\n",
    "\n",
    "            csv_results.append(csv_record)\n",
    "\n",
    "            # Add extracted information to df1 (legacy format)\n",
    "            new_row = {\n",
    "                \"serial_number\": str(serial_num) if serial_num else None,\n",
    "                \"name\": str(name) if name else None,\n",
    "                \"address\": str(addy) if addy else None,\n",
    "                \"date\": str(date) if date else None,\n",
    "                \"zipcode\": str(zipcode) if zipcode else None,\n",
    "                \"city\": str(city) if city else None,\n",
    "                \"state\": str(state) if state else None,\n",
    "            }\n",
    "\n",
    "            # Add the row\n",
    "            df1 = pd.concat([df1, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "            # Summary\n",
    "            print(\"Summary...\")\n",
    "            print(\"Serial Number:\", serial_num if serial_num else \"Missing\")\n",
    "            print(\"Event Type:\", event_type if event_type else \"Missing\")\n",
    "            print(\"Name:\", name if name else \"Missing\")\n",
    "            print(\"Address:\", complete_address if complete_address else \"Missing\")\n",
    "            print(\"Date:\", date if date else \"Missing\")\n",
    "            print(\"Zipcode:\", zipcode if zipcode else \"Missing\")\n",
    "            print(\"City:\", city if city else \"Missing\")\n",
    "            print(\"State:\", state if state else \"Missing\")\n",
    "            print(\"Matches found:\", matches, \"/ 7\")\n",
    "\n",
    "            print(\"File Metadata:\")\n",
    "            for key, value in metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "            # Add metadata to df1 (legacy format)\n",
    "            if serial_num:\n",
    "                df1.loc[df1[\"serial_number\"] == str(serial_num), \"filename\"] = metadata[\n",
    "                    \"filename\"\n",
    "                ]\n",
    "                df1.loc[\n",
    "                    df1[\"serial_number\"] == str(serial_num), \"file_creation_date\"\n",
    "                ] = metadata[\"file_creation_date\"]\n",
    "                df1.loc[\n",
    "                    df1[\"serial_number\"] == str(serial_num), \"file_modification_date\"\n",
    "                ] = metadata[\"file_modification_date\"]\n",
    "                df1.loc[df1[\"serial_number\"] == str(serial_num), \"file_location\"] = (\n",
    "                    metadata[\"file_location\"]\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "\n",
    "    elif filename.lower().endswith(\".bmp\"):\n",
    "        print(f\"BMP file (handled in separate cell): {filename}\")\n",
    "\n",
    "    elif filename.lower().endswith(\".pdf\"):\n",
    "        print(f\"PDF file (would need special handling): {filename}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping non-image file: {filename}\")\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "\n",
    "# Create and save CSV results to reports folder\n",
    "if csv_results:\n",
    "    results_df = pd.DataFrame(csv_results)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"NON-BMP IMAGE EXTRACTION RESULTS (CSV FORMAT)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results_df.to_csv(index=False))\n",
    "\n",
    "    # Create reports folder if it doesn't exist\n",
    "    reports_folder = os.path.join(os.getcwd(), \"reports\")\n",
    "    os.makedirs(reports_folder, exist_ok=True)\n",
    "\n",
    "    # Save to file in reports folder\n",
    "    output_file = os.path.join(reports_folder, \"non_bmp_image_extraction_results.csv\")\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nResults saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"\\nNo results to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd7fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: Process non-BMP image files with PaddleOCR (CPU-only) and CSV output\n",
    "\n",
    "# Safe imports (ok to re-import in a notebook cell)\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import zipcodes\n",
    "\n",
    "# PaddleOCR (CPU-only)\n",
    "import paddle\n",
    "from paddleocr import PaddleOCR\n",
    "\n",
    "# Add metadata columns to df1\n",
    "df1[\"filename\"] = df1[\"filename\"].astype(str)\n",
    "df1[\"file_creation_date\"] = df1[\"file_creation_date\"].astype(str)\n",
    "df1[\"file_modification_date\"] = df1[\"file_modification_date\"].astype(str)\n",
    "df1[\"file_location\"] = df1[\"file_location\"].astype(str)\n",
    "\n",
    "# Path to 'test' folder\n",
    "test_folder = os.path.join(os.getcwd(), \"test\")\n",
    "\n",
    "# Force CPU (current macOS wheels do not support 'mps')\n",
    "try:\n",
    "    paddle.set_device(\"cpu\")\n",
    "    print(\"Paddle set to CPU (MPS not available in this build).\")\n",
    "except Exception as e:\n",
    "    print(f\"Falling back to CPU due to error: {e}\")\n",
    "    paddle.set_device(\"cpu\")\n",
    "\n",
    "# Build PaddleOCR reader (avoid downscaling: large det_max_side_len)\n",
    "ocr_reader = PaddleOCR(\n",
    "    use_angle_cls=True,\n",
    "    lang=\"en\",\n",
    "    det_max_side_len=4096,  # prevent unwanted downscaling on high-res images\n",
    "    show_log=False,\n",
    ")\n",
    "\n",
    "# Get all files in 'test' folder (we'll still skip BMPs in the loop)\n",
    "file_list = os.listdir(test_folder)\n",
    "\n",
    "# Initialize CSV results list\n",
    "csv_results = []\n",
    "\n",
    "print(f\"Found {len(file_list)} files in test folder\")\n",
    "\n",
    "\n",
    "def normalize_paddle_results(paddle_result):\n",
    "    \"\"\"\n",
    "    Convert PaddleOCR result to a flat list of (bbox, text, confidence) like EasyOCR.\n",
    "    Paddle format (for one image) is typically:\n",
    "      [ [ [points, (text, conf)], [points, (text, conf)], ... ] ]\n",
    "    or already a single list depending on version.\n",
    "    \"\"\"\n",
    "    flat = []\n",
    "    if not paddle_result:\n",
    "        return flat\n",
    "\n",
    "    # Handle both nested [list] and already-flat list cases\n",
    "    if (\n",
    "        isinstance(paddle_result, list)\n",
    "        and len(paddle_result) > 0\n",
    "        and isinstance(paddle_result[0], list)\n",
    "        and len(paddle_result[0]) > 0\n",
    "        and isinstance(paddle_result[0][0], (list, tuple))\n",
    "        and len(paddle_result[0][0]) == 2\n",
    "    ):\n",
    "        candidates = paddle_result[0]\n",
    "    else:\n",
    "        candidates = paddle_result\n",
    "\n",
    "    for item in candidates:\n",
    "        # item is [points, (text, conf)]\n",
    "        try:\n",
    "            points, tc = item\n",
    "            text, conf = tc\n",
    "            # Normalize bbox as list of 4 points [[x1,y1],...]\n",
    "            bbox = points\n",
    "            flat.append((bbox, str(text), float(conf)))\n",
    "        except Exception:\n",
    "            # Be resilient if structure varies\n",
    "            continue\n",
    "    return flat\n",
    "\n",
    "\n",
    "def paddle_readtext_cpu(image_or_path, **kwargs):\n",
    "    \"\"\"\n",
    "    CPU-only OCR call wrapper (kept as a function for consistency).\n",
    "    \"\"\"\n",
    "    return ocr_reader.ocr(image_or_path, **kwargs)\n",
    "\n",
    "\n",
    "def extract_serial_with_improved_ocr(paddle_like_results, full_path):\n",
    "    \"\"\"Extract serial number focusing on OCR accuracy improvements\"\"\"\n",
    "\n",
    "    def preprocess_image_for_ocr(image_path):\n",
    "        \"\"\"Apply image preprocessing to improve OCR accuracy\"\"\"\n",
    "        import cv2\n",
    "        import numpy as np\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return None\n",
    "\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply different preprocessing techniques\n",
    "        preprocessed_images = []\n",
    "\n",
    "        # Original grayscale\n",
    "        preprocessed_images.append((\"original_gray\", gray))\n",
    "\n",
    "        # Gaussian blur to reduce noise\n",
    "        blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        preprocessed_images.append((\"blurred\", blurred))\n",
    "\n",
    "        # Sharpen\n",
    "        kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "        sharpened = cv2.filter2D(gray, -1, kernel)\n",
    "        preprocessed_images.append((\"sharpened\", sharpened))\n",
    "\n",
    "        # Adaptive threshold\n",
    "        adaptive = cv2.adaptiveThreshold(\n",
    "            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
    "        )\n",
    "        preprocessed_images.append((\"adaptive_thresh\", adaptive))\n",
    "\n",
    "        # Morphological operations to clean up\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "        morph = cv2.morphologyEx(adaptive, cv2.MORPH_CLOSE, kernel)\n",
    "        preprocessed_images.append((\"morphological\", morph))\n",
    "\n",
    "        return preprocessed_images\n",
    "\n",
    "    def run_multiple_ocr_passes(image_path):\n",
    "        \"\"\"Run OCR with different parameters and preprocessing using PaddleOCR\"\"\"\n",
    "        all_candidates = []\n",
    "\n",
    "        # Standard OCR (on original)\n",
    "        base_raw = paddle_readtext_cpu(image_path, cls=True)\n",
    "        base = normalize_paddle_results(base_raw)\n",
    "        for bbox, text, confidence in base:\n",
    "            if text.strip() and confidence > 0.3:\n",
    "                all_candidates.append(\n",
    "                    {\n",
    "                        \"text\": text.strip(),\n",
    "                        \"confidence\": confidence,\n",
    "                        \"source\": \"standard_ocr\",\n",
    "                        \"bbox\": bbox,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Try different preprocessing\n",
    "        preprocessed_images = preprocess_image_for_ocr(image_path)\n",
    "        if preprocessed_images:\n",
    "            for prep_name, prep_img in preprocessed_images:\n",
    "                try:\n",
    "                    # PaddleOCR accepts np.ndarray as input\n",
    "                    prep_raw = paddle_readtext_cpu(prep_img, cls=True)\n",
    "                    prep_results = normalize_paddle_results(prep_raw)\n",
    "\n",
    "                    for bbox, text, confidence in prep_results:\n",
    "                        if (\n",
    "                            text.strip() and confidence > 0.2\n",
    "                        ):  # Lower threshold for preprocessed\n",
    "                            all_candidates.append(\n",
    "                                {\n",
    "                                    \"text\": text.strip(),\n",
    "                                    \"confidence\": confidence\n",
    "                                    * 0.9,  # Slight penalty for preprocessed\n",
    "                                    \"source\": f\"preprocessed_{prep_name}\",\n",
    "                                    \"bbox\": bbox,\n",
    "                                }\n",
    "                            )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {prep_name} preprocessing: {e}\")\n",
    "\n",
    "        return all_candidates\n",
    "\n",
    "    def find_potential_serials(candidates):\n",
    "        \"\"\"Find potential serial numbers from all OCR candidates\"\"\"\n",
    "        potential_serials = []\n",
    "\n",
    "        # Common words that are clearly NOT serial numbers\n",
    "        non_serial_words = {\n",
    "            \"date\",\n",
    "            \"city\",\n",
    "            \"state\",\n",
    "            \"name\",\n",
    "            \"address\",\n",
    "            \"phone\",\n",
    "            \"email\",\n",
    "            \"zip\",\n",
    "            \"code\",\n",
    "            \"serial\",\n",
    "            \"number\",\n",
    "            \"model\",\n",
    "            \"make\",\n",
    "            \"caliber\",\n",
    "            \"type\",\n",
    "            \"manufacturer\",\n",
    "            \"license\",\n",
    "            \"permit\",\n",
    "            \"registration\",\n",
    "            \"form\",\n",
    "            \"page\",\n",
    "            \"section\",\n",
    "            \"dallas\",\n",
    "            \"texas\",\n",
    "            \"california\",\n",
    "            \"florida\",\n",
    "            \"new\",\n",
    "            \"york\",\n",
    "            \"smith\",\n",
    "            \"wesson\",\n",
    "            \"colt\",\n",
    "            \"ruger\",\n",
    "            \"glock\",\n",
    "            \"sig\",\n",
    "            \"sauer\",\n",
    "            \"the\",\n",
    "            \"and\",\n",
    "            \"for\",\n",
    "            \"with\",\n",
    "            \"this\",\n",
    "            \"that\",\n",
    "            \"from\",\n",
    "            \"have\",\n",
    "            \"been\",\n",
    "            \"firearm\",\n",
    "            \"pistol\",\n",
    "            \"rifle\",\n",
    "            \"gun\",\n",
    "            \"weapon\",\n",
    "            \"barrel\",\n",
    "            \"frame\",\n",
    "            \"slide\",\n",
    "        }\n",
    "\n",
    "        for candidate in candidates:\n",
    "            text = candidate[\"text\"]\n",
    "            confidence = candidate[\"confidence\"]\n",
    "            source = candidate[\"source\"]\n",
    "\n",
    "            # Remove spaces and special characters\n",
    "            cleaned = \"\".join(c for c in text if c.isalnum())\n",
    "\n",
    "            # Skip if it's clearly not a serial number\n",
    "            if cleaned.lower() in non_serial_words:\n",
    "                continue\n",
    "\n",
    "            # Skip if it's all letters and looks like a common word (likely not a serial)\n",
    "            if cleaned.isalpha() and len(cleaned) <= 8:\n",
    "                continue\n",
    "\n",
    "            # Check if it could be a serial (basic length check)\n",
    "            if 4 <= len(cleaned) <= 15 and cleaned.isalnum():\n",
    "                # Additional check: prefer mixed alphanumeric or longer sequences\n",
    "                has_letters = any(c.isalpha() for c in cleaned)\n",
    "                has_numbers = any(c.isdigit() for c in cleaned)\n",
    "\n",
    "                # Boost confidence for mixed alphanumeric (more likely to be serials)\n",
    "                if has_letters and has_numbers:\n",
    "                    confidence *= 1.2\n",
    "                elif len(cleaned) >= 6:  # Or longer sequences\n",
    "                    confidence *= 1.1\n",
    "\n",
    "                potential_serials.append(\n",
    "                    {\n",
    "                        \"serial\": cleaned,\n",
    "                        \"original_text\": text,\n",
    "                        \"confidence\": confidence,\n",
    "                        \"source\": source,\n",
    "                        \"bbox\": candidate[\"bbox\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Also try spaced character reconstruction for this candidate\n",
    "            if \" \" in text and len(text.split()) >= 3:\n",
    "                parts = [p.strip() for p in text.split() if p.strip().isalnum()]\n",
    "                if len(parts) >= 3:\n",
    "                    reconstructed = \"\".join(parts)\n",
    "                    if (\n",
    "                        4 <= len(reconstructed) <= 15\n",
    "                        and reconstructed.isalnum()\n",
    "                        and reconstructed.lower() not in non_serial_words\n",
    "                    ):\n",
    "                        potential_serials.append(\n",
    "                            {\n",
    "                                \"serial\": reconstructed,\n",
    "                                \"original_text\": text,\n",
    "                                \"confidence\": confidence * 0.8,\n",
    "                                \"source\": f\"{source}_reconstructed\",\n",
    "                                \"bbox\": candidate[\"bbox\"],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "        return potential_serials\n",
    "\n",
    "    # Run multiple OCR passes\n",
    "    print(\"Running multiple OCR passes for improved accuracy...\")\n",
    "    all_candidates = run_multiple_ocr_passes(full_path)\n",
    "\n",
    "    # Find potential serials\n",
    "    potential_serials = find_potential_serials(all_candidates)\n",
    "\n",
    "    # Remove duplicates and sort by confidence\n",
    "    unique_serials = {}\n",
    "    for serial_info in potential_serials:\n",
    "        serial = serial_info[\"serial\"]\n",
    "        if (\n",
    "            serial not in unique_serials\n",
    "            or serial_info[\"confidence\"] > unique_serials[serial][\"confidence\"]\n",
    "        ):\n",
    "            unique_serials[serial] = serial_info\n",
    "\n",
    "    # Sort by confidence\n",
    "    sorted_serials = sorted(\n",
    "        unique_serials.values(), key=lambda x: x[\"confidence\"], reverse=True\n",
    "    )\n",
    "\n",
    "    # Display top candidates for transparency\n",
    "    print(f\"\\nSerial number candidates found:\")\n",
    "    if sorted_serials:\n",
    "        for i, serial_info in enumerate(sorted_serials[:5]):  # Show top 5\n",
    "            marker = \"→ SELECTED\" if i == 0 else \"  \"\n",
    "            print(\n",
    "                f\"  {marker} '{serial_info['serial']}' (confidence: {serial_info['confidence']:.3f}) from {serial_info['source']}\"\n",
    "            )\n",
    "            print(f\"      Original text: '{serial_info['original_text']}'\")\n",
    "        return sorted_serials[0][\"serial\"]\n",
    "    else:\n",
    "        print(\"  No potential serial numbers found\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_event_type_semantic(results):\n",
    "    \"\"\"\n",
    "    Mirror BMP-style event detection with semantic priorities.\n",
    "    Returns best event_type or None.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    event_candidates = []\n",
    "\n",
    "    # Excluded administrative words\n",
    "    excluded_words = {\n",
    "        \"atf\",\n",
    "        \"bureau\",\n",
    "        \"federal\",\n",
    "        \"department\",\n",
    "        \"justice\",\n",
    "        \"alcohol\",\n",
    "        \"tobacco\",\n",
    "        \"firearms\",\n",
    "        \"explosives\",\n",
    "        \"form\",\n",
    "        \"section\",\n",
    "        \"page\",\n",
    "        \"number\",\n",
    "        \"code\",\n",
    "        \"licensee\",\n",
    "        \"information\",\n",
    "        \"details\",\n",
    "        \"description\",\n",
    "        \"brief\",\n",
    "        \"name\",\n",
    "        \"address\",\n",
    "        \"telephone\",\n",
    "        \"date\",\n",
    "        \"time\",\n",
    "        \"signature\",\n",
    "        \"certification\",\n",
    "    }\n",
    "\n",
    "    # Priority 1: Document purpose indicators\n",
    "    purpose_patterns = [\n",
    "        (\n",
    "            r\"(Theft|Loss|Stolen|Missing|Burglary|Robbery|Larceny).*?Report\",\n",
    "            \"Theft/Loss\",\n",
    "        ),\n",
    "        (r\"Inventory\\s+(Theft|Loss)\", \"Theft/Loss\"),\n",
    "        (\n",
    "            r\"(Purchase|Sale|Transfer|Registration|Acquisition|Disposition).*?Report\",\n",
    "            None,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for pattern, fixed_event in purpose_patterns:\n",
    "        for _, text, conf in results:\n",
    "            m = re.search(pattern, text, re.IGNORECASE)\n",
    "            if m and conf > 0.4:\n",
    "                event = fixed_event if fixed_event else m.group(1).title()\n",
    "                event_candidates.append((event, conf + 1.0, \"document_purpose\", text))\n",
    "\n",
    "    # Priority 2: Specific incident/crime types\n",
    "    incident_types = {\n",
    "        \"burglary\": \"Burglary\",\n",
    "        \"robbery\": \"Robbery\",\n",
    "        \"larceny\": \"Larceny\",\n",
    "        \"theft\": \"Theft\",\n",
    "        \"stolen\": \"Theft\",\n",
    "        \"missing\": \"Missing\",\n",
    "        \"lost\": \"Loss\",\n",
    "    }\n",
    "    for _, text, conf in results:\n",
    "        text_clean = text.lower().strip()\n",
    "        if text_clean in incident_types and conf > 0.5:\n",
    "            event_candidates.append(\n",
    "                (incident_types[text_clean], conf + 0.8, \"incident_type\", text)\n",
    "            )\n",
    "\n",
    "    # Priority 3: Action descriptions\n",
    "    action_patterns = [\n",
    "        (r\"was\\s+(stolen|taken|missing|lost)\", None),\n",
    "        (r\"were\\s+(stolen|taken|missing|lost)\", None),\n",
    "        (r\"firearm\\s+was\\s+(\\w+)\", None),\n",
    "        (r\"gun\\s+was\\s+(\\w+)\", None),\n",
    "    ]\n",
    "    for pattern, _ in action_patterns:\n",
    "        for _, text, conf in results:\n",
    "            m = re.search(pattern, text, re.IGNORECASE)\n",
    "            if m and conf > 0.6:\n",
    "                action = m.group(1).lower()\n",
    "                if action in incident_types:\n",
    "                    event_candidates.append(\n",
    "                        (incident_types[action], conf + 0.6, \"action_description\", text)\n",
    "                    )\n",
    "\n",
    "    # Priority 4: Section headers\n",
    "    section_patterns = [\n",
    "        (r\"(Theft|Loss|Stolen|Missing)\\s+Information\", None),\n",
    "        (r\"(Purchase|Sale|Transfer)\\s+Information\", None),\n",
    "    ]\n",
    "    for pattern, _ in section_patterns:\n",
    "        for _, text, conf in results:\n",
    "            m = re.search(pattern, text, re.IGNORECASE)\n",
    "            if m and conf > 0.6:\n",
    "                event = m.group(1).title()\n",
    "                if event.lower() not in excluded_words:\n",
    "                    event_candidates.append((event, conf + 0.4, \"section_header\", text))\n",
    "\n",
    "    # Priority 5: Meaningful single words\n",
    "    for _, text, conf in results:\n",
    "        t = text.strip()\n",
    "        if len(t.split()) == 1 and len(t) > 3 and conf > 0.7 and t.isalpha():\n",
    "            low = t.lower()\n",
    "            if low not in excluded_words and low in incident_types:\n",
    "                event_candidates.append(\n",
    "                    (incident_types[low], conf + 0.2, \"meaningful_word\", text)\n",
    "                )\n",
    "\n",
    "    if not event_candidates:\n",
    "        return None\n",
    "\n",
    "    priority_order = {\n",
    "        \"document_purpose\": 5,\n",
    "        \"incident_type\": 4,\n",
    "        \"action_description\": 3,\n",
    "        \"section_header\": 2,\n",
    "        \"meaningful_word\": 1,\n",
    "    }\n",
    "    event_candidates.sort(\n",
    "        key=lambda x: (priority_order.get(x[2], 0), x[1]), reverse=True\n",
    "    )\n",
    "    return event_candidates[0][0]\n",
    "\n",
    "\n",
    "for filename in file_list:\n",
    "    full_path = os.path.join(test_folder, filename)\n",
    "\n",
    "    # Skip if it's a folder\n",
    "    if os.path.isdir(full_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing: {filename}\")\n",
    "\n",
    "    # Check if it's an image file (excluding BMP which is handled in another cell)\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".tiff\", \".gif\")):\n",
    "        try:\n",
    "            # PaddleOCR (CPU-only)\n",
    "            paddle_raw = paddle_readtext_cpu(full_path, cls=True)\n",
    "            results = normalize_paddle_results(\n",
    "                paddle_raw\n",
    "            )  # [(bbox, text, confidence), ...]\n",
    "\n",
    "            # Place file in a DataFrame (kept for consistency/debugging)\n",
    "            img_id = filename.split(\"/\")[-1].split(\".\")[0]\n",
    "            box_dataframe = pd.DataFrame(\n",
    "                results, columns=[\"bbox\", \"text\", \"confidence\"]\n",
    "            )\n",
    "            box_dataframe[\"img_id\"] = img_id\n",
    "\n",
    "            # Read image for visualization\n",
    "            img = cv2.imread(full_path)\n",
    "\n",
    "            # Display image with detected text if loaded successfully\n",
    "            if img is not None and isinstance(img, np.ndarray):\n",
    "                for bbox, text, confidence in results:\n",
    "                    try:\n",
    "                        pts = np.array(bbox, np.int32)\n",
    "                        pts = pts.reshape((-1, 1, 2))\n",
    "                        cv2.polylines(\n",
    "                            img, [pts], isClosed=True, color=(0, 0, 255), thickness=2\n",
    "                        )\n",
    "                        x, y = pts[0][0]\n",
    "                        cv2.putText(\n",
    "                            img,\n",
    "                            text,\n",
    "                            (int(x), int(y) - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            1,\n",
    "                            (0, 0, 255),\n",
    "                            2,\n",
    "                        )\n",
    "                    except Exception:\n",
    "                        # be resilient to unexpected bbox formats\n",
    "                        pass\n",
    "\n",
    "                # Display the image\n",
    "                try:\n",
    "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    plt.figure(figsize=(12, 8))\n",
    "                    plt.imshow(img_rgb)\n",
    "                    plt.axis(\"off\")\n",
    "                    plt.title(f\"OCR Results for {filename}\")\n",
    "                    plt.show()\n",
    "                except Exception:\n",
    "                    print(f\"Warning: Could not display image {filename}\")\n",
    "\n",
    "            # OCR-focused serial extraction\n",
    "            print(\"\\n--- OCR-Focused Serial Number Detection ---\")\n",
    "\n",
    "            # Use OCR-focused extraction (reuses PaddleOCR under the hood)\n",
    "            serial_num = extract_serial_with_improved_ocr(results, full_path)\n",
    "\n",
    "            if not serial_num:\n",
    "                print(\"No serial number found with OCR-focused extraction.\")\n",
    "                # Show all detected text for manual inspection\n",
    "                if results:\n",
    "                    print(\"All OCR detections:\")\n",
    "                    for i, (bbox, text, confidence) in enumerate(results):\n",
    "                        print(f\"  {i+1}. '{text}' (confidence: {confidence:.4f})\")\n",
    "\n",
    "            print(\"--- End OCR-Focused Serial Number Detection ---\")\n",
    "\n",
    "            # Extract other metadata\n",
    "            matches = 0\n",
    "\n",
    "            # Initialize variables to avoid NameError\n",
    "            name = None\n",
    "            addy = None\n",
    "            date = None\n",
    "            zipcode = None\n",
    "            city = None\n",
    "            state = None\n",
    "\n",
    "            # Name extraction\n",
    "            name_pat = r\"\\b[A-Z][a-z]+\\s[A-Z][a-z]+\\b\"\n",
    "            for bbox, text, confidence in results:\n",
    "                match = re.search(name_pat, text)\n",
    "                if match and confidence > 0.8:\n",
    "                    name = match.group()\n",
    "                    matches += 1\n",
    "                    break\n",
    "\n",
    "            # Address extraction\n",
    "            address_pat = r\"\\b\\d{1,5}\\s[A-Z][a-z]+\\s[A-Z][a-z]+\\b\"\n",
    "            for bbox, text, confidence in results:\n",
    "                match = re.search(address_pat, text)\n",
    "                if match:\n",
    "                    addy = match.group()\n",
    "                    matches += 1\n",
    "                    break\n",
    "\n",
    "            # Date extraction (support / and -)\n",
    "            date_pat = r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b\"\n",
    "            for bbox, text, confidence in results:\n",
    "                match = re.search(date_pat, text)\n",
    "                if match:\n",
    "                    date = match.group()\n",
    "                    matches += 1\n",
    "                    break\n",
    "\n",
    "            # Event type extraction (semantic - mirrored from BMP logic)\n",
    "            event_type = extract_event_type_semantic(results)\n",
    "            if event_type:\n",
    "                matches += 1\n",
    "            else:\n",
    "                event_type = None\n",
    "\n",
    "            # Zipcode extraction\n",
    "            zipcode_pat = r\"\\b\\d{5}(-\\d{4})?\\b\"\n",
    "            if results:\n",
    "                # Get image dimensions for targeted zipcode search\n",
    "                try:\n",
    "                    all_y_coords = [\n",
    "                        point[1] for bbox, _, _ in results for point in bbox\n",
    "                    ]\n",
    "                    image_height = max(all_y_coords) if all_y_coords else 0\n",
    "                except Exception:\n",
    "                    image_height = 0\n",
    "\n",
    "                # Target middle region for zipcodes\n",
    "                top_boundary = image_height * 0.25\n",
    "                bottom_boundary = image_height * 0.50\n",
    "\n",
    "                # Find zipcode candidates\n",
    "                candidates = []\n",
    "                for bbox, text, confidence in results:\n",
    "                    t = text.strip()\n",
    "                    if re.match(zipcode_pat, t):\n",
    "                        try:\n",
    "                            top_y = min([point[1] for point in bbox])\n",
    "                            bottom_y = max([point[1] for point in bbox])\n",
    "                            text_center_y = (top_y + bottom_y) / 2\n",
    "                        except Exception:\n",
    "                            text_center_y = 0\n",
    "\n",
    "                        if top_boundary <= text_center_y <= bottom_boundary:\n",
    "                            candidates.append((t, confidence, text_center_y))\n",
    "\n",
    "                if candidates:\n",
    "                    # Return the candidate with highest confidence\n",
    "                    best_candidate = max(candidates, key=lambda x: x[1])\n",
    "                    zipcode = best_candidate[0]\n",
    "                    matches += 1\n",
    "\n",
    "            # Get city and state information from zipcode\n",
    "            if zipcode:\n",
    "                try:\n",
    "                    end = zipcodes.matching(str(zipcode))\n",
    "                    if end:\n",
    "                        city = end[0][\"city\"]\n",
    "                        state = end[0][\"state\"]\n",
    "                        matches += 2\n",
    "                except Exception as e:\n",
    "                    print(f\"Error looking up zipcode {zipcode}: {e}\")\n",
    "\n",
    "            # Get file metadata\n",
    "            metadata = get_file_metadata(full_path)\n",
    "\n",
    "            # Create relative path for source_file\n",
    "            relative_path = os.path.join(\"test\", filename)\n",
    "\n",
    "            # Construct complete address\n",
    "            complete_address = \"\"\n",
    "            if addy:\n",
    "                complete_address = addy\n",
    "                if city and state:\n",
    "                    complete_address += f\", {city}, {state}\"\n",
    "                    if zipcode:\n",
    "                        complete_address += f\" {zipcode}\"\n",
    "                elif zipcode:\n",
    "                    complete_address += f\" {zipcode}\"\n",
    "\n",
    "            # Create CSV record with standardized columns, using 'Missing' for empty values\n",
    "            csv_record = {\n",
    "                \"serial_number\": str(serial_num) if serial_num else \"Missing\",\n",
    "                \"event_type\": str(event_type) if event_type else \"Missing\",\n",
    "                \"event_date\": str(date) if date else \"Missing\",\n",
    "                \"associated_name\": str(name) if name else \"Missing\",\n",
    "                \"associated_address\": (\n",
    "                    str(complete_address) if complete_address else \"Missing\"\n",
    "                ),\n",
    "                \"source_file\": relative_path,\n",
    "                \"file_created\": metadata[\"file_creation_date\"],\n",
    "                \"file_modified\": metadata[\"file_modification_date\"],\n",
    "            }\n",
    "\n",
    "            csv_results.append(csv_record)\n",
    "\n",
    "            # Add extracted information to df1 (legacy format)\n",
    "            new_row = {\n",
    "                \"serial_number\": str(serial_num) if serial_num else None,\n",
    "                \"name\": str(name) if name else None,\n",
    "                \"address\": str(addy) if addy else None,\n",
    "                \"date\": str(date) if date else None,\n",
    "                \"zipcode\": str(zipcode) if zipcode else None,\n",
    "                \"city\": str(city) if city else None,\n",
    "                \"state\": str(state) if state else None,\n",
    "            }\n",
    "\n",
    "            # Add the row\n",
    "            df1 = pd.concat([df1, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "            # Summary\n",
    "            print(\"Summary...\")\n",
    "            print(\"Serial Number:\", serial_num if serial_num else \"Missing\")\n",
    "            print(\"Event Type:\", event_type if event_type else \"Missing\")\n",
    "            print(\"Name:\", name if name else \"Missing\")\n",
    "            print(\"Address:\", complete_address if complete_address else \"Missing\")\n",
    "            print(\"Date:\", date if date else \"Missing\")\n",
    "            print(\"Zipcode:\", zipcode if zipcode else \"Missing\")\n",
    "            print(\"City:\", city if city else \"Missing\")\n",
    "            print(\"State:\", state if state else \"Missing\")\n",
    "            print(\"Matches found:\", matches, \"/ 7\")\n",
    "\n",
    "            print(\"File Metadata:\")\n",
    "            for key, value in metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "            # Add metadata to df1 (legacy format)\n",
    "            if serial_num:\n",
    "                df1.loc[df1[\"serial_number\"] == str(serial_num), \"filename\"] = metadata[\n",
    "                    \"filename\"\n",
    "                ]\n",
    "                df1.loc[\n",
    "                    df1[\"serial_number\"] == str(serial_num), \"file_creation_date\"\n",
    "                ] = metadata[\"file_creation_date\"]\n",
    "                df1.loc[\n",
    "                    df1[\"serial_number\"] == str(serial_num), \"file_modification_date\"\n",
    "                ] = metadata[\"file_modification_date\"]\n",
    "                df1.loc[df1[\"serial_number\"] == str(serial_num), \"file_location\"] = (\n",
    "                    metadata[\"file_location\"]\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "\n",
    "    elif filename.lower().endswith(\".bmp\"):\n",
    "        print(f\"BMP file (handled in separate cell): {filename}\")\n",
    "\n",
    "    elif filename.lower().endswith(\".pdf\"):\n",
    "        print(f\"PDF file (would need special handling): {filename}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping non-image file: {filename}\")\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "\n",
    "# Create and save CSV results to reports folder\n",
    "if csv_results:\n",
    "    results_df = pd.DataFrame(csv_results)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"NON-BMP IMAGE EXTRACTION RESULTS (CSV FORMAT)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results_df.to_csv(index=False))\n",
    "\n",
    "    # Create reports folder if it doesn't exist\n",
    "    reports_folder = os.path.join(os.getcwd(), \"reports\")\n",
    "    os.makedirs(reports_folder, exist_ok=True)\n",
    "\n",
    "    # Save to file in reports folder\n",
    "    output_file = os.path.join(reports_folder, \"non_bmp_image_extraction_results.csv\")\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nResults saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"\\nNo results to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: Process non-BMP image files with PaddleOCR (CPU-only) and CSV output\n",
    "\n",
    "# Safe imports (ok to re-import in a notebook cell)\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import zipcodes\n",
    "\n",
    "# PaddleOCR (CPU-only)\n",
    "try:\n",
    "    import paddle\n",
    "    from paddleocr import PaddleOCR\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\n",
    "        \"Paddle not available in this kernel. Install with:\\n\"\n",
    "        \"  pip install paddlepaddle==2.6.1 'paddleocr>=2.7'\\n\"\n",
    "        \"or switch to your Python 3.11 env where Paddle is installed.\"\n",
    "    ) from e\n",
    "\n",
    "# Add metadata columns to df1\n",
    "df1[\"filename\"] = df1[\"filename\"].astype(str)\n",
    "df1[\"file_creation_date\"] = df1[\"file_creation_date\"].astype(str)\n",
    "df1[\"file_modification_date\"] = df1[\"file_modification_date\"].astype(str)\n",
    "df1[\"file_location\"] = df1[\"file_location\"].astype(str)\n",
    "\n",
    "# Path to 'test' folder\n",
    "test_folder = os.path.join(os.getcwd(), \"test\")\n",
    "\n",
    "# Force CPU (current macOS wheels do not support 'mps')\n",
    "try:\n",
    "    paddle.set_device(\"cpu\")\n",
    "    print(\"Paddle set to CPU (MPS not available in this build).\")\n",
    "except Exception as e:\n",
    "    print(f\"Falling back to CPU due to error: {e}\")\n",
    "    paddle.set_device(\"cpu\")\n",
    "\n",
    "# Build PaddleOCR reader (avoid downscaling: large det_max_side_len)\n",
    "ocr_reader = PaddleOCR(\n",
    "    use_angle_cls=True,\n",
    "    lang=\"en\",\n",
    "    det_max_side_len=4096,  # prevent unwanted downscaling on high-res images\n",
    "    show_log=False,\n",
    ")\n",
    "\n",
    "# Get all files in 'test' folder (we'll still skip BMPs in the loop)\n",
    "file_list = os.listdir(test_folder)\n",
    "\n",
    "# Initialize CSV results list\n",
    "csv_results = []\n",
    "\n",
    "print(f\"Found {len(file_list)} files in test folder\")\n",
    "\n",
    "\n",
    "def normalize_paddle_results(paddle_result):\n",
    "    \"\"\"\n",
    "    Convert PaddleOCR result to a flat list of (bbox, text, confidence) like EasyOCR.\n",
    "    Paddle format (for one image) is typically:\n",
    "      [ [ [points, (text, conf)], [points, (text, conf)], ... ] ]\n",
    "    or already a single list depending on version.\n",
    "    \"\"\"\n",
    "    flat = []\n",
    "    if not paddle_result:\n",
    "        return flat\n",
    "\n",
    "    # Handle both nested [list] and already-flat list cases\n",
    "    if (\n",
    "        isinstance(paddle_result, list)\n",
    "        and len(paddle_result) > 0\n",
    "        and isinstance(paddle_result[0], list)\n",
    "        and len(paddle_result[0]) > 0\n",
    "        and isinstance(paddle_result[0][0], (list, tuple))\n",
    "        and len(paddle_result[0][0]) == 2\n",
    "    ):\n",
    "        candidates = paddle_result[0]\n",
    "    else:\n",
    "        candidates = paddle_result\n",
    "\n",
    "    for item in candidates:\n",
    "        # item is [points, (text, conf)]\n",
    "        try:\n",
    "            points, tc = item\n",
    "            text, conf = tc\n",
    "            # Normalize bbox as list of 4 points [[x1,y1],...]\n",
    "            bbox = points\n",
    "            flat.append((bbox, str(text), float(conf)))\n",
    "        except Exception:\n",
    "            # Be resilient if structure varies\n",
    "            continue\n",
    "    return flat\n",
    "\n",
    "\n",
    "def paddle_readtext_cpu(image_or_path, **kwargs):\n",
    "    \"\"\"\n",
    "    CPU-only OCR call wrapper (kept as a function for consistency).\n",
    "    \"\"\"\n",
    "    return ocr_reader.ocr(image_or_path, **kwargs)\n",
    "\n",
    "\n",
    "def extract_serial_with_improved_ocr(paddle_like_results, full_path):\n",
    "    \"\"\"Extract serial number focusing on OCR accuracy improvements\"\"\"\n",
    "\n",
    "    def preprocess_image_for_ocr(image_path):\n",
    "        \"\"\"Apply image preprocessing to improve OCR accuracy\"\"\"\n",
    "        import cv2\n",
    "        import numpy as np\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return None\n",
    "\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply different preprocessing techniques\n",
    "        preprocessed_images = []\n",
    "\n",
    "        # Original grayscale\n",
    "        preprocessed_images.append((\"original_gray\", gray))\n",
    "\n",
    "        # Gaussian blur to reduce noise\n",
    "        blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        preprocessed_images.append((\"blurred\", blurred))\n",
    "\n",
    "        # Sharpen\n",
    "        kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "        sharpened = cv2.filter2D(gray, -1, kernel)\n",
    "        preprocessed_images.append((\"sharpened\", sharpened))\n",
    "\n",
    "        # Adaptive threshold\n",
    "        adaptive = cv2.adaptiveThreshold(\n",
    "            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
    "        )\n",
    "        preprocessed_images.append((\"adaptive_thresh\", adaptive))\n",
    "\n",
    "        # Morphological operations to clean up\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "        morph = cv2.morphologyEx(adaptive, cv2.MORPH_CLOSE, kernel)\n",
    "        preprocessed_images.append((\"morphological\", morph))\n",
    "\n",
    "        return preprocessed_images\n",
    "\n",
    "    def run_multiple_ocr_passes(image_path):\n",
    "        \"\"\"Run OCR with different parameters and preprocessing using PaddleOCR\"\"\"\n",
    "        all_candidates = []\n",
    "\n",
    "        # Standard OCR (on original)\n",
    "        base_raw = paddle_readtext_cpu(image_path, cls=True)\n",
    "        base = normalize_paddle_results(base_raw)\n",
    "        for bbox, text, confidence in base:\n",
    "            if text.strip() and confidence > 0.3:\n",
    "                all_candidates.append(\n",
    "                    {\n",
    "                        \"text\": text.strip(),\n",
    "                        \"confidence\": confidence,\n",
    "                        \"source\": \"standard_ocr\",\n",
    "                        \"bbox\": bbox,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Try different preprocessing\n",
    "        preprocessed_images = preprocess_image_for_ocr(image_path)\n",
    "        if preprocessed_images:\n",
    "            for prep_name, prep_img in preprocessed_images:\n",
    "                try:\n",
    "                    # PaddleOCR accepts np.ndarray as input\n",
    "                    prep_raw = paddle_readtext_cpu(prep_img, cls=True)\n",
    "                    prep_results = normalize_paddle_results(prep_raw)\n",
    "\n",
    "                    for bbox, text, confidence in prep_results:\n",
    "                        if (\n",
    "                            text.strip() and confidence > 0.2\n",
    "                        ):  # Lower threshold for preprocessed\n",
    "                            all_candidates.append(\n",
    "                                {\n",
    "                                    \"text\": text.strip(),\n",
    "                                    \"confidence\": confidence\n",
    "                                    * 0.9,  # Slight penalty for preprocessed\n",
    "                                    \"source\": f\"preprocessed_{prep_name}\",\n",
    "                                    \"bbox\": bbox,\n",
    "                                }\n",
    "                            )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {prep_name} preprocessing: {e}\")\n",
    "\n",
    "        return all_candidates\n",
    "\n",
    "    def find_potential_serials(candidates):\n",
    "        \"\"\"Find potential serial numbers from all OCR candidates\"\"\"\n",
    "        potential_serials = []\n",
    "\n",
    "        # Common words that are clearly NOT serial numbers\n",
    "        non_serial_words = {\n",
    "            \"date\",\n",
    "            \"city\",\n",
    "            \"state\",\n",
    "            \"name\",\n",
    "            \"address\",\n",
    "            \"phone\",\n",
    "            \"email\",\n",
    "            \"zip\",\n",
    "            \"code\",\n",
    "            \"serial\",\n",
    "            \"number\",\n",
    "            \"model\",\n",
    "            \"make\",\n",
    "            \"caliber\",\n",
    "            \"type\",\n",
    "            \"manufacturer\",\n",
    "            \"license\",\n",
    "            \"permit\",\n",
    "            \"registration\",\n",
    "            \"form\",\n",
    "            \"page\",\n",
    "            \"section\",\n",
    "            \"dallas\",\n",
    "            \"texas\",\n",
    "            \"california\",\n",
    "            \"florida\",\n",
    "            \"new\",\n",
    "            \"york\",\n",
    "            \"smith\",\n",
    "            \"wesson\",\n",
    "            \"colt\",\n",
    "            \"ruger\",\n",
    "            \"glock\",\n",
    "            \"sig\",\n",
    "            \"sauer\",\n",
    "            \"the\",\n",
    "            \"and\",\n",
    "            \"for\",\n",
    "            \"with\",\n",
    "            \"this\",\n",
    "            \"that\",\n",
    "            \"from\",\n",
    "            \"have\",\n",
    "            \"been\",\n",
    "            \"firearm\",\n",
    "            \"pistol\",\n",
    "            \"rifle\",\n",
    "            \"gun\",\n",
    "            \"weapon\",\n",
    "            \"barrel\",\n",
    "            \"frame\",\n",
    "            \"slide\",\n",
    "        }\n",
    "\n",
    "        for candidate in candidates:\n",
    "            text = candidate[\"text\"]\n",
    "            confidence = candidate[\"confidence\"]\n",
    "            source = candidate[\"source\"]\n",
    "\n",
    "            # Remove spaces and special characters\n",
    "            cleaned = \"\".join(c for c in text if c.isalnum())\n",
    "\n",
    "            # Skip if it's clearly not a serial number\n",
    "            if cleaned.lower() in non_serial_words:\n",
    "                continue\n",
    "\n",
    "            # Skip if it's all letters and looks like a common word (likely not a serial)\n",
    "            if cleaned.isalpha() and len(cleaned) <= 8:\n",
    "                continue\n",
    "\n",
    "            # Check if it could be a serial (basic length check)\n",
    "            if 4 <= len(cleaned) <= 15 and cleaned.isalnum():\n",
    "                # Additional check: prefer mixed alphanumeric or longer sequences\n",
    "                has_letters = any(c.isalpha() for c in cleaned)\n",
    "                has_numbers = any(c.isdigit() for c in cleaned)\n",
    "\n",
    "                # Boost confidence for mixed alphanumeric (more likely to be serials)\n",
    "                if has_letters and has_numbers:\n",
    "                    confidence *= 1.2\n",
    "                elif len(cleaned) >= 6:  # Or longer sequences\n",
    "                    confidence *= 1.1\n",
    "\n",
    "                potential_serials.append(\n",
    "                    {\n",
    "                        \"serial\": cleaned,\n",
    "                        \"original_text\": text,\n",
    "                        \"confidence\": confidence,\n",
    "                        \"source\": source,\n",
    "                        \"bbox\": candidate[\"bbox\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Also try spaced character reconstruction for this candidate\n",
    "            if \" \" in text and len(text.split()) >= 3:\n",
    "                parts = [p.strip() for p in text.split() if p.strip().isalnum()]\n",
    "                if len(parts) >= 3:\n",
    "                    reconstructed = \"\".join(parts)\n",
    "                    if (\n",
    "                        4 <= len(reconstructed) <= 15\n",
    "                        and reconstructed.isalnum()\n",
    "                        and reconstructed.lower() not in non_serial_words\n",
    "                    ):\n",
    "                        potential_serials.append(\n",
    "                            {\n",
    "                                \"serial\": reconstructed,\n",
    "                                \"original_text\": text,\n",
    "                                \"confidence\": confidence * 0.8,\n",
    "                                \"source\": f\"{source}_reconstructed\",\n",
    "                                \"bbox\": candidate[\"bbox\"],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "        return potential_serials\n",
    "\n",
    "    # Run multiple OCR passes\n",
    "    print(\"Running multiple OCR passes for improved accuracy...\")\n",
    "    all_candidates = run_multiple_ocr_passes(full_path)\n",
    "\n",
    "    # Find potential serials\n",
    "    potential_serials = find_potential_serials(all_candidates)\n",
    "\n",
    "    # Remove duplicates and sort by confidence\n",
    "    unique_serials = {}\n",
    "    for serial_info in potential_serials:\n",
    "        serial = serial_info[\"serial\"]\n",
    "        if (\n",
    "            serial not in unique_serials\n",
    "            or serial_info[\"confidence\"] > unique_serials[serial][\"confidence\"]\n",
    "        ):\n",
    "            unique_serials[serial] = serial_info\n",
    "\n",
    "    # Sort by confidence\n",
    "    sorted_serials = sorted(\n",
    "        unique_serials.values(), key=lambda x: x[\"confidence\"], reverse=True\n",
    "    )\n",
    "\n",
    "    # Display top candidates for transparency\n",
    "    print(f\"\\nSerial number candidates found:\")\n",
    "    if sorted_serials:\n",
    "        for i, serial_info in enumerate(sorted_serials[:5]):  # Show top 5\n",
    "            marker = \"→ SELECTED\" if i == 0 else \"  \"\n",
    "            print(\n",
    "                f\"  {marker} '{serial_info['serial']}' (confidence: {serial_info['confidence']:.3f}) from {serial_info['source']}\"\n",
    "            )\n",
    "            print(f\"      Original text: '{serial_info['original_text']}'\")\n",
    "        return sorted_serials[0][\"serial\"]\n",
    "    else:\n",
    "        print(\"  No potential serial numbers found\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_event_type_semantic(results):\n",
    "    \"\"\"\n",
    "    Mirror BMP-style event detection with semantic priorities.\n",
    "    Returns best event_type or None.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    event_candidates = []\n",
    "\n",
    "    # Excluded administrative words\n",
    "    excluded_words = {\n",
    "        \"atf\",\n",
    "        \"bureau\",\n",
    "        \"federal\",\n",
    "        \"department\",\n",
    "        \"justice\",\n",
    "        \"alcohol\",\n",
    "        \"tobacco\",\n",
    "        \"firearms\",\n",
    "        \"explosives\",\n",
    "        \"form\",\n",
    "        \"section\",\n",
    "        \"page\",\n",
    "        \"number\",\n",
    "        \"code\",\n",
    "        \"licensee\",\n",
    "        \"information\",\n",
    "        \"details\",\n",
    "        \"description\",\n",
    "        \"brief\",\n",
    "        \"name\",\n",
    "        \"address\",\n",
    "        \"telephone\",\n",
    "        \"date\",\n",
    "        \"time\",\n",
    "        \"signature\",\n",
    "        \"certification\",\n",
    "    }\n",
    "\n",
    "    # Priority 1: Document purpose indicators\n",
    "    purpose_patterns = [\n",
    "        (\n",
    "            r\"(Theft|Loss|Stolen|Missing|Burglary|Robbery|Larceny).*?Report\",\n",
    "            \"Theft/Loss\",\n",
    "        ),\n",
    "        (r\"Inventory\\s+(Theft|Loss)\", \"Theft/Loss\"),\n",
    "        (\n",
    "            r\"(Purchase|Sale|Transfer|Registration|Acquisition|Disposition).*?Report\",\n",
    "            None,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for pattern, fixed_event in purpose_patterns:\n",
    "        for _, text, conf in results:\n",
    "            m = re.search(pattern, text, re.IGNORECASE)\n",
    "            if m and conf > 0.4:\n",
    "                event = fixed_event if fixed_event else m.group(1).title()\n",
    "                event_candidates.append((event, conf + 1.0, \"document_purpose\", text))\n",
    "\n",
    "    # Priority 2: Specific incident/crime types\n",
    "    incident_types = {\n",
    "        \"burglary\": \"Burglary\",\n",
    "        \"robbery\": \"Robbery\",\n",
    "        \"larceny\": \"Larceny\",\n",
    "        \"theft\": \"Theft\",\n",
    "        \"stolen\": \"Theft\",\n",
    "        \"missing\": \"Missing\",\n",
    "        \"lost\": \"Loss\",\n",
    "    }\n",
    "    for _, text, conf in results:\n",
    "        text_clean = text.lower().strip()\n",
    "        if text_clean in incident_types and conf > 0.5:\n",
    "            event_candidates.append(\n",
    "                (incident_types[text_clean], conf + 0.8, \"incident_type\", text)\n",
    "            )\n",
    "\n",
    "    # Priority 3: Action descriptions\n",
    "    action_patterns = [\n",
    "        (r\"was\\s+(stolen|taken|missing|lost)\", None),\n",
    "        (r\"were\\s+(stolen|taken|missing|lost)\", None),\n",
    "        (r\"firearm\\s+was\\s+(\\w+)\", None),\n",
    "        (r\"gun\\s+was\\s+(\\w+)\", None),\n",
    "    ]\n",
    "    for pattern, _ in action_patterns:\n",
    "        for _, text, conf in results:\n",
    "            m = re.search(pattern, text, re.IGNORECASE)\n",
    "            if m and conf > 0.6:\n",
    "                action = m.group(1).lower()\n",
    "                if action in incident_types:\n",
    "                    event_candidates.append(\n",
    "                        (incident_types[action], conf + 0.6, \"action_description\", text)\n",
    "                    )\n",
    "\n",
    "    # Priority 4: Section headers\n",
    "    section_patterns = [\n",
    "        (r\"(Theft|Loss|Stolen|Missing)\\s+Information\", None),\n",
    "        (r\"(Purchase|Sale|Transfer)\\s+Information\", None),\n",
    "    ]\n",
    "    for pattern, _ in section_patterns:\n",
    "        for _, text, conf in results:\n",
    "            m = re.search(pattern, text, re.IGNORECASE)\n",
    "            if m and conf > 0.6:\n",
    "                event = m.group(1).title()\n",
    "                if event.lower() not in excluded_words:\n",
    "                    event_candidates.append((event, conf + 0.4, \"section_header\", text))\n",
    "\n",
    "    # Priority 5: Meaningful single words\n",
    "    for _, text, conf in results:\n",
    "        t = text.strip()\n",
    "        if len(t.split()) == 1 and len(t) > 3 and conf > 0.7 and t.isalpha():\n",
    "            low = t.lower()\n",
    "            if low not in excluded_words and low in incident_types:\n",
    "                event_candidates.append(\n",
    "                    (incident_types[low], conf + 0.2, \"meaningful_word\", text)\n",
    "                )\n",
    "\n",
    "    if not event_candidates:\n",
    "        return None\n",
    "\n",
    "    priority_order = {\n",
    "        \"document_purpose\": 5,\n",
    "        \"incident_type\": 4,\n",
    "        \"action_description\": 3,\n",
    "        \"section_header\": 2,\n",
    "        \"meaningful_word\": 1,\n",
    "    }\n",
    "    event_candidates.sort(\n",
    "        key=lambda x: (priority_order.get(x[2], 0), x[1]), reverse=True\n",
    "    )\n",
    "    return event_candidates[0][0]\n",
    "\n",
    "\n",
    "for filename in file_list:\n",
    "    full_path = os.path.join(test_folder, filename)\n",
    "\n",
    "    # Skip if it's a folder\n",
    "    if os.path.isdir(full_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing: {filename}\")\n",
    "\n",
    "    # Check if it's an image file (excluding BMP which is handled in another cell)\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".tiff\", \".gif\")):\n",
    "        try:\n",
    "            # PaddleOCR (CPU-only)\n",
    "            paddle_raw = paddle_readtext_cpu(full_path, cls=True)\n",
    "            results = normalize_paddle_results(\n",
    "                paddle_raw\n",
    "            )  # [(bbox, text, confidence), ...]\n",
    "\n",
    "            # Place file in a DataFrame (kept for consistency/debugging)\n",
    "            img_id = filename.split(\"/\")[-1].split(\".\")[0]\n",
    "            box_dataframe = pd.DataFrame(\n",
    "                results, columns=[\"bbox\", \"text\", \"confidence\"]\n",
    "            )\n",
    "            box_dataframe[\"img_id\"] = img_id\n",
    "\n",
    "            # Read image for visualization\n",
    "            img = cv2.imread(full_path)\n",
    "\n",
    "            # Display image with detected text if loaded successfully\n",
    "            if img is not None and isinstance(img, np.ndarray):\n",
    "                for bbox, text, confidence in results:\n",
    "                    try:\n",
    "                        pts = np.array(bbox, np.int32)\n",
    "                        pts = pts.reshape((-1, 1, 2))\n",
    "                        cv2.polylines(\n",
    "                            img, [pts], isClosed=True, color=(0, 0, 255), thickness=2\n",
    "                        )\n",
    "                        x, y = pts[0][0]\n",
    "                        cv2.putText(\n",
    "                            img,\n",
    "                            text,\n",
    "                            (int(x), int(y) - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            1,\n",
    "                            (0, 0, 255),\n",
    "                            2,\n",
    "                        )\n",
    "                    except Exception:\n",
    "                        # be resilient to unexpected bbox formats\n",
    "                        pass\n",
    "\n",
    "                # Display the image\n",
    "                try:\n",
    "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    plt.figure(figsize=(12, 8))\n",
    "                    plt.imshow(img_rgb)\n",
    "                    plt.axis(\"off\")\n",
    "                    plt.title(f\"OCR Results for {filename}\")\n",
    "                    plt.show()\n",
    "                except Exception:\n",
    "                    print(f\"Warning: Could not display image {filename}\")\n",
    "\n",
    "            # OCR-focused serial extraction\n",
    "            print(\"\\n--- OCR-Focused Serial Number Detection ---\")\n",
    "\n",
    "            # Use OCR-focused extraction (reuses PaddleOCR under the hood)\n",
    "            serial_num = extract_serial_with_improved_ocr(results, full_path)\n",
    "\n",
    "            if not serial_num:\n",
    "                print(\"No serial number found with OCR-focused extraction.\")\n",
    "                # Show all detected text for manual inspection\n",
    "                if results:\n",
    "                    print(\"All OCR detections:\")\n",
    "                    for i, (bbox, text, confidence) in enumerate(results):\n",
    "                        print(f\"  {i+1}. '{text}' (confidence: {confidence:.4f})\")\n",
    "\n",
    "            print(\"--- End OCR-Focused Serial Number Detection ---\")\n",
    "\n",
    "            # Extract other metadata\n",
    "            matches = 0\n",
    "\n",
    "            # Initialize variables to avoid NameError\n",
    "            name = None\n",
    "            addy = None\n",
    "            date = None\n",
    "            zipcode = None\n",
    "            city = None\n",
    "            state = None\n",
    "\n",
    "            # Name extraction\n",
    "            name_pat = r\"\\b[A-Z][a-z]+\\s[A-Z][a-z]+\\b\"\n",
    "            for bbox, text, confidence in results:\n",
    "                match = re.search(name_pat, text)\n",
    "                if match and confidence > 0.8:\n",
    "                    name = match.group()\n",
    "                    matches += 1\n",
    "                    break\n",
    "\n",
    "            # Address extraction\n",
    "            address_pat = r\"\\b\\d{1,5}\\s[A-Z][a-z]+\\s[A-Z][a-z]+\\b\"\n",
    "            for bbox, text, confidence in results:\n",
    "                match = re.search(address_pat, text)\n",
    "                if match:\n",
    "                    addy = match.group()\n",
    "                    matches += 1\n",
    "                    break\n",
    "\n",
    "            # Date extraction (support / and -)\n",
    "            date_pat = r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b\"\n",
    "            for bbox, text, confidence in results:\n",
    "                match = re.search(date_pat, text)\n",
    "                if match:\n",
    "                    date = match.group()\n",
    "                    matches += 1\n",
    "                    break\n",
    "\n",
    "            # Event type extraction (semantic - mirrored from BMP logic)\n",
    "            event_type = extract_event_type_semantic(results)\n",
    "            if event_type:\n",
    "                matches += 1\n",
    "            else:\n",
    "                event_type = None\n",
    "\n",
    "            # Zipcode extraction\n",
    "            zipcode_pat = r\"\\b\\d{5}(-\\d{4})?\\b\"\n",
    "            if results:\n",
    "                # Get image dimensions for targeted zipcode search\n",
    "                try:\n",
    "                    all_y_coords = [\n",
    "                        point[1] for bbox, _, _ in results for point in bbox\n",
    "                    ]\n",
    "                    image_height = max(all_y_coords) if all_y_coords else 0\n",
    "                except Exception:\n",
    "                    image_height = 0\n",
    "\n",
    "                # Target middle region for zipcodes\n",
    "                top_boundary = image_height * 0.25\n",
    "                bottom_boundary = image_height * 0.50\n",
    "\n",
    "                # Find zipcode candidates\n",
    "                candidates = []\n",
    "                for bbox, text, confidence in results:\n",
    "                    t = text.strip()\n",
    "                    if re.match(zipcode_pat, t):\n",
    "                        try:\n",
    "                            top_y = min([point[1] for point in bbox])\n",
    "                            bottom_y = max([point[1] for point in bbox])\n",
    "                            text_center_y = (top_y + bottom_y) / 2\n",
    "                        except Exception:\n",
    "                            text_center_y = 0\n",
    "\n",
    "                        if top_boundary <= text_center_y <= bottom_boundary:\n",
    "                            candidates.append((t, confidence, text_center_y))\n",
    "\n",
    "                if candidates:\n",
    "                    # Return the candidate with highest confidence\n",
    "                    best_candidate = max(candidates, key=lambda x: x[1])\n",
    "                    zipcode = best_candidate[0]\n",
    "                    matches += 1\n",
    "\n",
    "            # Get city and state information from zipcode\n",
    "            if zipcode:\n",
    "                try:\n",
    "                    end = zipcodes.matching(str(zipcode))\n",
    "                    if end:\n",
    "                        city = end[0][\"city\"]\n",
    "                        state = end[0][\"state\"]\n",
    "                        matches += 2\n",
    "                except Exception as e:\n",
    "                    print(f\"Error looking up zipcode {zipcode}: {e}\")\n",
    "\n",
    "            # Get file metadata\n",
    "            metadata = get_file_metadata(full_path)\n",
    "\n",
    "            # Create relative path for source_file\n",
    "            relative_path = os.path.join(\"test\", filename)\n",
    "\n",
    "            # Construct complete address\n",
    "            complete_address = \"\"\n",
    "            if addy:\n",
    "                complete_address = addy\n",
    "                if city and state:\n",
    "                    complete_address += f\", {city}, {state}\"\n",
    "                    if zipcode:\n",
    "                        complete_address += f\" {zipcode}\"\n",
    "                elif zipcode:\n",
    "                    complete_address += f\" {zipcode}\"\n",
    "\n",
    "            # Create CSV record with standardized columns, using 'Missing' for empty values\n",
    "            csv_record = {\n",
    "                \"serial_number\": str(serial_num) if serial_num else \"Missing\",\n",
    "                \"event_type\": str(event_type) if event_type else \"Missing\",\n",
    "                \"event_date\": str(date) if date else \"Missing\",\n",
    "                \"associated_name\": str(name) if name else \"Missing\",\n",
    "                \"associated_address\": (\n",
    "                    str(complete_address) if complete_address else \"Missing\"\n",
    "                ),\n",
    "                \"source_file\": relative_path,\n",
    "                \"file_created\": metadata[\"file_creation_date\"],\n",
    "                \"file_modified\": metadata[\"file_modification_date\"],\n",
    "            }\n",
    "\n",
    "            csv_results.append(csv_record)\n",
    "\n",
    "            # Add extracted information to df1 (legacy format)\n",
    "            new_row = {\n",
    "                \"serial_number\": str(serial_num) if serial_num else None,\n",
    "                \"name\": str(name) if name else None,\n",
    "                \"address\": str(addy) if addy else None,\n",
    "                \"date\": str(date) if date else None,\n",
    "                \"zipcode\": str(zipcode) if zipcode else None,\n",
    "                \"city\": str(city) if city else None,\n",
    "                \"state\": str(state) if state else None,\n",
    "            }\n",
    "\n",
    "            # Add the row\n",
    "            df1 = pd.concat([df1, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "            # Summary\n",
    "            print(\"Summary...\")\n",
    "            print(\"Serial Number:\", serial_num if serial_num else \"Missing\")\n",
    "            print(\"Event Type:\", event_type if event_type else \"Missing\")\n",
    "            print(\"Name:\", name if name else \"Missing\")\n",
    "            print(\"Address:\", complete_address if complete_address else \"Missing\")\n",
    "            print(\"Date:\", date if date else \"Missing\")\n",
    "            print(\"Zipcode:\", zipcode if zipcode else \"Missing\")\n",
    "            print(\"City:\", city if city else \"Missing\")\n",
    "            print(\"State:\", state if state else \"Missing\")\n",
    "            print(\"Matches found:\", matches, \"/ 7\")\n",
    "\n",
    "            print(\"File Metadata:\")\n",
    "            for key, value in metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "            # Add metadata to df1 (legacy format)\n",
    "            if serial_num:\n",
    "                df1.loc[df1[\"serial_number\"] == str(serial_num), \"filename\"] = metadata[\n",
    "                    \"filename\"\n",
    "                ]\n",
    "                df1.loc[\n",
    "                    df1[\"serial_number\"] == str(serial_num), \"file_creation_date\"\n",
    "                ] = metadata[\"file_creation_date\"]\n",
    "                df1.loc[\n",
    "                    df1[\"serial_number\"] == str(serial_num), \"file_modification_date\"\n",
    "                ] = metadata[\"file_modification_date\"]\n",
    "                df1.loc[df1[\"serial_number\"] == str(serial_num), \"file_location\"] = (\n",
    "                    metadata[\"file_location\"]\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "\n",
    "    elif filename.lower().endswith(\".bmp\"):\n",
    "        print(f\"BMP file (handled in separate cell): {filename}\")\n",
    "\n",
    "    elif filename.lower().endswith(\".pdf\"):\n",
    "        print(f\"PDF file (would need special handling): {filename}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping non-image file: {filename}\")\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "\n",
    "# Create and save CSV results to reports folder\n",
    "if csv_results:\n",
    "    results_df = pd.DataFrame(csv_results)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"NON-BMP IMAGE EXTRACTION RESULTS (CSV FORMAT)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results_df.to_csv(index=False))\n",
    "\n",
    "    # Create reports folder if it doesn't exist\n",
    "    reports_folder = os.path.join(os.getcwd(), \"reports\")\n",
    "    os.makedirs(reports_folder, exist_ok=True)\n",
    "\n",
    "    # Save to file in reports folder\n",
    "    output_file = os.path.join(reports_folder, \"non_bmp_image_extraction_results.csv\")\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nResults saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"\\nNo results to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6016d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: Build Full Image Extraction Report with All Available Fields\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Ensure df1 exists\n",
    "try:\n",
    "    df1  # noqa: F821\n",
    "except NameError:\n",
    "    raise RuntimeError(\"df1 is not defined earlier in the notebook.\")\n",
    "\n",
    "ROOT = os.getcwd()\n",
    "\n",
    "\n",
    "def _to_rel(p: str) -> str:\n",
    "    if isinstance(p, str) and p:\n",
    "        try:\n",
    "            return os.path.relpath(p, ROOT)\n",
    "        except Exception:\n",
    "            return os.path.basename(p)\n",
    "    return p\n",
    "\n",
    "\n",
    "def _normalize_source_file(s: str) -> str:\n",
    "    if isinstance(s, str) and s:\n",
    "        try:\n",
    "            return os.path.relpath(s, ROOT) if os.path.isabs(s) else s\n",
    "        except Exception:\n",
    "            return os.path.basename(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def _blank_mask(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(str)\n",
    "    return series.isna() | s.str.strip().isin([\"\", \"None\", \"nan\", \"NaT\"])\n",
    "\n",
    "\n",
    "def _is_blank_val(v) -> bool:\n",
    "    if v is None:\n",
    "        return True\n",
    "    try:\n",
    "        if pd.isna(v):\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return isinstance(v, str) and v.strip() in (\"\", \"None\", \"nan\", \"NaT\")\n",
    "\n",
    "\n",
    "# 1) Standardize / backfill unified semantic columns\n",
    "# event_type <= event_name\n",
    "if \"event_type\" not in df1.columns:\n",
    "    df1[\"event_type\"] = None\n",
    "if \"event_name\" in df1.columns:\n",
    "    mask = _blank_mask(df1[\"event_type\"])\n",
    "    df1.loc[mask, \"event_type\"] = df1.loc[mask, \"event_name\"]\n",
    "\n",
    "# event_date <= date\n",
    "if \"event_date\" not in df1.columns:\n",
    "    df1[\"event_date\"] = None\n",
    "if \"date\" in df1.columns:\n",
    "    mask = _blank_mask(df1[\"event_date\"])\n",
    "    df1.loc[mask, \"event_date\"] = df1.loc[mask, \"date\"]\n",
    "\n",
    "# associated_name <= name\n",
    "if \"associated_name\" not in df1.columns:\n",
    "    df1[\"associated_name\"] = None\n",
    "if \"name\" in df1.columns:\n",
    "    mask = _blank_mask(df1[\"associated_name\"])\n",
    "    df1.loc[mask, \"associated_name\"] = df1.loc[mask, \"name\"]\n",
    "\n",
    "# Prepare address components\n",
    "for col in [\"associated_address\", \"address\"]:\n",
    "    if col not in df1.columns:\n",
    "        df1[col] = None\n",
    "for col in [\"city\", \"state\", \"zipcode\"]:\n",
    "    if col not in df1.columns:\n",
    "        df1[col] = None\n",
    "\n",
    "\n",
    "# Build full associated_address = \"<street>, <City>, <State> <Zip>\"\n",
    "def _compose_full_address(row) -> str | None:\n",
    "    # Prefer an existing complete address if present\n",
    "    ca = row.get(\"complete_address\") if \"complete_address\" in row else None\n",
    "    if isinstance(ca, str) and ca.strip():\n",
    "        return ca.strip()\n",
    "\n",
    "    street = row.get(\"associated_address\")\n",
    "    if _is_blank_val(street):\n",
    "        street = row.get(\"address\")\n",
    "\n",
    "    city = row.get(\"city\")\n",
    "    state = row.get(\"state\")\n",
    "    zipcode = row.get(\"zipcode\")\n",
    "\n",
    "    has_street = not _is_blank_val(street)\n",
    "    has_city = not _is_blank_val(city)\n",
    "    has_state = not _is_blank_val(state)\n",
    "    has_zip = not _is_blank_val(zipcode)\n",
    "\n",
    "    if not any([has_street, has_city, has_state, has_zip]):\n",
    "        return None\n",
    "\n",
    "    parts = []\n",
    "    if has_street:\n",
    "        parts.append(str(street).strip())\n",
    "\n",
    "    loc = []\n",
    "    if has_city:\n",
    "        loc.append(str(city).strip())\n",
    "    if has_state:\n",
    "        loc.append(str(state).strip())\n",
    "\n",
    "    loc_str = \", \".join(loc) if loc else \"\"\n",
    "    if loc_str:\n",
    "        if parts:\n",
    "            parts[-1] = f\"{parts[-1]}, {loc_str}\"\n",
    "        else:\n",
    "            parts.append(loc_str)\n",
    "\n",
    "    if has_zip:\n",
    "        if parts:\n",
    "            parts[-1] = f\"{parts[-1]} {str(zipcode).strip()}\"\n",
    "        else:\n",
    "            parts.append(str(zipcode).strip())\n",
    "\n",
    "    return parts[0] if parts else None\n",
    "\n",
    "\n",
    "df1[\"associated_address\"] = df1.apply(_compose_full_address, axis=1)\n",
    "\n",
    "# file_created/modified <= file_creation_date/modification_date (keep only file_created/file_modified in report)\n",
    "if \"file_created\" not in df1.columns:\n",
    "    df1[\"file_created\"] = None\n",
    "if \"file_creation_date\" in df1.columns:\n",
    "    mask = _blank_mask(df1[\"file_created\"])\n",
    "    df1.loc[mask, \"file_created\"] = df1.loc[mask, \"file_creation_date\"]\n",
    "\n",
    "if \"file_modified\" not in df1.columns:\n",
    "    df1[\"file_modified\"] = None\n",
    "if \"file_modification_date\" in df1.columns:\n",
    "    mask = _blank_mask(df1[\"file_modified\"])\n",
    "    df1.loc[mask, \"file_modified\"] = df1.loc[mask, \"file_modification_date\"]\n",
    "\n",
    "# Create/normalize source_file (prefer file_location relative; else infer from filename)\n",
    "if \"source_file\" not in df1.columns:\n",
    "    df1[\"source_file\"] = None\n",
    "\n",
    "if \"file_location\" in df1.columns:\n",
    "    mask = _blank_mask(df1[\"source_file\"])\n",
    "    df1.loc[mask, \"source_file\"] = df1.loc[mask, \"file_location\"].apply(_to_rel)\n",
    "\n",
    "if \"filename\" in df1.columns:\n",
    "    mask = _blank_mask(df1[\"source_file\"])\n",
    "    df1.loc[mask, \"source_file\"] = df1.loc[mask, \"filename\"].apply(\n",
    "        lambda n: os.path.join(\"test\", n) if isinstance(n, str) and n else n\n",
    "    )\n",
    "\n",
    "df1[\"source_file\"] = df1[\"source_file\"].apply(_normalize_source_file)\n",
    "\n",
    "# 2) Augment with metadata fields (platform, creation_source, creation_reliable)\n",
    "for col in [\"platform\", \"creation_source\", \"creation_reliable\"]:\n",
    "    if col not in df1.columns:\n",
    "        df1[col] = None\n",
    "\n",
    "# Populate metadata by re-calling get_file_metadata where missing\n",
    "if \"file_location\" in df1.columns:\n",
    "\n",
    "    def _augment_row(row):\n",
    "        need_meta = (\n",
    "            _is_blank_val(row.get(\"platform\"))\n",
    "            or _is_blank_val(row.get(\"creation_source\"))\n",
    "            or _is_blank_val(row.get(\"creation_reliable\"))\n",
    "        )\n",
    "        loc = row.get(\"file_location\")\n",
    "        cand = None\n",
    "        if isinstance(loc, str) and loc:\n",
    "            cand = loc if os.path.isabs(loc) else os.path.join(ROOT, loc)\n",
    "        if need_meta and isinstance(cand, str) and os.path.exists(cand):\n",
    "            try:\n",
    "                md = get_file_metadata(cand)  # from Cell 2\n",
    "                row[\"platform\"] = (\n",
    "                    row.get(\"platform\")\n",
    "                    if not _is_blank_val(row.get(\"platform\"))\n",
    "                    else md.get(\"platform\")\n",
    "                )\n",
    "                row[\"creation_source\"] = (\n",
    "                    row.get(\"creation_source\")\n",
    "                    if not _is_blank_val(row.get(\"creation_source\"))\n",
    "                    else md.get(\"creation_source\")\n",
    "                )\n",
    "                row[\"creation_reliable\"] = (\n",
    "                    row.get(\"creation_reliable\")\n",
    "                    if not _is_blank_val(row.get(\"creation_reliable\"))\n",
    "                    else md.get(\"creation_reliable\")\n",
    "                )\n",
    "                if _is_blank_val(row.get(\"file_created\")):\n",
    "                    row[\"file_created\"] = md.get(\"file_creation_date\")\n",
    "                if _is_blank_val(row.get(\"file_modified\")):\n",
    "                    row[\"file_modified\"] = md.get(\"file_modification_date\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        return row\n",
    "\n",
    "    df1 = df1.apply(_augment_row, axis=1)\n",
    "\n",
    "# Normalize file_location for report view to relative (if present)\n",
    "if \"file_location\" in df1.columns:\n",
    "    df1[\"file_location\"] = df1[\"file_location\"].apply(_to_rel)\n",
    "\n",
    "# 3) Deduplicate (serial_number + source_file)\n",
    "if {\"serial_number\", \"source_file\"}.issubset(df1.columns):\n",
    "    before = len(df1)\n",
    "    df1 = df1.drop_duplicates(subset=[\"serial_number\", \"source_file\"], keep=\"first\")\n",
    "    after = len(df1)\n",
    "    if before != after:\n",
    "        print(\n",
    "            f\"Deduplicated rows: {before - after} removed (serial_number + source_file).\"\n",
    "        )\n",
    "\n",
    "# 4) Final report column order (suppress duplicate/legacy columns)\n",
    "preferred_order = [\n",
    "    \"serial_number\",\n",
    "    \"event_type\",\n",
    "    \"event_date\",\n",
    "    \"associated_name\",\n",
    "    \"associated_address\",\n",
    "    \"source_file\",  # keep only this (drop filename)\n",
    "    \"file_created\",  # keep only this (drop file_creation_date)\n",
    "    \"file_modified\",  # keep only this (drop file_modification_date)\n",
    "    \"file_location\",\n",
    "    \"platform\",\n",
    "    \"creation_source\",\n",
    "    \"creation_reliable\",\n",
    "]\n",
    "# Exclude legacy/duplicate columns:\n",
    "# - date/name/address/event_name/complete_address are legacy inputs\n",
    "# - zipcode/city/state are now part of associated_address\n",
    "# - filename duplicates source_file\n",
    "# - file_creation_date/file_modification_date duplicate file_created/file_modified\n",
    "suppress_cols = {\n",
    "    \"date\",\n",
    "    \"name\",\n",
    "    \"address\",\n",
    "    \"event_name\",\n",
    "    \"complete_address\",\n",
    "    \"zipcode\",\n",
    "    \"city\",\n",
    "    \"state\",\n",
    "    \"filename\",\n",
    "    \"file_creation_date\",\n",
    "    \"file_modification_date\",\n",
    "}\n",
    "extra_cols = [\n",
    "    c for c in df1.columns if c not in preferred_order and c not in suppress_cols\n",
    "]\n",
    "final_cols = [c for c in preferred_order if c in df1.columns] + extra_cols\n",
    "\n",
    "report_df = df1[final_cols].copy()\n",
    "\n",
    "# 5) Output\n",
    "reports_folder = os.path.join(os.getcwd(), \"reports\")\n",
    "os.makedirs(reports_folder, exist_ok=True)\n",
    "report_path = os.path.join(reports_folder, \"full_image_extraction_report.csv\")\n",
    "report_df.to_csv(report_path, index=False)\n",
    "\n",
    "print(f\"Full report written: {report_path}\")\n",
    "print(f\"Rows: {len(report_df)} | Columns: {len(report_df.columns)}\")\n",
    "print(\"Columns order:\")\n",
    "print(final_cols)\n",
    "\n",
    "display(report_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: Build Full Image Extraction Report with All Available Fields\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure df1 exists\n",
    "try:\n",
    "    df1  # noqa: F821\n",
    "except NameError:\n",
    "    raise RuntimeError(\"df1 is not defined earlier in the notebook.\")\n",
    "\n",
    "# 1) Standardize / create unified semantic columns used in prior lightweight CSV outputs\n",
    "if \"event_type\" not in df1.columns and \"event_name\" in df1.columns:\n",
    "    df1[\"event_type\"] = df1[\"event_name\"]\n",
    "elif \"event_type\" not in df1.columns:\n",
    "    df1[\"event_type\"] = None\n",
    "\n",
    "if \"event_date\" not in df1.columns and \"date\" in df1.columns:\n",
    "    df1[\"event_date\"] = df1[\"date\"]\n",
    "\n",
    "if \"associated_name\" not in df1.columns and \"name\" in df1.columns:\n",
    "    df1[\"associated_name\"] = df1[\"name\"]\n",
    "\n",
    "if \"associated_address\" not in df1.columns and \"address\" in df1.columns:\n",
    "    df1[\"associated_address\"] = df1[\"address\"]\n",
    "\n",
    "if \"file_created\" not in df1.columns and \"file_creation_date\" in df1.columns:\n",
    "    df1[\"file_created\"] = df1[\"file_creation_date\"]\n",
    "\n",
    "if \"file_modified\" not in df1.columns and \"file_modification_date\" in df1.columns:\n",
    "    df1[\"file_modified\"] = df1[\"file_modification_date\"]\n",
    "\n",
    "# Relative path helpers\n",
    "ROOT = os.getcwd()\n",
    "\n",
    "\n",
    "def _to_rel(p: str) -> str:\n",
    "    if isinstance(p, str) and p:\n",
    "        try:\n",
    "            return os.path.relpath(p, ROOT)\n",
    "        except Exception:\n",
    "            return os.path.basename(p)\n",
    "    return p\n",
    "\n",
    "\n",
    "def _normalize_source_file(s: str) -> str:\n",
    "    if isinstance(s, str) and s:\n",
    "        try:\n",
    "            return os.path.relpath(s, ROOT) if os.path.isabs(s) else s\n",
    "        except Exception:\n",
    "            return os.path.basename(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "# Create/normalize source_file (always make it relative)\n",
    "if \"source_file\" not in df1.columns:\n",
    "    if \"file_location\" in df1.columns:\n",
    "        df1[\"source_file\"] = df1[\"file_location\"].apply(_to_rel)\n",
    "    elif \"filename\" in df1.columns:\n",
    "        # Assume files live under ./test for relative paths\n",
    "        df1[\"source_file\"] = df1[\"filename\"].apply(\n",
    "            lambda n: os.path.join(\"test\", n) if isinstance(n, str) and n else n\n",
    "        )\n",
    "    else:\n",
    "        df1[\"source_file\"] = None\n",
    "else:\n",
    "    df1[\"source_file\"] = df1[\"source_file\"].apply(_normalize_source_file)\n",
    "\n",
    "# 2) Augment with metadata fields (platform, creation_source, creation_reliable) if absent\n",
    "for col in [\"platform\", \"creation_source\", \"creation_reliable\"]:\n",
    "    if col not in df1.columns:\n",
    "        df1[col] = None\n",
    "\n",
    "# Populate metadata columns by re-calling get_file_metadata where missing\n",
    "if \"file_location\" in df1.columns:\n",
    "\n",
    "    def _augment_row(row):\n",
    "        needs_platform = (\n",
    "            row.get(\"platform\") in (None, \"\")\n",
    "            or row.get(\"creation_source\") in (None, \"\")\n",
    "            or row.get(\"creation_reliable\") in (None, \"\")\n",
    "        )\n",
    "        loc = row.get(\"file_location\")\n",
    "        if needs_platform and isinstance(loc, str) and os.path.exists(loc):\n",
    "            try:\n",
    "                md = get_file_metadata(loc)  # defined earlier in the notebook\n",
    "                row[\"platform\"] = md.get(\"platform\")\n",
    "                row[\"creation_source\"] = md.get(\"creation_source\")\n",
    "                row[\"creation_reliable\"] = md.get(\"creation_reliable\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        return row\n",
    "\n",
    "    df1 = df1.apply(_augment_row, axis=1)\n",
    "\n",
    "# Normalize file_location to relative path for the report view\n",
    "if \"file_location\" in df1.columns:\n",
    "    df1[\"file_location\"] = df1[\"file_location\"].apply(_to_rel)\n",
    "\n",
    "# 3) Deduplicate (serial_number + source_file)\n",
    "if {\"serial_number\", \"source_file\"}.issubset(df1.columns):\n",
    "    before = len(df1)\n",
    "    df1 = df1.drop_duplicates(subset=[\"serial_number\", \"source_file\"], keep=\"first\")\n",
    "    after = len(df1)\n",
    "    if before != after:\n",
    "        print(\n",
    "            f\"Deduplicated rows: {before - after} removed (serial_number + source_file).\"\n",
    "        )\n",
    "\n",
    "# 4) Final report column order\n",
    "preferred_order = [\n",
    "    \"serial_number\",\n",
    "    \"event_type\",\n",
    "    \"event_date\",\n",
    "    \"associated_name\",\n",
    "    \"associated_address\",\n",
    "    \"zipcode\",\n",
    "    \"city\",\n",
    "    \"state\",\n",
    "    \"source_file\",  # relative\n",
    "    \"filename\",\n",
    "    \"file_created\",\n",
    "    \"file_modified\",\n",
    "    \"file_creation_date\",\n",
    "    \"file_modification_date\",\n",
    "    \"file_location\",  # relative\n",
    "    \"platform\",\n",
    "    \"creation_source\",\n",
    "    \"creation_reliable\",\n",
    "]\n",
    "extra_cols = [c for c in df1.columns if c not in preferred_order]\n",
    "final_cols = [c for c in preferred_order if c in df1.columns] + extra_cols\n",
    "\n",
    "report_df = df1[final_cols].copy()\n",
    "\n",
    "# 5) Output\n",
    "reports_folder = os.path.join(os.getcwd(), \"reports\")\n",
    "os.makedirs(reports_folder, exist_ok=True)\n",
    "report_path = os.path.join(reports_folder, \"full_image_extraction_report.csv\")\n",
    "report_df.to_csv(report_path, index=False)\n",
    "\n",
    "print(f\"Full report written: {report_path}\")\n",
    "print(f\"Rows: {len(report_df)} | Columns: {len(report_df.columns)}\")\n",
    "print(\"Columns order:\")\n",
    "print(final_cols)\n",
    "\n",
    "display(report_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f9695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: Build Full Image Extraction Report with All Available Fields\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure df1 exists\n",
    "try:\n",
    "    df1  # noqa: F821\n",
    "except NameError:\n",
    "    raise RuntimeError(\"df1 is not defined earlier in the notebook.\")\n",
    "\n",
    "# 1. Standardize / create unified semantic columns used in prior lightweight CSV outputs\n",
    "# Map existing columns to target report schema\n",
    "if \"event_type\" not in df1.columns and \"event_name\" in df1.columns:\n",
    "    df1[\"event_type\"] = df1[\"event_name\"]\n",
    "elif \"event_type\" not in df1.columns:\n",
    "    df1[\"event_type\"] = None\n",
    "\n",
    "if \"event_date\" not in df1.columns and \"date\" in df1.columns:\n",
    "    df1[\"event_date\"] = df1[\"date\"]\n",
    "\n",
    "if \"associated_name\" not in df1.columns and \"name\" in df1.columns:\n",
    "    df1[\"associated_name\"] = df1[\"name\"]\n",
    "\n",
    "if \"associated_address\" not in df1.columns and \"address\" in df1.columns:\n",
    "    df1[\"associated_address\"] = df1[\"address\"]\n",
    "\n",
    "if \"file_created\" not in df1.columns and \"file_creation_date\" in df1.columns:\n",
    "    df1[\"file_created\"] = df1[\"file_creation_date\"]\n",
    "\n",
    "if \"file_modified\" not in df1.columns and \"file_modification_date\" in df1.columns:\n",
    "    df1[\"file_modified\"] = df1[\"file_modification_date\"]\n",
    "\n",
    "# Create source_file (human-friendly) separate from full file_location if not already present\n",
    "if \"source_file\" not in df1.columns:\n",
    "    if \"filename\" in df1.columns:\n",
    "        df1[\"source_file\"] = df1[\"filename\"]\n",
    "    elif \"file_location\" in df1.columns:\n",
    "        df1[\"source_file\"] = df1[\"file_location\"].apply(\n",
    "            lambda p: os.path.basename(p) if isinstance(p, str) else p\n",
    "        )\n",
    "    else:\n",
    "        df1[\"source_file\"] = None\n",
    "\n",
    "# 2. Augment with metadata fields (platform, creation_source, creation_reliable) if absent\n",
    "for col in [\"platform\", \"creation_source\", \"creation_reliable\"]:\n",
    "    if col not in df1.columns:\n",
    "        df1[col] = None\n",
    "\n",
    "# Populate metadata columns by re-calling get_file_metadata where missing\n",
    "if \"file_location\" in df1.columns:\n",
    "\n",
    "    def _augment_row(row):\n",
    "        needs_platform = (\n",
    "            row.get(\"platform\") in (None, \"\")\n",
    "            or row.get(\"creation_source\") in (None, \"\")\n",
    "            or row.get(\"creation_reliable\") in (None, \"\")\n",
    "        )\n",
    "        loc = row.get(\"file_location\")\n",
    "        if needs_platform and isinstance(loc, str) and os.path.exists(loc):\n",
    "            try:\n",
    "                md = get_file_metadata(loc)  # uses earlier defined function\n",
    "                row[\"platform\"] = md.get(\"platform\")\n",
    "                row[\"creation_source\"] = md.get(\"creation_source\")\n",
    "                row[\"creation_reliable\"] = md.get(\"creation_reliable\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        return row\n",
    "\n",
    "    df1 = df1.apply(_augment_row, axis=1)\n",
    "\n",
    "# 3. (Optional) Deduplicate if multiple identical serial_number + source_file rows produced\n",
    "if {\"serial_number\", \"source_file\"}.issubset(df1.columns):\n",
    "    before = len(df1)\n",
    "    df1 = df1.drop_duplicates(subset=[\"serial_number\", \"source_file\"], keep=\"first\")\n",
    "    after = len(df1)\n",
    "    if before != after:\n",
    "        print(\n",
    "            f\"Deduplicated rows: {before - after} removed (serial_number + source_file).\"\n",
    "        )\n",
    "\n",
    "# 4. Define final report column order (include everything available + fallbacks)\n",
    "preferred_order = [\n",
    "    \"serial_number\",\n",
    "    \"event_type\",\n",
    "    \"event_date\",\n",
    "    \"associated_name\",\n",
    "    \"associated_address\",\n",
    "    \"zipcode\",\n",
    "    \"city\",\n",
    "    \"state\",\n",
    "    \"source_file\",\n",
    "    \"filename\",\n",
    "    \"file_created\",\n",
    "    \"file_modified\",\n",
    "    \"file_creation_date\",\n",
    "    \"file_modification_date\",\n",
    "    \"file_location\",\n",
    "    \"platform\",\n",
    "    \"creation_source\",\n",
    "    \"creation_reliable\",\n",
    "]\n",
    "\n",
    "# Include any additional dynamic columns (e.g., event_name, confidence scores if later added)\n",
    "extra_cols = [c for c in df1.columns if c not in preferred_order]\n",
    "final_cols = [c for c in preferred_order if c in df1.columns] + extra_cols\n",
    "\n",
    "report_df = df1[final_cols].copy()\n",
    "\n",
    "# 5. Output directory & file\n",
    "reports_folder = os.path.join(os.getcwd(), \"reports\")\n",
    "os.makedirs(reports_folder, exist_ok=True)\n",
    "report_path = os.path.join(reports_folder, \"full_image_extraction_report.csv\")\n",
    "report_df.to_csv(report_path, index=False)\n",
    "\n",
    "print(f\"Full report written: {report_path}\")\n",
    "print(f\"Rows: {len(report_df)} | Columns: {len(report_df.columns)}\")\n",
    "print(\"Columns order:\")\n",
    "print(final_cols)\n",
    "\n",
    "# Display a preview\n",
    "display(report_df.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allied",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "vincent": {
   "sessionId": "b5b8fd08ca7555b21d5de782_2025-08-04T14-23-19-448Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
